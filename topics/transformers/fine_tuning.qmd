---
title: "Fine-Tuning BERT on TweetEval Sentiment"

execute:
  message: false
  warning: false
  
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{r set_up_python}
#| echo: false

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = "C:\\Users\\internet\\AppData\\Local\\Programs\\Python\\Python311\\python.exe"
  
} else {
  
  python_path = "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python311\\python.exe"
}

reticulate::use_python(python_path, required = TRUE)

```

```{python supress_warning}
#| echo: false


import warnings
warnings.filterwarnings("ignore")

import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

```


```{python import_data_set}
#| results: hide

#!pip install -q transformers datasets accelerate

from datasets import load_dataset
from transformers import BertTokenizerFast
from collections import Counter

dataset = load_dataset("tweet_eval", "sentiment")

N_train = 2000
N_val   = 500

dataset["train"] = dataset["train"].select(range(N_train))

dataset["validation"] = dataset["validation"].select(range(N_val))

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

```
## Dataset: TweetEval Sentiment

For this tutorial we use the **TweetEval Sentiment** dataset, a benchmark collection of tweets labeled for sentiment analysis. The dataset was introduced as part of the TweetEval benchmark, which unifies several Twitter NLP tasks under a common framework.

The sentiment subset contains tweets annotated with three classes:

- **0 — Negative**
- **1 — Neutral**
- **2 — Positive**

TweetEval is well-suited for fine-tuning transformer models because:

- Tweets contain slang, emojis, irregular grammar, abbreviations, and sarcasm.
- These characteristics make the domain challenging for general pre-trained models.
- Fine-tuning allows BERT to adapt from formal text (BooksCorpus, Wikipedia) to the informal, noisy style of Twitter.

### Dataset Size (Before Downsampling)

The full dataset includes approximately:

- **45,000 tweets** for training  
- **12,000 tweets** for validation  
- **2,000 tweets** for testing  

Since full training can be slow during development, this notebook uses a **downsampled version**

## Tokenization

Before we can feed text into BERT, we must convert each tweet into the numerical format that the model expects. BERT does not work directly with raw strings—it operates on token IDs and attention masks. The tokenizer performs this conversion and applies several important preprocessing steps.

BERT uses a **WordPiece tokenizer**, which breaks text into subword units. This allows the model to handle noisy and informal language often found in tweets, including slang, abbreviations, hashtags, and even misspellings. If a word is not in the vocabulary, it is decomposed into smaller subwords that BERT can still interpret meaningfully.

During tokenization, the tokenizer also:

-   **Adds special tokens** such as `[CLS]` (classification token) and `[SEP]` (separator).
-   **Truncates** sequences so they fit within a fixed maximum length (64 tokens in this tutorial).
-   **Pads** shorter sequences so all inputs are the same length.
-   **Builds an attention mask**, which tells BERT which tokens are real and which are padding.

After tokenization, each example in the dataset contains:

-   `input_ids`: the numerical tokens representing the text\
-   `attention_mask`: indicators showing which positions should be attended to\
-   `label`: the sentiment class

We apply the tokenizer to the entire dataset using the `map()` function and then reformat the output so it can be used directly with PyTorch.

```{python tokenize}
#| results: hide

def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=64
    )

tokenized_dataset = dataset.map(tokenize_batch, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

tokenized_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "label"]
)


```

## Preparing DataLoaders

After tokenization, the dataset contains all the components BERT needs—`input_ids`, `attention_mask`, and `label`. The next step is to prepare these examples for efficient training.

Neural networks in PyTorch expect data to be provided through **DataLoaders**, which handle batching, shuffling, and iteration over the dataset. This is especially important for transformer models, where training is computationally intensive and must be performed in batches that fit into GPU memory.

We construct two DataLoaders:

-   **Training DataLoader**\
    Shuffles the data at every epoch to prevent the model from learning order-specific patterns and to encourage better generalization.

-   **Validation DataLoader**\
    Does not shuffle the data and is used to evaluate model performance after training without introducing randomness.

Each batch produced by the DataLoader contains:

-   a tensor of tokenized tweets (`input_ids`)
-   a tensor of attention masks (`attention_mask`)
-   a tensor of labels

These will be passed directly into BERT during training and evaluation. Creating DataLoaders ensures that the model receives input in a structured, optimized, and reproducible format.

```{python data_loaders}
#| results: hide

# Creating DataLoaders for training and validation

from torch.utils.data import DataLoader

batch_size = 32

train_loader = DataLoader(
    tokenized_dataset["train"],
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    tokenized_dataset["validation"],
    batch_size=batch_size,
    shuffle=False
)


```


## Baseline Model (No Fine-Tuning)

Before applying any supervised training, it is essential to establish a **baseline performance level**. This baseline measures how well a pre-trained BERT model performs *without* learning anything about sentiment classification and *without* adapting to the style of Twitter language.

BERT is trained on large, general-purpose corpora (BooksCorpus and Wikipedia), but it has **never been exposed to sentiment labels** and has **never been trained on the informal, noisy structure of tweets**. Tweets differ from formal text in several ways: they are short, contain slang, emojis, abbreviations, hashtags, and often rely on irony or sarcasm. A pretrained BERT encoder, used as-is, typically performs poorly on such data.

To evaluate this, we construct a model by attaching a small **classification head** to the `[CLS]` embedding produced by BERT. This head is required because BERT alone cannot output sentiment labels; it only produces contextual representations. In the baseline scenario:

- all of BERT’s encoder parameters are **frozen**,  
- the classification head remains **randomly initialized**,  
- no supervised training is performed on either component,  
- the model therefore performs **zero-shot** sentiment classification.

Zero-shot here means that the model has seen **zero sentiment-labeled examples** during training; it relies solely on BERT’s general language understanding and a random classifier.

Evaluating this zero-shot model gives us a meaningful lower bound:  
**How well does BERT handle tweet sentiment without any supervised training or domain adaptation?**

This baseline accuracy will later serve as a reference point when comparing partial and full fine-tuning results.


```{python baseline}
#| results: hide
# Baseline Model (No Fine-Tuning)

from transformers import BertForSequenceClassification
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Load a pre-trained BERT classifier
baseline_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
).to(device)

# Freeze all encoder layers → no fine-tuning
for param in baseline_model.bert.parameters():
    param.requires_grad = False

# Evaluation function
def evaluate(model, data_loader):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for batch in data_loader:
            ids = batch["input_ids"].to(device)
            mask = batch["attention_mask"].to(device)
            y = batch["label"].to(device)

            logits = model(ids, attention_mask=mask).logits
            preds = torch.argmax(logits, dim=1)

            correct += (preds == y).sum().item()
            total += y.size(0)

    return correct / total

```

## Partial Fine-Tuning
### What Fine-Tuning Means in This Context

Fine-tuning is a **supervised learning process** in which BERT is trained on labeled examples for a specific downstream task—in this case, sentiment classification. During fine-tuning, the model receives tweet–label pairs such as:

- “I love this!” → Positive  
- “This is awful.” → Negative  
- “It’s okay, I guess.” → Neutral  

By learning from these supervised labels, BERT adjusts its internal parameters so that its representations become more useful for predicting sentiment. This is fundamentally different from the baseline model, where no labeled training occurs and no parameters are updated. Fine-tuning therefore means:

- the model learns from **labeled sentiment data**,  
- selected parameters are updated during training,  
- performance improves significantly over the zero-shot baseline.

With this definition in place, we now describe how *partial* fine-tuning selectively updates only the upper layers of BERT while keeping the lower layers frozen.

---

The baseline model demonstrates how poorly a pre-trained BERT performs when it is not adapted to the TweetEval domain. To improve performance while still keeping computational cost low, we apply **partial fine-tuning**.

BERT consists of 12 transformer encoder layers stacked on top of one another. The lower layers typically learn general linguistic features such as token identity, morphology, and short-range dependencies. The upper layers capture more **task-specific** information, including sentiment cues and semantic relationships.

In partial fine-tuning, we freeze the **lower encoder layers** (layers 0–8) and train only the **upper layers** (layers 9–11) together with the classification head. This approach:

- reduces the number of trainable parameters,  
- speeds up training,  
- lowers VRAM requirements,  
- reduces the risk of overfitting on a small dataset,  
- still allows BERT to learn important domain-specific patterns from tweets.  

Partial fine-tuning typically yields a large improvement in accuracy compared to the baseline while remaining efficient enough to run quickly in environments like Google Colab.

The next code block implements partial fine-tuning for one epoch and evaluates its performance.

```{python partial_tune}
#| results: hide
# Partial Fine-Tuning (train upper layers only)

from transformers import BertForSequenceClassification
from torch.optim import AdamW
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load a fresh BERT model
partial_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
).to(device)

# Freeze lower BERT layers: 0–8
for layer_idx in range(9):
    for param in partial_model.bert.encoder.layer[layer_idx].parameters():
        param.requires_grad = False

# Train only layers 9, 10, 11 + classifier head
trainable_params = [p for p in partial_model.parameters() if p.requires_grad]


optimizer = AdamW(trainable_params, lr=2e-5)

# One training epoch
partial_model.train()
for batch in train_loader:
    optimizer.zero_grad()

    ids = batch["input_ids"].to(device)
    mask = batch["attention_mask"].to(device)
    y = batch["label"].to(device)

    outputs = partial_model(ids, attention_mask=mask, labels=y)
    loss = outputs.loss
    loss.backward()
    optimizer.step()


```


## Full Fine-Tuning

Partial fine-tuning restricts learning to the upper encoder layers, allowing BERT to adjust some of its representations while keeping the lower layers fixed. To give the model maximal flexibility, we now perform **full fine-tuning**, where **every parameter in the network is updated** based on labeled sentiment examples.

### What Makes Full Fine-Tuning Different?

In full fine-tuning:

- **All 12 transformer layers are trainable**,  
- The attention heads inside each layer are updated,  
- The intermediate feedforward networks are updated,  
- Layer normalization parameters are updated,  
- The classification head is updated as well.

This means the entire model — from low-level token embeddings to high-level semantics — can adapt to the specific characteristics of Twitter language.

Full fine-tuning is the complete opposite of the baseline:

- In the baseline: **no supervised learning** occurs and all encoder weights remain fixed.  
- In partial fine-tuning: **only the upper layers and classifier learn**.  
- In full fine-tuning: **all layers and the classifier learn simultaneously**.

Because sentiment classification is a supervised task, full fine-tuning allows BERT to integrate the sentiment labels deeply into every layer of its internal representation.

### Why Full Fine-Tuning Usually Performs Best

Tweets contain irregular grammar, emojis, sarcasm, abbreviations, and other linguistic patterns that differ significantly from BERT’s pretraining domain. When all parameters are trainable, the model can:

- refine embeddings for informal vocabulary,  
- adjust attention patterns to better detect sentiment cues,  
- reorganize its semantic space around the three sentiment labels,  
- learn subtle distinctions that partial tuning cannot fully capture.

The trade-offs are higher computational cost and potentially longer training time, but the improvement in performance is typically notable.

The next code block performs one epoch of full fine-tuning and then evaluates the model on the validation set.

```{python full_train}
#| results: hide
# Full Fine-Tuning (train ALL layers)

from transformers import BertForSequenceClassification
from torch.optim import AdamW
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Load a fresh BERT model for full fine-tuning
full_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
).to(device)

# Unfreeze ALL parameters → full end-to-end training
for param in full_model.parameters():
    param.requires_grad = True

# Count total trainable parameters
trainable_params = [p for p in full_model.parameters() if p.requires_grad]


optimizer = AdamW(trainable_params, lr=2e-5)

# One training epoch
full_model.train()
for batch in train_loader:
    optimizer.zero_grad()

    ids = batch["input_ids"].to(device)
    mask = batch["attention_mask"].to(device)
    y = batch["label"].to(device)

    outputs = full_model(ids, attention_mask=mask, labels=y)
    loss = outputs.loss
    loss.backward()
    optimizer.step()


```


## Comparing the Three Models

Now that we have trained all three versions of the model—baseline, partial fine-tuning, and full fine-tuning—we can compare their performance directly. Understanding **which parts of the model are allowed to learn** is essential for interpreting these results.

### What Learns in Each Model?

- **Baseline (Zero-Shot)**
  - BERT encoder: **frozen**
  - Classification head: **untrained**
  - Supervised learning: **none**
  
  The model makes predictions using BERT’s general language understanding and a random classifier. No part of the network adapts to sentiment labels or Twitter-style text.

- **Partial Fine-Tuning**
  - BERT layers 0–8: **frozen**
  - BERT layers 9–11: **trainable**
  - Classification head: **trainable**
  - Supervised learning: **upper encoder layers + classifier**

  The model can learn high-level sentiment cues but keeps foundational linguistic representations fixed.

- **Full Fine-Tuning**
  - All BERT layers: **trainable**
  - Classification head: **trainable**
  - Supervised learning: **entire model**

  Every parameter in BERT adjusts to sentiment labels and Twitter’s linguistic characteristics.

### Why These Differences Matter

Evaluating the models side by side clearly demonstrates the effect of supervision:

- The **baseline** reflects how poorly a non-adapted model handles Twitter sentiment.  
- **Partial fine-tuning** often yields a large performance boost by allowing only the upper semantic layers to adapt.  
- **Full fine-tuning** typically achieves the best performance because all layers participate in learning the sentiment task.

This progression — from zero learning, to partial learning, to full learning — illustrates the core principle of transfer learning with BERT.

The following code block prints the accuracy of each model for direct comparison.


### Why Comparison Matters

Evaluating all three models side by side reveals:

- **The impact of domain mismatch:**  
  The baseline model performs poorly because raw BERT has never seen the structure of tweets, which often include emojis, slang, abbreviations, and informal grammar.

- **The benefit of partial fine-tuning:**  
  Training only the top transformer layers allows the model to adjust its higher-level semantic representations, significantly improving accuracy without updating the entire network.

- **The advantages of full fine-tuning:**  
  Updating all layers gives the model maximal flexibility to adapt to the tweet domain, typically yielding the highest performance.

### What We Expect to See

- Baseline accuracy: very low  
- Partial fine-tuning: large improvement  
- Full fine-tuning: best performance overall  

This comparison provides the clearest demonstration of why fine-tuning is essential in modern NLP workflows. The next code block prints all three accuracies side by side.


```{python evaluation}

import pandas as pd

baseline_acc = evaluate(baseline_model, val_loader)

partial_acc = evaluate(partial_model, val_loader)

full_acc = evaluate(full_model, val_loader)

# Collect metrics into a DataFrame
results_df = pd.DataFrame({
    "Model": [
        "Baseline (no fine-tuning)",
        "Partial fine-tuning",
        "Full fine-tuning"
    ],
    "Accuracy": [
        round(baseline_acc, 4),
        round(partial_acc, 4),
        round(full_acc, 4)
    ]
})

print(results_df)


```


## Qualitative Comparison

Numerical accuracy provides a useful summary, but qualitative examples make the differences between the three models immediately visible. By examining real tweets, we can observe how each model behaves based on **which parts of the network were allowed to learn**.

### How Each Model Behaves on Individual Tweets

- **Baseline (Zero-Shot)**
  - BERT encoder: frozen  
  - Classifier: untrained  
  - No supervised learning  

  Because neither the encoder nor the classifier has seen sentiment labels, the model often misinterprets sentiment cues, ignores emojis, and fails to recognize sarcasm or informal grammar. Predictions may appear arbitrary or overly biased toward one class.

- **Partial Fine-Tuning**
  - Only upper BERT layers + classifier learn  
  - Lower linguistic layers remain fixed  

  This model improves considerably on clear sentiment cues—positive phrases, strong negative language, common emoji patterns. However, it may still struggle with subtle or ambiguous cases because only part of the model adapted to the task.

- **Full Fine-Tuning**
  - All BERT layers learn from supervised labels  

  This model generally provides the most reliable and stable predictions. It adapts deeply to Twitter’s informal style, handles emojis better, and captures sentiment even in short or noisy tweets. Sarcasm and ambiguous expressions may still be difficult, but performance is consistently superior.

### What We Look For in This Comparison

By comparing the predictions of all three models on the same tweet, we can see:

- how **supervised learning** changes the model’s understanding,  
- how much improvement occurs when more layers are allowed to adapt,  
- how full fine-tuning overcomes many of the weaknesses of the zero-shot and partially tuned models.

The next code block selects random validation tweets and prints the predictions from all three models alongside their true labels.


```{python compare_tweets}

# Qualitative Comparison: Predictions on Sample Tweets

import random
import torch

label_names = dataset["train"].features["label"].names

# Select 10 random validation examples
indices = random.sample(range(len(dataset["validation"])), 10)

print("\n=== QUALITATIVE MODEL COMPARISON ===\n")

for idx in indices:
    text = dataset["validation"][idx]["text"]
    true_label = label_names[dataset["validation"][idx]["label"]]

    # Tokenize a single tweet
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=64
    ).to(device)

    # Predictions from each model
    baseline_pred = label_names[baseline_model(**inputs).logits.argmax(dim=1).item()]
    partial_pred  = label_names[partial_model(**inputs).logits.argmax(dim=1).item()]
    full_pred     = label_names[full_model(**inputs).logits.argmax(dim=1).item()]

    print(f"Tweet: {text}")
    print(f"True label:       {true_label}")
    print(f"Baseline:         {baseline_pred}")
    print(f"Partial-tuned:    {partial_pred}")
    print(f"Full-tuned:       {full_pred}")
    print("-" * 80)


```

## Summary: From Zero-Shot to Full Fine-Tuning

This notebook demonstrates the full progression of adapting BERT to a supervised sentiment classification task:

1. **Baseline (Zero-Shot)**  
   - No supervised learning  
   - BERT encoder frozen  
   - Classifier untrained  
   - Predictions come from general language knowledge only  

2. **Partial Fine-Tuning**  
   - Upper BERT layers + classifier learn from labeled data  
   - Lower layers remain frozen  
   - Model adapts partially to sentiment cues and Twitter language  

3. **Full Fine-Tuning**  
   - Every parameter in the network learns  
   - Model fully adapts to both the task and the domain  
   - Highest performance and most robust predictions  

This progression illustrates how supervised fine-tuning transforms BERT from a general-purpose text encoder into a task-specific sentiment classifier. The baseline shows how little BERT understands tweet sentiment without labeled training, partial fine-tuning demonstrates the benefit of updating high-level semantic layers, and full fine-tuning provides the strongest results by allowing all layers to adjust to the task.
