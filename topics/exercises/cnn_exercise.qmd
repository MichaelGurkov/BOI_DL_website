---
title: "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)"
format: html
---

# Convolutional Neural Network (CNN) Practice with MNIST

In this exercise you will:

-   Download the MNIST digit dataset from Kaggle.\
-   Perform a short exploratory data analysis (EDA).\
-   Build, train, and evaluate a basic Convolutional Neural Network (CNN).

Use **Python** (Google Colab) and **TensorFlow/Keras**.

## 1. Download MNIST Data from Kaggle

In this exercise we will use Kaggle’s **Digit Recognizer** dataset, which is a CSV version of the MNIST handwritten digits.

### 1.1 – Get the dataset from Kaggle

1.  Go to <https://www.kaggle.com>.
2.  Log in to your Kaggle account.
3.  In the search bar, type: **Digit Recognizer**.
4.  Open the competition named **“Digit Recognizer”**.
5.  Go to the **Data** tab.
6.  Download the following files:
    -   `train.csv`
    -   `test.csv` (optional, for later use such as Kaggle submission)

**Task 1**\
Make sure you have the file: `cnn_mnist_kaggle/data/train.csv` ready for the next step.

## 2. Load and Inspect the Data (EDA)

### 2.1 – Load the data

-   Load `train.csv` into a DataFrame.
-   Print its dimensions (number of rows and columns).
-   Display the first 5 rows to understand the structure.

### 2.2 – Basic EDA

-   Display basic dataset info (`dtypes`, missing values, etc.).
-   Review summary statistics of the pixel columns.
-   Count how many examples there are for each digit (0–9).
-   Plot a bar chart showing the frequency of each digit.

### 2.3 – Visualize sample digits

-   Select at least three rows from the dataset.
-   Extract the pixel values and reshape them into 28×28 images.
-   Display each image along with its true label.
-   Briefly describe what you observe (e.g., clarity, noise, differences between digits).

**Task 2**\
Perform a complete EDA: inspect structure, analyze label distribution, and visualize several example digits.

## 3. Prepare the Data for the CNN

### 3.1 – Separate features and labels

-   Split the dataset into:
    -   `X`: all pixel columns.
    -   `y`: the `label` column.
-   Verify their shapes.

### 3.2 – Create a training/validation split

-   Split the data into a training set and a validation set (for example: 80% / 20%).
-   Make sure the split preserves class proportions (stratified split).
-   Record the final sample counts for each split.

### 3.3 – Reshape and normalize the images

-   Reshape each observation from a flat vector of 784 values into a 28×28×1 tensor.
-   Convert pixel values to floats.
-   Normalize pixel values to the range \[0, 1\].

### 3.4 – Encode the labels

-   Convert the labels into one-hot encoded vectors with 10 classes (0–9).
-   Confirm the resulting shape of the encoded labels.

**Task 3**\
Prepare the data so it is ready to be fed into a CNN: split, reshape, normalize, and one-hot encode.


## 4. Build a Basic Convolutional Neural Network (CNN)

### 4.1 – Define the CNN architecture
Construct a simple CNN model using the following structure:

1. **Input layer**: images of shape 28×28×1  
2. **Convolution + Activation**:  
   - A convolutional layer (e.g., 32 filters, 3×3 kernel)  
   - ReLU activation  
3. **Max-Pooling**:  
   - 2×2 pooling window  
4. **Second Convolution + Activation**:  
   - A second convolutional layer (e.g., 64 filters, 3×3 kernel)  
   - ReLU activation  
5. **Second Max-Pooling**  
6. **Flatten layer** to convert feature maps to a vector  
7. **Fully connected (Dense) layer** with a chosen number of units (e.g., 128)  
8. **Output layer** with 10 units (digits 0–9) and softmax activation

### 4.2 – Summarize the model
- Print a model summary to verify the layer shapes.  
- Confirm that the number of parameters makes sense.

**Task 4**  
Build a CNN following the architecture above and generate a model summary.


## 5. Compile and Train the Model

### 5.1 – Compile the model
- Choose an optimizer (e.g., Adam).
- Use an appropriate loss function for multi-class classification (e.g., categorical crossentropy).
- Track accuracy as the evaluation metric.
- Confirm that the model compiles without errors.

### 5.2 – Train the model
- Train the model for a chosen number of epochs (e.g., 5–10).
- Use a reasonable batch size (e.g., 64 or 128).
- Provide the validation set for monitoring performance.
- Record training and validation accuracy after each epoch.
- Observe whether the model is overfitting, underfitting, or training as expected.

**Task 5**  
Compile the CNN and train it while monitoring accuracy and loss on both training and validation sets.

## 6. Evaluate the Model

### 6.1 – Visualize training progress
- Plot the training accuracy vs. epochs.
- Plot the validation accuracy vs. epochs.
- (Optional) Also plot training and validation loss.
- Comment on the learning curves:
  - Is validation accuracy close to training accuracy?
  - Do you see signs of overfitting?
  - Is more training needed?

### 6.2 – Evaluate model performance
- Evaluate the final model on the validation set and record:
  - Validation accuracy  
  - Validation loss
- Generate predictions for the validation set.
- Convert probabilities to class labels (0–9).

### 6.3 – Analyze errors
- Create a confusion matrix of true vs. predicted labels.
- Identify which digits are most frequently misclassified.
- Look at a few misclassified examples:
  - Display the image
  - Show the true label and predicted label
  - Briefly explain what might have caused the mistake

**Task 6**  
Evaluate the model quantitatively (accuracy, confusion matrix) and qualitatively (misclassified examples), and interpret the results.

