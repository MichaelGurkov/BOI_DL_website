---
title: "Convolutional Neural Network (CNN) for Image Classification"
---

Build, train, and evaluate a Convolutional Neural Network using the Sign Language MNIST dataset.


# Import and Load Data
- Import necessary libraries, including TensorFlow and Keras layers.
- Load the training and test datasets from CSV files.

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = "C:\\Users\\internet\\AppData\\Local\\Programs\\Python\\Python311\\python.exe"

} else {
  
  python_path = "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python311\\python.exe"
}

reticulate::use_python(python_path, required = TRUE)

```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt

import tensorflow as tf

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input


from tensorflow.keras import backend as K


from tf_keras_vis.gradcam import Gradcam
from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus
from tf_keras_vis.utils.scores import CategoricalScore
from tf_keras_vis.utils.model_modifiers import ReplaceToLinear
from tf_keras_vis.utils import normalize
from tf_keras_vis.saliency import Saliency

```

```{python import_data}
#| code-fold: true
#| code-summary: "Show the code"


train_set = pd.read_csv(os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"),
"data\\sign_mnist_train.csv"))

test_set = pd.read_csv(os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"),
"data\\sign_mnist_test.csv"))


```

# Preprocessing Labels
- The Sign Language MNIST dataset contains labels for letters A–Z, but the letter **J** has no images.
- Because of this, the dataset's labels skip the index 9.
- To make the labels continuous (0–23), we:
  - Keep labels 0–8 as they are.
  - Subtract 1 from all labels ≥ 10.
- Then we apply one-hot encoding to obtain 24 output classes.
- A helper function (`reverse_remap`) converts predictions back to the original A–Z index range when needed.


```{python auxilary_functions}
#| code-fold: true
#| code-summary: "Show the code"


def preprocess_data(df, img_height=28, img_width=28):
  
  processed_df = df / 255.0

  processed_df = processed_df.values.reshape(-1, img_height, img_width, 1).copy()

  return processed_df


def preprocess_labels(label_series, num_classes=24):
    """
    Remaps labels to skip index 9 (J) and applies one-hot encoding.

    Parameters:
    - label_series: a pandas Series or 1D array of labels (originally 0–25, with 9 missing)
    - num_classes: total number of actual classes (default 24)

    Returns:
    - One-hot encoded labels of shape (n_samples, num_classes)
    """
    labels = np.array(label_series)

    remapped_labels = np.array([l - 1 if l > 9 else l for l in labels])

    categorical_labels = to_categorical(remapped_labels, num_classes=num_classes)

    return categorical_labels


# Reverse the earlier remapping: add 1 to all labels ≥ 9
def reverse_remap(labels):
    return [l + 1 if l >= 9 else l for l in labels]



def show_predictions(x_data, y_true, y_pred, indices=None, n=6):
    if indices is None:
        indices = np.random.choice(len(x_data), n, replace=False)

    plt.figure(figsize=(12, 6))
    for i, idx in enumerate(indices):
        plt.subplot(2, n // 2, i + 1)
        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')
        plt.title(f"Pred: {y_pred[idx]}\nTrue: {y_true[idx]}")
        plt.axis('off')
    plt.tight_layout()
    plt.show()



```


```{python preprocessing}
#| code-fold: true
#| code-summary: "Show the code"



# Separate labels
y_train = preprocess_labels(train_set['label'])

y_test = preprocess_labels(test_set['label'])

# Remove labels from the pixel data
x_train = preprocess_data(train_set.drop('label', axis=1))

x_test = preprocess_data(test_set.drop('label', axis=1))

```

# Define CNN Model
- Build a Convolutional Neural Network using the Keras Functional API:
  - An input layer for images of shape (28, 28, 1).
  - **Block 1:** two Conv2D layers with 32 filters (3×3, ReLU, padding='same'), followed by a MaxPooling2D layer (2×2).
  - **Block 2:** two Conv2D layers with 64 filters (3×3, ReLU, padding='same').
  - **Block 3:** two Conv2D layers with 128 filters (3×3, ReLU, padding='same').
  - A Flatten layer to convert feature maps into a vector.
  - A Dense layer with 256 units and ReLU activation.
  - A Dropout layer with rate 0.3 to reduce overfitting.
  - A final Dense layer with 24 units and softmax activation for classification.
- Compile the model using the Adam optimizer and the categorical cross-entropy loss function.

```{python define_model}
#| code-fold: true
#| code-summary: "Show the code"
#| output: false


# Input
inputs = Input(shape=(28, 28, 1))

# Block 1
x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

# Block 2
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)

# Block 3
x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)

# Dense head
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)

# Output
outputs = Dense(24, activation='softmax')(x)

# Final model
model = Model(inputs, outputs)

# Compile
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

```

# Train the Model
- Fit the model on the training data for 5 epochs with batch size 128.
- Use the test data as validation during training.

```{python fit_model}
#| code-fold: true
#| code-summary: "Show the code"
#| output: false


history = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=10,
    batch_size=128,
    verbose = 0
)

```

# Evaluate the Model
- Evaluate the model on the test data.
- Report test accuracy.

```{python evaluation}
#| code-fold: true
#| code-summary: "Show the code"
#| output: false


loss, accuracy = model.evaluate(x_test, y_test)

print(f"Test Accuracy: {accuracy:.4f}")

```

# Visualizing Model Decisions with Grad-CAM

Convolutional Neural Networks can achieve high accuracy, but it is often unclear 
which parts of an image drive their predictions. To make the model more 
interpretable, we use visualization techniques such as **Grad-CAM**, 
**Guided Backpropagation**, and **Guided Grad-CAM**.

- **Grad-CAM (Gradient-weighted Class Activation Mapping)**  
  Highlights the image regions that had the strongest influence on the model’s 
  predicted class. It does this by examining the gradients flowing into the 
  *last convolutional layer*, which contains spatial information.

- **Guided Backpropagation**  
  Computes fine-grained pixel-level gradients that show which pixels most 
  strongly support the predicted class. The result is a high-resolution 
  saliency map.

- **Guided Grad-CAM**  
  Combines the coarse, region-level information from Grad-CAM with the 
  fine details from Guided Backpropagation.  
  The result is a high-resolution heatmap that highlights exactly which parts 
  of the image contributed to the model’s decision.

Below, we apply all three methods to a selected test image to see what the model 
focused on when predicting the hand sign.


The figure below shows four visualizations side by side:

- **Original Image** – the input image given to the model.
- **Grad-CAM** – a coarse heatmap showing which regions of the image contributed most to the model’s prediction.
- **Guided Backpropagation** – a fine-grained gradient map showing which pixels support the predicted class.
- **Guided Grad-CAM** – an overlay combining Grad-CAM with Guided Backprop to produce a high-resolution explanation.

```{python get_visualisation_metrics}
#| code-fold: true
#| code-summary: "Show the code"

# Choose which test image to analyze
idx = 3
image = x_test[idx:idx+1]  # shape (1, 28, 28, 1)


for layer in reversed(model.layers):
    if isinstance(layer, tf.keras.layers.Conv2D):
        last_conv_layer_name = layer.name
        break


# ---- 1. Predict class ----
pred_probs = model.predict(image, verbose=0)[0]
pred_index = np.argmax(pred_probs)
print("Predicted class:", pred_index)

# ---- 2. Grad-CAM ----
score = CategoricalScore(pred_index)

gradcam = Gradcam(
    model,
    model_modifier=None,
    clone=True
)

cam = gradcam(
    score,
    image,
    penultimate_layer=last_conv_layer_name
)
heatmap = cam[0]   # remove batch dimension



def model_modifier(m):
    # Replace the final softmax activation with linear
    m.layers[-1].activation = tf.keras.activations.linear


# ---- 3. Guided Backprop ----
saliency = Saliency(
    model,
    model_modifier=model_modifier,
    clone=True
)

gbp = saliency(score, image)   # guided backprop gradients, shape (1, 28, 28, N)
gbp = gbp[0]                   # remove batch dimension → shape (28, 28, N)
gbp = normalize(gbp)           # rescale 0–1



# ---- 4. Guided Grad-CAM (Grad-CAM × Guided Backprop) ----
guided_gradcam = gbp * heatmap


```


```{python plot_results}
#| code-fold: true
#| code-summary: "Show the code"

# Normalize Grad-CAM heatmap to 0–1 for display
heatmap_norm = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)

# Convert Guided Backprop to a single-channel saliency map
gbp_gray = normalize(gbp)

plt.figure(figsize=(13, 4))

# Original
plt.subplot(1, 4, 1)
plt.title("Original")
plt.imshow(image[0].reshape(28, 28), cmap='gray')
plt.axis('off')

# Grad-CAM
plt.subplot(1, 4, 2)
plt.title("Grad-CAM")
plt.imshow(heatmap_norm, cmap='jet')
plt.axis('off')

# Guided Backprop
plt.subplot(1, 4, 3)
plt.title("Guided Backprop")
plt.imshow(gbp_gray, cmap='gray')
plt.axis('off')

# Guided Grad-CAM
plt.subplot(1, 4, 4)
plt.title("Guided Grad-CAM")
plt.imshow(image[0].reshape(28, 28), cmap='gray')
plt.imshow(heatmap_norm, cmap='jet', alpha=0.45)
plt.axis('off')

plt.tight_layout()
plt.show()

```


