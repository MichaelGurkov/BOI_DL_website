---
title: "Optimization methods"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd

import numpy as np

import os


from func_package.tests import (test_forward_propagation,
test_backward_propagation, test_model)

from func_package.model import model


# Plot predictions
import matplotlib.pyplot as plt

```

# Gradient descent

```{python}

# test_forward_propagation()
# 
# test_backward_propagation()


```


```{python}

test_model()

```


```{python }

from func_package.model import model

X = np.array([[0, 0, 1, 1],
              [0, 1, 0, 1]])   # shape (2, 4)
Y = np.array([[0, 1, 1, 0]])     # shape (1, 4)

  # 2) Define a 2‑layer network: 2 inputs → 4 hidden units → 1 output
layer_dims = [2, 4, 1]

optimizer="adam",
learning_rate=0.01,
num_epochs=2000,
batch_size=None,
print_cost=True,
print_every=200,
beta1=0.9, beta2=0.999, epsilon=1e-8
"""
  Trains a L‑layer neural network.

  Arguments:
  X, Y            -- input data and labels, shapes (n_x, m), (n_y, m)
  layer_dims      -- list of layer dimensions [n_x, n_h1, ..., n_y]
  optimizer       -- "gd", "momentum", "rmsprop", or "adam"
  learning_rate   -- step size
  num_epochs      -- number of full passes over the data
  batch_size      -- size of mini‑batches; if None, uses full batch
  print_cost      -- if True, print cost every `print_every` epochs
  print_every     -- frequency (in epochs) of printing the cost
  **hyperparams   -- extra optimizer settings (beta, beta1, beta2, epsilon)

  Returns:
  parameters -- learned parameters dict (W1…WL, b1…bL)
  costs      -- list of costs printed
  """
m = X.shape[1]

if batch_size is None:
  batch_size = m

# 1) Initialize parameters & optimizer state
parameters = initialize_parameters(layer_dims)

opt_state  = initialize_optimizer_state(parameters, optimizer, **hyperparams)
costs = []

# 2) Training loop
for epoch in range(1, num_epochs+1):

  epoch_cost = 0

  minibatches = get_minibatches(X, Y, batch_size, shuffle=True)

  for X_batch, Y_batch in minibatches:
    # Forward
    weights, biases = split_parameters(parameters)
    AL, caches = forward_propagation(
    X_batch, weights, biases,
    hidden_activation="relu",
    output_activation="sigmoid"
    )

    # Compute cost & backward
    cost, _ = compute_cost(AL, Y_batch)
    grads = backward_propagation(
      AL, Y_batch, caches,
      hidden_activation="relu",
      output_activation="sigmoid"
    )


    # Update parameters
    parameters, opt_state = update_parameters(
      parameters, grads, opt_state,
      optimizer=optimizer,
      learning_rate=learning_rate,
      **hyperparams
    )
    epoch_cost += cost * (X_batch.shape[1] / m)

  # Record / print cost
  if epoch % print_every == 0:
    costs.append(epoch_cost)
  if print_cost:
    print(f"Cost after epoch {epoch}: {epoch_cost:.6f}")


```


# Stochastic gradient descent

# Momentum

# RMSProp

# Adam