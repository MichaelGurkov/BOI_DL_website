---
title: "Single layer Neural Network for Binary Classification"
---

Implement a simple neural network from scratch and compare its performance to logistic regression on a 2D dataset.

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd

import numpy as np

import os


```


# Define the Algorithm
- Specify model structure: input layer → hidden layer → output.
- Initialize parameters `W1`, `b1`, `W2`, `b2`.
- Define forward propagation using `tanh` and `sigmoid` activations.
- Define backward propagation to compute gradients.
- Implement parameter updates using gradient descent.

## Auxiliary functions

### Implement Training Functions

- Define helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.


```{python activation_function}
#| code-fold: true
#| code-summary: "Show the activation function code"

import numpy as np


def activate(Z, activation_function="tanh"):
    """
    Apply an activation function elementwise.
    """
    if activation_function == "tanh":
        return np.tanh(Z)  # squashes values to [-1, 1]
    elif activation_function == "sigmoid":
        return 1.0 / (1.0 + np.exp(-Z))  # squashes values to [0, 1]
    else:
        raise ValueError("activation_function must be 'tanh' or 'sigmoid'.")


```


```{python initialize_parameters}
#| code-fold: true
#| code-summary: "Show the parameters initialization code"

import numpy as np


def initialize_parameters(X, num_hidden_layer_neurons, scale_const=0.01, seed=1):
    """
    Initialize weights and biases for a single hidden-layer network.
    """
    np.random.seed(seed)

    n_features = X.shape[1]  # number of input features

    # Small random weights help avoid saturation of activations at start
    W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const
    b1 = np.zeros((num_hidden_layer_neurons, 1))
    W2 = np.random.randn(1, num_hidden_layer_neurons) * scale_const
    b2 = np.zeros((1, 1))

    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}

```




![Forward and backward propagation in a single-hidden-layer neural network. The forward pass takes inputs $X$ through weights $W^{[1]}, W^{[2]}$ and biases $b^{[1]}, b^{[2]}$ to produce pre-activations $Z^{[1]}, Z^{[2]}$, activations $A^{[1]}, A^{[2]}$, and final output $y$. The backward pass computes gradients of activations, pre-activations, weights, and biases $(dA, dZ, dW, db)$ from the output layer back to the input layer for parameter updates.](images/prop.png){fig-align="center" width="70%"}




```{python forward_propagation}
#| code-fold: true
#| code-summary: "Show the forward propagation code"

import numpy as np


def forward_propagation(parameters, X_adj):
    """
    Perform one forward pass.
    """
    W1, b1, W2, b2 = parameters["W1"], parameters["b1"], parameters["W2"], parameters["b2"]

    Z1 = np.dot(W1, X_adj) + b1  # linear transform: hidden layer
    A1 = activate(Z1, "tanh")    # non-linear activation for hidden layer
    Z2 = np.dot(W2, A1) + b2     # linear transform: output layer
    A2 = activate(Z2, "sigmoid") # probability output for binary classification

    return {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}


```



```{python backward_propagation}
#| code-fold: true
#| code-summary: "Show the backward propagation code"

import numpy as np


def backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):
    """
    Compute gradients for parameters using backpropagation.
    """
    N = X_adj.shape[1]  # number of samples

    W2 = parameters["W2"]
    A1, A2 = forward_propagation_values["A1"], forward_propagation_values["A2"]

    dZ2 = A2 - Y_adj                      # derivative of loss w.r.t. Z2 (sigmoid+BCE Loss)
    dW2 = np.dot(dZ2, A1.T) / N           # gradient for W2
    db2 = np.sum(dZ2, axis=1, keepdims=True) / N  # gradient for b2

    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2) # backprop through tanh: derivative is 1 - A1^2
    dW1 = np.dot(dZ1, X_adj.T) / N        # gradient for W1
    db1 = np.sum(dZ1, axis=1, keepdims=True) / N  # gradient for b1

    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}


```



```{python update_parameters}
#| code-fold: true
#| code-summary: "Show the parameters update code"

import numpy as np

def update_parameters(parameters, grads, learning_rate=0.01):
    """
    Update parameters using gradient descent.
    """
    # subtract learning_rate * gradient for each parameter
    W1 = parameters["W1"] - learning_rate * grads["dW1"]
    b1 = parameters["b1"] - learning_rate * grads["db1"]
    W2 = parameters["W2"] - learning_rate * grads["dW2"]
    b2 = parameters["b2"] - learning_rate * grads["db2"]

    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}


```



- Create a wrapper function `train_neural_network` to run the training loop.


```{python train_neural_network}
#| code-fold: true
#| code-summary: "Show the neural network training code"

def train_neural_network(X, Y, num_iterations, num_hidden_layer_neurons=4):
    """
    Trains a simple 1-hidden-layer neural network using gradient descent.
    """
    # Transpose X so columns are examples, reshape Y to row vector
    X_adj = X.T.copy()                         
    Y_adj = Y.values.reshape(1, -1).copy()     

    # Initialize weights and biases
    parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)

    for iteration in range(num_iterations):
        # Forward pass
        forward_values = forward_propagation(parameters, X_adj.copy())

        # Backward pass
        grads = backward_propagation(parameters, forward_values, X_adj.copy(), Y_adj.copy())

        # Parameter update
        parameters = update_parameters(parameters, grads)

    return parameters



```





### Implement Prediction Function
- Create a `predict` function that runs forward propagation and thresholds outputs.

```{python auxiliary_functions}
#| code-fold: true
#| code-summary: "Show the code"

def predict(nn_parameters, X, threshold=0.5):
    """
    Generates binary predictions from a trained neural network.
    """
    # Transpose X so columns are examples
    X_adj = X.T.copy()

    # Forward pass to get output layer activations
    forward_values = forward_propagation(nn_parameters, X_adj.copy())

    # Output probabilities from sigmoid
    y_pred = forward_values["A2"]

    # Apply threshold to get class labels {0,1}
    y_pred = y_pred > threshold

    return y_pred.astype(int).ravel()  # flatten to 1D array


```


# Application on planar data

#### Load and Prepare Data
- Load the `planar_data.csv` dataset.
- Separate features `X` and target `Y`.

```{python load_data}
#| code-fold: true
#| code-summary: "Show the code"


raw_df = pd.read_csv(os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"), "data\\planar_data.csv"))

features = ["x_coord","y_coord"]

target = "label"

X = raw_df[features].copy()

Y = raw_df[target].copy()


```


#### Train the Neural Network
- Train the network with `num_iterations=10000` and `num_hidden_layer_neurons=4`.

```{python fit_model}

nn_parameters = train_neural_network(X,Y, num_iterations=10000, num_hidden_layer_neurons=4)

```

#### Evaluate the Model
- Generate predictions on the training set.
- Compute and print classification accuracy.
```{python predictions}

from sklearn.metrics import accuracy_score

y_pred = predict(nn_parameters, X.copy())

nn_score = accuracy_score(Y, y_pred)

print(f"Neural network score is {np.round(nn_score,4)}")

```


### Compare with Logistic Regression
- Train a logistic regression model on the same data.
- Generate decision boundary predictions for both models.

```{python log_reg}
#| code-fold: true
#| code-summary: "Show the code"

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()

log_reg.fit(X,Y)

```


### Visualize Results
- Use helper functions to:
  - Plot the actual data.
  - Plot the decision boundary of logistic regression.
  - Plot the decision boundary of the neural network.
   
```{python auxiliary_plotting_functions}
#| code-fold: true
#| code-summary: "Show the code"

import numpy as np

import matplotlib.pyplot as plt

def generate_grid(x_min, x_max, y_min, y_max, step_size=0.02):
    """
    Generates a grid of points covering the given range with the specified step size.
    
    Parameters:
    - x_min, x_max: float, range for x-axis.
    - y_min, y_max: float, range for y-axis.
    - step_size: float, resolution of the grid.
    
    Returns:
    - XX, YY: Meshgrid arrays for plotting.
    - grid_points: Flattened array of grid coordinates.
    """
    xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size),
                         np.arange(y_min, y_max, step_size))
    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Flatten the grid
    
    return xx, yy, grid_points

def plot_decision_boundary(xx, yy, pred_grid, X, y, title, cmap=plt.cm.RdBu):
    """
    Plots the decision boundary for a given prediction grid.
    
    Parameters:
    - xx, yy: Meshgrid arrays for plotting.
    - pred_grid: Prediction values reshaped to match xx and yy.
    - X: Original dataset (features).
    - y: Labels for the dataset.
    - title: Title of the plot.
    - cmap: Colormap for visualization.
    """
    plt.figure(figsize=(8, 6))
    
    # Plot the decision boundary
    plt.contourf(xx, yy, pred_grid, alpha=0.6, cmap=cmap)
    
    # Scatter plot of actual data points
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", cmap=cmap, s=40)
    
    plt.title(title)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()


```


```{python preparing grid}
#| code-fold: true
#| code-summary: "Show the code"

xx, yy, grid_points  = generate_grid(x_min = X["x_coord"].min(), x_max = X["x_coord"].max(),
y_min = X["y_coord"].min(), y_max = X["y_coord"].max())


nn_pred_grid = predict(nn_parameters, grid_points)

nn_pred_grid = np.array(nn_pred_grid).reshape(xx.shape)

log_reg_pred_grid = log_reg.predict(pd.DataFrame(grid_points, columns=X.columns))

log_reg_pred_grid = np.array(log_reg_pred_grid).reshape(xx.shape)

```

This is how actual data looks like

```{python plot_actual_data}
#| code-fold: true
#| code-summary: "Show the code"

plt.clf()

plt.scatter(X["x_coord"], X["y_coord"], c=Y, s=40, cmap=plt.cm.Spectral);

plt.title("Actual data")

plt.show()

```

This is a classification by logistic regression



```{python plot_log_reg_predictions}
#| code-fold: true
#| code-summary: "Show the code"

plot_decision_boundary(xx, yy, log_reg_pred_grid, X.to_numpy(),
Y.to_numpy(),title="Logistic Regression Decision Boundary")


```


And this is a classification by neural network

```{python plot_nn_predictions}
#| code-fold: true
#| code-summary: "Show the code"

plot_decision_boundary(xx, yy, nn_pred_grid, X.to_numpy(),
Y.to_numpy(),title="Neural Network Decision Boundary")


```



# Appendix


## Derivation: Sigmoid + Binary Cross-Entropy

We can show that using a sigmoid activation in the output layer with binary cross-entropy (BCE) loss leads to a very simple gradient formula.

**Goal**
$$
\frac{\partial L}{\partial z} = a - y
$$

**Setup** For a single example with label $y \in \{0,1\}$:
$$
z = W^{[2]} a^{[1]} + b^{[2]}, \qquad a = \sigma(z) = \frac{1}{1+e^{-z}}
$$
The BCE loss for this example is:
$$
\ell(a,y) = -\big[y \log a + (1-y)\log(1-a)\big]
$$
For a batch of $N$ examples:
$$
L = \frac{1}{N}\sum_{i=1}^N \ell\!\big(a^{(i)}, y^{(i)}\big)
$$

**Step 1 (loss wrt $a$)**
$$
\frac{\partial \ell}{\partial a}
= -\!\left(\frac{y}{a} - \frac{1-y}{1-a}\right)
= \frac{a - y}{a(1-a)}
$$

**Step 2 (sigmoid derivative)**
$$
\frac{\partial a}{\partial z} = a(1-a)
$$

**Step 3 (chain rule)**
$$
\frac{\partial \ell}{\partial z}
= \frac{\partial \ell}{\partial a}\cdot\frac{\partial a}{\partial z}
= \frac{a - y}{a(1-a)} \cdot a(1-a)
= a - y
$$

**Batch form**
$$
\frac{\partial L}{\partial Z} = \frac{1}{N}(A - Y)
$$

**Code correspondence**


```{python}
#| eval: false

dZ2 = A2 - Y_adj

dW2 = np.dot(dZ2, A1.T) / N

db2 = np.sum(dZ2, axis=1, keepdims=True) / N

```


## Neural Network Architecture Playbook

**Definitions**
- **Depth**: The number of hidden layers stacked between input and output.  
- **Width**: The number of neurons in each hidden layer.

*(Let `d` be the number of input features.)*

### Step 1 — Build a small, trainable baseline
- Start with **1–3 hidden layers**.  
- Set hidden layer sizes as multiples of `d` (e.g., `d`, `2d`, `4d`).  
- Add dropout (0–0.5) and weight decay (AdamW) for regularization.  
- Always keep a validation split and monitor training/validation curves.

### Step 2 — Diagnose with learning curves
- **Underfitting (high train & val error):** widen hidden layers first, then add more layers if needed.  
- **Overfitting (low train, high val):** add data/augmentation; increase dropout or weight decay; use early stopping; simplify by reducing neurons or layers.  
- **Optimization issues (unstable or diverging):** lower the learning rate; adjust batch size; use better initialization; consider normalization.

### Step 3 — Grow capacity deliberately
- Increase width modestly before adding depth.  
- If you deepen the network, add residual connections or normalization to stabilize training.  
- Increase epochs only once the model trains stably.

### Step 4 — Tune by systematic search
- Use **Random Search** or **Hyperband** over:
  - **Layers:** {1, 2, 3, 4, 6}  
  - **Widths:** {`d`, `2d`, `4d`, `8d`}  
  - **Dropout:** [0, 0.5]  
  - **Weight decay:** small grid (e.g., 1e-6 … 1e-3)  




