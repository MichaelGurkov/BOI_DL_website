---
title: "Logistic regression as Neural Network"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd

import numpy as np

import os

from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split

```

# **The Algorithm**

1. **Define the Model Structure**  
   - Weights (`w`) and bias (`b`).

2. **Initialize Model Parameters**  
   - Set initial values for `w` and `b`.

3. **Training Loop (Gradient Descent Iteration)**  
   - **Compute gradients (backward propagation)**.  
   - Update parameters (gradient descent).  

4. **Return Trained Parameters**  
   - Output optimized `w` and `b`.  



# Functions definition

#### Auxiliary functions

```{python auxilary_functions}
#| code-fold: true
#| code-summary: "Show the code"

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def initialize_weights(dim):
    w = np.zeros(dim).reshape(dim,1)
    b = 0.0
    return w, b

def get_predictions(model, X):
    w = model[0]
    b = model[1]
    preds = sigmoid(np.dot(w.T, X) + b)
    return preds


def calculate_cost(w,b,X,y):

  A = sigmoid(np.dot(w.T,X) + b)

  cost = -(np.dot(y,np.log(A).T) + np.dot((1-y),np.log(1-A).T))

  return cost


def improve_weights(w, b,X,y,learning_rate = 0.09):

  grads = propagate(w,b,X,y)

  dw = grads["dw"]

  db = grads["db"]

  w = w - learning_rate * dw

  b = b - learning_rate * db

  params = [w,b]

  return params


```

#### Backward propagation (gradient calculation)

```{python propagation_function}
#| code-fold: true
#| code-summary: "Show the code"

def propagate(w,b,X,y):

  m = X.shape[0]

  A = sigmoid(np.dot(w.T,X) + b)

  dz = A - y

  dw = np.dot(X, dz.T) / m

  db = np.sum(dz) / m

  grads = {"dw": dw,
            "db": db}

  return grads


```


#### Neural network implementation

```{python neural_network_function}
#| code-fold: true
#| code-summary: "Show the code"

def neural_network_weights(X,y, num_rounds = 100, learning_rate = 0.09):

  w,b = initialize_weights(X.shape[0])

  for i in range(num_rounds):
    w,b = improve_weights(w,b,X,y,learning_rate = learning_rate)
    
  return w,b

```




# Application on loan approval data

#### Import and preprocess data
```{python load_data}
#| code-fold: true
#| code-summary: "Show the code"


raw_df = pd.read_csv(os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"), "data\\loan_data.csv"))

features = ["person_age","person_income","loan_amnt"]

target = "loan_status"

X_mat = raw_df[features].copy()

y_vec = raw_df[target].copy()

X_train, X_test, y_train, y_test = train_test_split(X_mat, y_vec, test_size=0.1)


```

::: {.callout-note}
Neural networks often represent the feature matrix in a **features × observations** \($m \times n$\) format instead of the traditional **observations × features** \($n \times m$\) structure. This transposed format optimizes matrix multiplications, aligns with batch-wise computation, and improves memory efficiency in deep learning frameworks. By structuring the data this way, we ensure faster execution, better GPU utilization, and smoother backpropagation during training.
:::



```{python scale_and_transform_data}
#| code-fold: true
#| code-summary: "Show the code"


X_train_scaled = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)

X_train_nn_scaled = X_train_scaled.T.values.copy()

X_test_scaled = (X_test - X_train.mean(axis=0)) / X_train.std(axis=0) 

X_test_nn_scaled = X_test_scaled.T.values.copy()

y_nn = y_train.T.values.copy()

y_nn_test = y_test.T.values.copy()

```


#### Training neural network

```{python fit_model}

nn_params = neural_network_weights(X = X_train_nn_scaled,y = y_nn, num_rounds=100, learning_rate=0.001)


```

#### Predictions on test set

```{python predictions}

from sklearn.metrics import accuracy_score

nn_predictions = get_predictions(nn_params, X_test_nn_scaled)

nn_score = accuracy_score(y_nn_test, (nn_predictions > 0.5).T)

print(f"Neural network score is {np.round(nn_score,4)}")

```


# Appendix


## **Algorithm: Backward Propagation in Logistic Regression**

### **1. Define the Model Structure**
- The model consists of:
  - **Weights**: A vector of parameters corresponding to input features, denoted as \($w$\).
  - **Bias**: A scalar parameter, denoted as \( $b$ \).

### **2. Initialize Model Parameters**
- Call `initialize_weights(n_{\text{features}})`, which initializes:
  - \( $w$ \) (weights) to small random values or zeros.
  - \( $b$ \) (bias) to zero.

### **3. Training Loop (Gradient Descent Iteration)**
- Repeat for a given number of iterations \($\text{num\_rounds}$\):

  #### **a. Compute Gradients (Backward Propagation)**
  - Use `propagate(w, b, X, y)`:
    - Compute **predicted probabilities** using the sigmoid function:
      $$
      A = \sigma(w^T X + b)
      $$
      where:
      $$
      \sigma(z) = \frac{1}{1 + e^{-z}}
      $$
    - Compute **error** (difference between predicted and actual values):
      $$
      dz = A - y
      $$
    - Compute **gradients** (partial derivatives):
      - **Gradient of weights**:
        $$
        dw = \frac{1}{m} X dz^T
        $$
      - **Gradient of bias**:
        $$
        db = \frac{1}{m} \sum dz
        $$
    - Return \($dw$\) and \($db$\).

  #### **b. Update Parameters (Gradient Descent)**
  - Call `improve_weights(w, b, X, y, $\text{learning_rate})$`, which:
    - Retrieves gradients from `propagate(w, b, X, y)`.
    - Updates parameters using gradient descent:
      - **Update weights**:
        $w = w - \text{learning rate} \times dw$
        
      - **Update bias**:
        $b = b - \text{learning rate} \times db$

### **4. Return Trained Parameters**
- After completing the training loop, return \($w$\) and \($b$\), the optimized parameters.
