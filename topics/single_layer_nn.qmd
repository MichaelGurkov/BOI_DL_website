---
title: "Single layer Neural Network"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd

import numpy as np

import os


```

# **The Algorithm**

1. **Define the Model Structure**  
   - Weights (`w`) and bias (`b`).

2. **Initialize Model Parameters**  
   - Set initial values for `w` and `b`.

3. **Training Loop (Gradient Descent Iteration)**  
   - **Compute gradients (backward propagation)**.  
   - Update parameters (gradient descent).  

4. **Return Trained Parameters**  
   - Output optimized `w` and `b`.  



# Functions definition

#### Auxiliary functions

```{python auxilary_functions}
#| code-fold: true
#| code-summary: "Show the code"

def initialize_parameters(X,y, num_hidden_layer_neurons = 4):

  scaling_constant = 0.01
 
  num_features = X.shape[1]

  num_output_layers = y.shape[0]

  weights_hidden = np.random.randn(num_hidden_layer_neurons,num_features) * scaling_constant

  weights_output = np.random.randn(num_hidden_layer_neurons, 1)* scaling_constant

  bias_hidden = np.zeros((num_hidden_layer_neurons,1))

  bias_output = np.zeros((1, 1))

  parameters = {"weights_hidden": weights_hidden,"bias_hidden":bias_hidden,
                "weights_output": weights_output,"bias_output":bias_output}

  return parameters


def forward_propagation(X, parameters):
  
  num_samples = X.shape[0]

  num_features = X.shape[1]
  
  num_features = num_features

  weights_hidden = parameters["weights_hidden"]
  weights_output = parameters["weights_output"]

  bias_hidden = parameters["bias_hidden"]
  bias_output = parameters["bias_output"]

  values_hidden = np.dot(weights_hidden, X.T) + bias_hidden # dims: (num_of_hidden_neurons,num_samples)

  values_hidden_active = np.tanh(values_hidden) # dims: (num_of_hidden_neurons,num_samples)

  values_output = np.dot(weights_output.T, values_hidden_active) + bias_output # dims: (num_samples)

  values_output_active = np.tanh(values_output)

  predictions = values_output_active
  
  current_network_values = {"values_hidden": values_hidden,
                            "values_hidden_active": values_hidden_active,
                            "values_output": values_output,
                            "values_output_active": values_output_active}
  
  return predictions, current_network_values


def backward_propagation(parameters, current_network_values, X, y):
  weights_hidden = parameters["weights_hidden"]
  weights_output = parameters["weights_output"]

  bias_hidden = parameters["bias_hidden"]
  bias_output = parameters["bias_output"]

  values_hidden_active = current_network_values["values_hidden_active"]

  values_output = current_network_values["values_output"]

  values_output_active = current_network_values["values_output_active"]

  num_samples = X.shape[0]

  #####calculate gradients
  
  y = y.values.reshape(1,-1)

  d_values_output = values_output_active - y # dims: (num_samples,1)
  
  d_weights_output = (1 / num_samples) * np.dot(values_hidden_active, d_values_output.T) # dims: (num_of_hidden_neurons, 1)

  d_bias_output = (1 / num_samples) * np.sum(d_values_output,axis = 1, keepdims=True)

 
  d_values_hidden = np.dot(weights_output, d_values_output) *(1 - np.power(values_hidden_active, 2))

  d_weights_hidden = (1 / num_samples) * np.dot(d_values_hidden, X)

  d_bias_hidden = (1 / num_samples) * np.sum(d_values_hidden,axis = 1, keepdims=True) 

  
  grads = {"d_weights_hidden": d_weights_hidden, "d_bias_hidden": d_bias_hidden,
           "d_weights_output": d_weights_output, "d_bias_output": d_bias_output}
  
  return grads
  

def update_parameters(parameters, grads, learning_rate = 1.2):

  weights_hidden = parameters["weights_hidden"]
  weights_output = parameters["weights_output"]

  bias_hidden = parameters["bias_hidden"]
  bias_output = parameters["bias_output"]

  d_weights_hidden = grads["d_weights_hidden"]
  d_bias_hidden = grads["d_bias_hidden"]

  d_weights_output = grads["d_weights_output"]
  d_bias_output = grads["d_bias_output"]

  weights_hidden = weights_hidden - learning_rate * d_weights_hidden
  bias_hidden = bias_hidden - learning_rate * d_bias_hidden

  weights_output = weights_output - learning_rate * d_weights_output
  bias_output = bias_output - learning_rate * d_bias_output

  parameters = {"weights_hidden": weights_hidden,"bias_hidden":bias_hidden,
                "weights_output": weights_output,"bias_output":bias_output}
  
  return parameters


def predict(parameters, X, threshold = 0.5):
  
  predictions, current_network_values = forward_propagation(X, parameters)
  
  predictions = pd.Series(predictions.ravel())

  predictions = predictions.apply(lambda x: 1 if x > threshold else 0)
  
  return predictions

```



#### Neural network implementation

```{python neural_network_function}
#| code-fold: true
#| code-summary: "Show the code"

def train_neural_network(X,y,num_iterations = 100):
  # Initialize the model's parameters
  # Loop:
  #  - Implement forward propagation to get the predictions
  #  - Implement backward propagation to get the gradients
  #  - Update parameters (gradient descent)

  parameters = initialize_parameters(X,y)

  for iteration in range(num_iterations):

    predictions, current_network_values = forward_propagation(X.copy(), parameters)

    grads = backward_propagation(parameters, current_network_values, X.copy(), y.copy())

    parameters = update_parameters(parameters, grads)

  return parameters

```




# Application on planar data

#### Import and preprocess data
```{python load_data}
#| code-fold: true
#| code-summary: "Show the code"


raw_df = pd.read_csv(os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"), "data\\planar_data.csv"))

features = ["x_coord","y_coord"]

target = "label"

X = raw_df[features].copy()

Y = raw_df[target].copy()


```


#### Training neural network

```{python fit_model}

nn_params = train_neural_network(X = X,y = Y,num_iterations = 10000)


```

#### Predictions on test set

```{python predictions}

from sklearn.metrics import accuracy_score

nn_predictions = predict(nn_params, X)

nn_score = accuracy_score(Y, nn_predictions)

print(f"Neural network score is {np.round(nn_score,4)}")

```


### Comparison with Logistic regression

```{python log_reg}
#| code-fold: true
#| code-summary: "Show the code"

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()

log_reg.fit(X,Y)

```


#### Auxiliary plotting functions

```{python auxiliary_plotting_functions}
#| code-fold: true
#| code-summary: "Show the code"

import numpy as np

import matplotlib.pyplot as plt

def generate_grid(x_min, x_max, y_min, y_max, step_size=0.02):
    """
    Generates a grid of points covering the given range with the specified step size.
    
    Parameters:
    - x_min, x_max: float, range for x-axis.
    - y_min, y_max: float, range for y-axis.
    - step_size: float, resolution of the grid.
    
    Returns:
    - XX, YY: Meshgrid arrays for plotting.
    - grid_points: Flattened array of grid coordinates.
    """
    xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size),
                         np.arange(y_min, y_max, step_size))
    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Flatten the grid
    
    return xx, yy, grid_points

def plot_decision_boundary(xx, yy, pred_grid, X, y, title, cmap=plt.cm.RdBu):
    """
    Plots the decision boundary for a given prediction grid.
    
    Parameters:
    - xx, yy: Meshgrid arrays for plotting.
    - pred_grid: Prediction values reshaped to match xx and yy.
    - X: Original dataset (features).
    - y: Labels for the dataset.
    - title: Title of the plot.
    - cmap: Colormap for visualization.
    """
    plt.figure(figsize=(8, 6))
    
    # Plot the decision boundary
    plt.contourf(xx, yy, pred_grid, alpha=0.6, cmap=cmap)
    
    # Scatter plot of actual data points
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", cmap=cmap, s=40)
    
    plt.title(title)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()


```


```{python preparing grid}
#| code-fold: true
#| code-summary: "Show the code"

xx, yy, grid_points  = generate_grid(x_min = X["x_coord"].min(), x_max = X["x_coord"].max(),
y_min = X["y_coord"].min(), y_max = X["y_coord"].max())


nn_pred_grid = predict(nn_params, grid_points)

nn_pred_grid = np.array(nn_pred_grid).reshape(xx.shape)

log_reg_pred_grid = log_reg.predict(pd.DataFrame(grid_points, columns=X.columns))

log_reg_pred_grid = np.array(log_reg_pred_grid).reshape(xx.shape)

```

This is how actual data looks like

```{python plot_actual_data}
#| code-fold: true
#| code-summary: "Show the code"

plt.clf()

plt.scatter(X["x_coord"], X["y_coord"], c=Y, s=40, cmap=plt.cm.Spectral);

plt.title("Actual data")

plt.show()

```

This is a classification by logistic regression



```{python plot_log_reg_predictions}
#| code-fold: true
#| code-summary: "Show the code"

plot_decision_boundary(xx, yy, log_reg_pred_grid, X.to_numpy(),
Y.to_numpy(),title="Logistic Regression Decision Boundary")


```


And this is a classification by neural network

```{python plot_nn_predictions}
#| code-fold: true
#| code-summary: "Show the code"

plot_decision_boundary(xx, yy, nn_pred_grid, X.to_numpy(),
Y.to_numpy(),title="Neural Network Decision Boundary")


```



