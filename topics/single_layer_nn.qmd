---
title: "Single layer Neural Network"
---

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd

import numpy as np

import os


```

# **The Algorithm**

1. **Define the Model Structure**  
   - Weights (`w`) and bias (`b`).

2. **Initialize Model Parameters**  
   - Set initial values for `w` and `b`.

3. **Training Loop (Gradient Descent Iteration)**  
   - **Compute gradients (backward propagation)**.  
   - Update parameters (gradient descent).  

4. **Return Trained Parameters**  
   - Output optimized `w` and `b`.  



# Functions definition

## Auxiliary functions

### NN training functions

```{python nn_training_functions}
#| code-fold: true
#| code-summary: "Show the code"

def activate(Z, activation_function = "tanh"):
  
  if activation_function == "tanh":
    
    A = np.tanh(Z)
  
  elif activation_function == "sigmoid":
    
    A = 1 / (1 + np.exp(-Z))
    
  return(A)


def initialize_parameters(X_adj,num_hidden_layer_neurons, scale_const = 0.01):
  np.random.seed(1)

  n_features = X_adj.shape[1]  # Number of input features

  W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const

  b1 = np.zeros((num_hidden_layer_neurons,1))

  W2 = np.random.randn(1,num_hidden_layer_neurons) * scale_const

  b2 = np.zeros((1,1))

  parameters = {"W1": W1, "b1": b1, "W2":W2, "b2":b2}

  return parameters

def forward_propagation(parameters,X_adj):
  
  W1 = parameters["W1"]
  
  b1 = parameters["b1"]

  W2 = parameters["W2"]

  b2 = parameters["b2"]

  Z1 = np.dot(W1,X_adj) + b1

  A1 = activate(Z1)

  Z2 = np.dot(W2,A1) + b2

  A2 = activate(Z2,activation_function="sigmoid")

  forward_propagation_values = {"Z1":Z1, "A1":A1, "Z2":Z2, "A2":A2}

  return forward_propagation_values

def backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):

  p = X_adj.shape[1]

  W1 = parameters["W1"]

  W2 = parameters["W2"]

  A1 = forward_propagation_values["A1"]

  A2 = forward_propagation_values["A2"]

  dZ2 = A2 - Y_adj

  dW2 = np.dot(dZ2, A1.T) / p

  db2 = np.sum(dZ2, axis=1, keepdims=True) / p

  dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))

  dW1 = np.dot(dZ1,X_adj.T) / p

  db1 = np.sum(dZ1, axis=1, keepdims=True) / p

  grads = {"dW1":dW1, "db1":db1, "dW2":dW2, "db2":db2}

  return grads


def update_parameters(parameters, grads, learning_rate=0.01):
  
  W1 = parameters["W1"]

  b1 = parameters["b1"]

  W2 = parameters["W2"]

  b2 = parameters["b2"]

  
  dW1 = grads["dW1"]

  db1 = grads["db1"]

  dW2 = grads["dW2"]

  db2 = grads["db2"]

  
  W1 = W1 - learning_rate * dW1

  b1 = b1 - learning_rate * db1

  W2 = W2 - learning_rate * dW2

  b2 = b2 - learning_rate * db2

  
  parameters = {"W1": W1, "b1": b1, "W2":W2, "b2":b2}

  return parameters

```



#### Neural network implementation

```{python neural_network_function}
#| code-fold: true
#| code-summary: "Show the code"

def train_neural_network(X,Y, num_iterations,num_hidden_layer_neurons = 4):
    
    # Initialize the model's parameters
    # Loop:
    #  - Implement forward propagation to get the predictions
    #  - Implement backward propagation to get the gradients
    #  - Update parameters (gradient descent)


  X_adj = X.T.copy()

  Y_adj = Y.values.reshape(1,-1).copy()

  parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)

  for interation in range(num_iterations):
    
    forward_propagation_values = forward_propagation(parameters,X_adj.copy())

    grads = backward_propagation(parameters, forward_propagation_values, X_adj.copy(), Y_adj.copy())

    parameters = update_parameters(parameters, grads)

  return parameters

```





### Auxilary functions

```{python auxiliary_functions}
#| code-fold: true
#| code-summary: "Show the code"

def predict(nn_parameters, X, threshold = 0.5):

  X_adj = X.T.copy()

  forward_propagation_values = forward_propagation(nn_parameters, X_adj.copy())

  y_pred = forward_propagation_values["A2"]

  y_pred = y_pred > threshold

  return y_pred.astype(int).ravel()

```


# Application on planar data

#### Import and preprocess data
```{python load_data}
#| code-fold: true
#| code-summary: "Show the code"


raw_df = pd.read_csv(os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"), "data\\planar_data.csv"))

features = ["x_coord","y_coord"]

target = "label"

X = raw_df[features].copy()

Y = raw_df[target].copy()


```


#### Training neural network

```{python fit_model}

nn_parameters = train_neural_network(X,Y, num_iterations=10000, num_hidden_layer_neurons=4)

```

#### Predictions on test set

```{python predictions}

from sklearn.metrics import accuracy_score

y_pred = predict(nn_parameters, X.copy())

nn_score = accuracy_score(Y, y_pred)

print(f"Neural network score is {np.round(nn_score,4)}")

```


### Comparison with Logistic regression

```{python log_reg}
#| code-fold: true
#| code-summary: "Show the code"

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()

log_reg.fit(X,Y)

```


#### Auxiliary plotting functions

```{python auxiliary_plotting_functions}
#| code-fold: true
#| code-summary: "Show the code"

import numpy as np

import matplotlib.pyplot as plt

def generate_grid(x_min, x_max, y_min, y_max, step_size=0.02):
    """
    Generates a grid of points covering the given range with the specified step size.
    
    Parameters:
    - x_min, x_max: float, range for x-axis.
    - y_min, y_max: float, range for y-axis.
    - step_size: float, resolution of the grid.
    
    Returns:
    - XX, YY: Meshgrid arrays for plotting.
    - grid_points: Flattened array of grid coordinates.
    """
    xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size),
                         np.arange(y_min, y_max, step_size))
    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Flatten the grid
    
    return xx, yy, grid_points

def plot_decision_boundary(xx, yy, pred_grid, X, y, title, cmap=plt.cm.RdBu):
    """
    Plots the decision boundary for a given prediction grid.
    
    Parameters:
    - xx, yy: Meshgrid arrays for plotting.
    - pred_grid: Prediction values reshaped to match xx and yy.
    - X: Original dataset (features).
    - y: Labels for the dataset.
    - title: Title of the plot.
    - cmap: Colormap for visualization.
    """
    plt.figure(figsize=(8, 6))
    
    # Plot the decision boundary
    plt.contourf(xx, yy, pred_grid, alpha=0.6, cmap=cmap)
    
    # Scatter plot of actual data points
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor="k", cmap=cmap, s=40)
    
    plt.title(title)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()


```


```{python preparing grid}
#| code-fold: true
#| code-summary: "Show the code"

xx, yy, grid_points  = generate_grid(x_min = X["x_coord"].min(), x_max = X["x_coord"].max(),
y_min = X["y_coord"].min(), y_max = X["y_coord"].max())


nn_pred_grid = predict(nn_parameters, grid_points)

nn_pred_grid = np.array(nn_pred_grid).reshape(xx.shape)

log_reg_pred_grid = log_reg.predict(pd.DataFrame(grid_points, columns=X.columns))

log_reg_pred_grid = np.array(log_reg_pred_grid).reshape(xx.shape)

```

This is how actual data looks like

```{python plot_actual_data}
#| code-fold: true
#| code-summary: "Show the code"

plt.clf()

plt.scatter(X["x_coord"], X["y_coord"], c=Y, s=40, cmap=plt.cm.Spectral);

plt.title("Actual data")

plt.show()

```

This is a classification by logistic regression



```{python plot_log_reg_predictions}
#| code-fold: true
#| code-summary: "Show the code"

plot_decision_boundary(xx, yy, log_reg_pred_grid, X.to_numpy(),
Y.to_numpy(),title="Logistic Regression Decision Boundary")


```


And this is a classification by neural network

```{python plot_nn_predictions}
#| code-fold: true
#| code-summary: "Show the code"

plot_decision_boundary(xx, yy, nn_pred_grid, X.to_numpy(),
Y.to_numpy(),title="Neural Network Decision Boundary")


```



