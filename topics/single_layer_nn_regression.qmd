---
title: "Single layer Neural Network for Regression"
---

Implement a simple neural network from scratch and compare its performance to linear regression on a 2D dataset.

```{r set_up_python, echo=FALSE}
#|echo: FALSE

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = "C:\\Users\\internet\\AppData\\Local\\Programs\\Python\\Python311\\python.exe"

} else {
  
  python_path = "C:\\Users\\Home\\AppData\\Local\\Programs\\Python\\Python311\\python.exe"
}

reticulate::use_python(python_path, required = TRUE)
```

```{python import_libraries}
#| code-fold: true
#| code-summary: "Show the code"


import pandas as pd

import numpy as np

import os


```


# Define the Algorithm
- Specify model structure: input layer → hidden layer → output.
- Initialize parameters `W1`, `b1`, `W2`, `b2`.
- Define forward propagation using `tanh` in the hidden layer and **linear** activation in the output (`A2 = Z2`).
- Define backward propagation for **mean squared error (MSE)** loss.
- Implement parameter updates using gradient descent.


## Auxiliary functions

### Implement Training Functions

- Define helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.


```{python activation_function}
#| code-fold: true
#| code-summary: "Show the activation function code"

import numpy as np


def activate(Z, activation_function="tanh"):
    """
    Apply an activation function elementwise.
    """
    if activation_function == "tanh":
        return np.tanh(Z)  # squashes values to [-1, 1]
    elif activation_function == "sigmoid":
        return 1.0 / (1.0 + np.exp(-Z))  # squashes values to [0, 1]
    else:
        raise ValueError("activation_function must be 'tanh' or 'sigmoid'.")


```


```{python initialize_parameters}
#| code-fold: true
#| code-summary: "Show the parameters initialization code"

import numpy as np


def initialize_parameters(X, num_hidden_layer_neurons, scale_const=0.01, seed=1):
    """
    Initialize weights and biases for a single hidden-layer network.
    """
    np.random.seed(seed)

    n_features = X.shape[1]  # number of input features

    # Small random weights help avoid saturation of activations at start
    W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const
    b1 = np.zeros((num_hidden_layer_neurons, 1))
    W2 = np.random.randn(1, num_hidden_layer_neurons) * scale_const
    b2 = np.zeros((1, 1))

    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}

```




![Forward and backward propagation in a single-hidden-layer neural network. The forward pass takes inputs $X$ through weights $W^{[1]}, W^{[2]}$ and biases $b^{[1]}, b^{[2]}$ to produce pre-activations $Z^{[1]}, Z^{[2]}$, activations $A^{[1]}, A^{[2]}$, and final output $y$. The backward pass computes gradients of activations, pre-activations, weights, and biases $(dA, dZ, dW, db)$ from the output layer back to the input layer for parameter updates.](images/prop.png){fig-align="center" width="70%"}




```{python forward_propagation}
#| code-fold: true
#| code-summary: "Show the forward propagation code"

import numpy as np

def forward_propagation(parameters, X_adj):
    """
    Perform one forward pass (regression: linear output).
    """
    W1, b1 = parameters["W1"], parameters["b1"]
    W2, b2 = parameters["W2"], parameters["b2"]

    Z1 = np.dot(W1, X_adj) + b1      # hidden layer affine transform
    A1 = activate(Z1, "tanh")        # hidden nonlinearity
    Z2 = np.dot(W2, A1) + b2         # output layer affine transform
    A2 = Z2                          # linear output for regression

    return {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}

```



```{python backward_propagation}
#| code-fold: true
#| code-summary: "Show the backward propagation code"

import numpy as np

def backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):
    """
    Compute gradients for parameters using backpropagation (MSE loss, linear output).
    """
    N = X_adj.shape[1]  # number of samples

    W2 = parameters["W2"]
    A1, A2 = forward_propagation_values["A1"], forward_propagation_values["A2"]

    # Output layer: for MSE with linear output, dZ2 = A2 - Y
    dZ2 = A2 - Y_adj
    dW2 = np.dot(dZ2, A1.T) / N
    db2 = np.sum(dZ2, axis=1, keepdims=True) / N

    # Hidden layer
    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2)
    dW1 = np.dot(dZ1, X_adj.T) / N
    db1 = np.sum(dZ1, axis=1, keepdims=True) / N

    return {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}


```



```{python update_parameters}
#| code-fold: true
#| code-summary: "Show the parameters update code"

import numpy as np

def update_parameters(parameters, grads, learning_rate=0.01):
    """
    Update parameters using gradient descent.
    """
    # subtract learning_rate * gradient for each parameter
    W1 = parameters["W1"] - learning_rate * grads["dW1"]
    b1 = parameters["b1"] - learning_rate * grads["db1"]
    W2 = parameters["W2"] - learning_rate * grads["dW2"]
    b2 = parameters["b2"] - learning_rate * grads["db2"]

    return {"W1": W1, "b1": b1, "W2": W2, "b2": b2}


```



- Create a wrapper function `train_neural_network` to run the training loop.


```{python train_neural_network}
#| code-fold: true
#| code-summary: "Show the neural network training code"

def train_neural_network(X, Y, num_iterations, num_hidden_layer_neurons=4):
    """
    Trains a simple 1-hidden-layer neural network using gradient descent.
    """
    # Transpose X so columns are examples, reshape Y to row vector
    X_adj = X.T.copy()                         
    Y_adj = Y.values.reshape(1, -1).copy()     

    # Initialize weights and biases
    parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)

    for iteration in range(num_iterations):
        # Forward pass
        forward_values = forward_propagation(parameters, X_adj.copy())

        # Backward pass
        grads = backward_propagation(parameters, forward_values, X_adj.copy(), Y_adj.copy())

        # Parameter update
        parameters = update_parameters(parameters, grads)

    return parameters



```





### Implement Prediction Function
- Create a `predict` function that runs forward propagation and returns continuous outputs.

```{python auxiliary_functions}
#| code-fold: true
#| code-summary: "Show the code"

def predict(nn_parameters, X):
    """
    Generates continuous predictions from a trained neural network (regression).
    """
    # Transpose X so columns are examples
    X_adj = X.T.copy()

    # Forward pass to get output layer values
    forward_values = forward_propagation(nn_parameters, X_adj.copy())

    # For regression, output is linear (A2)
    return forward_values["A2"].ravel()


```


# Application on Hitters data

#### Load and Prepare Data
- Load the `hitters.csv` dataset.
- Separate features `X` and target `Y`.
- Split to train and test set

```{python load_data}
#| code-fold: true
#| code-summary: "Load Hitters, drop NAs, split train/test, standardize X"

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# Adjust path if needed
data_path = os.path.join(os.path.expanduser("~\\Documents\\BOI_DL_website"), "data\\Hitters.csv")
raw_df = pd.read_csv(data_path)

target = "Salary"

# Numeric-only to keep it simple (categoricals can be one-hot encoded later)
numeric_cols = raw_df.select_dtypes(include=[np.number]).columns.tolist()
features = [c for c in numeric_cols if c != target]

# Drop rows with any NA in selected features or target
df = raw_df[features + [target]].dropna().copy()

X = df[features].copy()
Y = df[target].copy()

# Train/test split
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

# Standardize features: fit on train, transform both; assign back to same vars
scaler = StandardScaler().fit(X_train)
X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)
X_test  = pd.DataFrame(scaler.transform(X_test),  columns=X_test.columns,  index=X_test.index)

print(f"Train: {X_train.shape}, Test: {X_test.shape}")



```


#### Train the Neural Network
- Train the network with `num_iterations=10000` and `num_hidden_layer_neurons=4`.

```{python fit_model}

nn_parameters = train_neural_network(
    X_train, Y_train, num_iterations=10000, num_hidden_layer_neurons=4
)


```

#### Evaluate the Model
- Generate predictions on the training set.
- Compute and print MSE / MAE / R².
```{python predictions}

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

nn_pred = predict(nn_parameters, X.copy())

nn_mse = mean_squared_error(Y, nn_pred)
nn_mae = mean_absolute_error(Y, nn_pred)
nn_r2  = r2_score(Y, nn_pred)

print(f"Neural network — MSE: {nn_mse:.4f}, MAE: {nn_mae:.4f}, R^2: {nn_r2:.4f}")


```


### Compare with Linear Regression
- Train a linear regression model on the same data.


```{python lin_reg}
#| code-fold: true
#| code-summary: "Show the code"

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

lin_reg = LinearRegression()
lin_reg.fit(X, Y)

lin_pred = lin_reg.predict(X)

lin_mse = mean_squared_error(Y, lin_pred)
lin_mae = mean_absolute_error(Y, lin_pred)
lin_r2  = r2_score(Y, lin_pred)

print(f"Linear regression — MSE: {lin_mse:.4f}, MAE: {lin_mae:.4f}, R^2: {lin_r2:.4f}")


```



<!-- # Appendix -->

<!-- ## Derivation: Mean Squared Error (MSE) with Linear Output -->

<!-- With a linear output $A^{[2]} = Z^{[2]}$ and labels $Y$, -->

<!-- $$ -->
<!-- L = \frac{1}{2N}\,\lVert A^{[2]} - Y \rVert_F^{2} -->
<!-- $$ -->

<!-- Then -->
<!-- $$ -->
<!-- \frac{\partial L}{\partial Z^{[2]}} = \frac{1}{N}\,(A^{[2]} - Y) -->
<!-- $$ -->

<!-- Gradients: -->
<!-- $$ -->
<!-- dW^{[2]} = \frac{1}{N}\,(A^{[2]} - Y)\,A^{[1]T}, \qquad -->
<!-- db^{[2]} = \frac{1}{N}\sum_{i=1}^N (A^{[2]} - Y)_{:,i} -->
<!-- $$ -->

<!-- For a hidden tanh layer: -->
<!-- $$ -->
<!-- dZ^{[1]} = W^{[2]T}\,(A^{[2]} - Y)\,\odot\,\big(1 - A^{[1]}\odot A^{[1]}\big) -->
<!-- $$ -->

<!-- $$ -->
<!-- dW^{[1]} = \frac{1}{N}\, dZ^{[1]} X^T, \qquad -->
<!-- db^{[1]} = \frac{1}{N}\sum_{i=1}^N dZ^{[1]}_{:,i} -->
<!-- $$ -->

<!-- **Code correspondence** -->

<!-- ```{python} -->
<!-- #| eval: false -->
<!-- dZ2 = A2 - Y_adj -->
<!-- dW2 = np.dot(dZ2, A1.T) / N -->
<!-- db2 = np.sum(dZ2, axis=1, keepdims=True) / N -->
<!-- dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2) -->
<!-- dW1 = np.dot(dZ1, X_adj.T) / N -->
<!-- db1 = np.sum(dZ1, axis=1, keepdims=True) / N -->
<!-- ``` -->
