[
  {
    "objectID": "topics/single_layer_nn.html",
    "href": "topics/single_layer_nn.html",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "",
    "text": "Implement a simple neural network from scratch and compare its performance to logistic regression on a 2D dataset.\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os"
  },
  {
    "objectID": "topics/single_layer_nn.html#auxiliary-functions",
    "href": "topics/single_layer_nn.html#auxiliary-functions",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nImplement Training Functions\n\nDefine helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.\nCreate a wrapper function train_neural_network to run the training loop.\n\n\n\nShow the code\ndef activate(Z, activation_function = \"tanh\"):\n  \n  if activation_function == \"tanh\":\n    \n    A = np.tanh(Z)\n  \n  elif activation_function == \"sigmoid\":\n    \n    A = 1 / (1 + np.exp(-Z))\n    \n  return(A)\n\n\ndef initialize_parameters(X_adj,num_hidden_layer_neurons, scale_const = 0.01):\n  np.random.seed(1)\n\n  n_features = X_adj.shape[1]  # Number of input features\n\n  W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n\n  b1 = np.zeros((num_hidden_layer_neurons,1))\n\n  W2 = np.random.randn(1,num_hidden_layer_neurons) * scale_const\n\n  b2 = np.zeros((1,1))\n\n  parameters = {\"W1\": W1, \"b1\": b1, \"W2\":W2, \"b2\":b2}\n\n  return parameters\n\ndef forward_propagation(parameters,X_adj):\n  \n  W1 = parameters[\"W1\"]\n  \n  b1 = parameters[\"b1\"]\n\n  W2 = parameters[\"W2\"]\n\n  b2 = parameters[\"b2\"]\n\n  Z1 = np.dot(W1,X_adj) + b1\n\n  A1 = activate(Z1)\n\n  Z2 = np.dot(W2,A1) + b2\n\n  A2 = activate(Z2,activation_function=\"sigmoid\")\n\n  forward_propagation_values = {\"Z1\":Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n\n  return forward_propagation_values\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n\n  p = X_adj.shape[1]\n\n  W1 = parameters[\"W1\"]\n\n  W2 = parameters[\"W2\"]\n\n  A1 = forward_propagation_values[\"A1\"]\n\n  A2 = forward_propagation_values[\"A2\"]\n\n  dZ2 = A2 - Y_adj\n\n  dW2 = np.dot(dZ2, A1.T) / p\n\n  db2 = np.sum(dZ2, axis=1, keepdims=True) / p\n\n  dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n\n  dW1 = np.dot(dZ1,X_adj.T) / p\n\n  db1 = np.sum(dZ1, axis=1, keepdims=True) / p\n\n  grads = {\"dW1\":dW1, \"db1\":db1, \"dW2\":dW2, \"db2\":db2}\n\n  return grads\n\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n  \n  W1 = parameters[\"W1\"]\n\n  b1 = parameters[\"b1\"]\n\n  W2 = parameters[\"W2\"]\n\n  b2 = parameters[\"b2\"]\n\n  \n  dW1 = grads[\"dW1\"]\n\n  db1 = grads[\"db1\"]\n\n  dW2 = grads[\"dW2\"]\n\n  db2 = grads[\"db2\"]\n\n  \n  W1 = W1 - learning_rate * dW1\n\n  b1 = b1 - learning_rate * db1\n\n  W2 = W2 - learning_rate * dW2\n\n  b2 = b2 - learning_rate * db2\n\n  \n  parameters = {\"W1\": W1, \"b1\": b1, \"W2\":W2, \"b2\":b2}\n\n  return parameters\n\n\n\n\nShow the code\ndef train_neural_network(X,Y, num_iterations,num_hidden_layer_neurons = 4):\n    \n    # Initialize the model's parameters\n    # Loop:\n    #  - Implement forward propagation to get the predictions\n    #  - Implement backward propagation to get the gradients\n    #  - Update parameters (gradient descent)\n\n\n  X_adj = X.T.copy()\n\n  Y_adj = Y.values.reshape(1,-1).copy()\n\n  parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n  for interation in range(num_iterations):\n    \n    forward_propagation_values = forward_propagation(parameters,X_adj.copy())\n\n    grads = backward_propagation(parameters, forward_propagation_values, X_adj.copy(), Y_adj.copy())\n\n    parameters = update_parameters(parameters, grads)\n\n  return parameters\n\n\n\n\nImplement Prediction Function\n\nCreate a predict function that runs forward propagation and thresholds outputs.\n\n\n\nShow the code\ndef predict(nn_parameters, X, threshold = 0.5):\n\n  X_adj = X.T.copy()\n\n  forward_propagation_values = forward_propagation(nn_parameters, X_adj.copy())\n\n  y_pred = forward_propagation_values[\"A2\"]\n\n  y_pred = y_pred &gt; threshold\n\n  return y_pred.astype(int).ravel()"
  },
  {
    "objectID": "topics/optim_methods.html",
    "href": "topics/optim_methods.html",
    "title": "Optimization Methods in Neural Networks",
    "section": "",
    "text": "Implement and evaluate different optimization strategies for training a simple neural network"
  },
  {
    "objectID": "topics/optim_methods.html#model",
    "href": "topics/optim_methods.html#model",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nlayer_dims = [2, 4, 1]\n\ngd_parameters, gd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions",
    "href": "topics/optim_methods.html#predictions",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_gd = predict_nn(X, gd_parameters)\n\ngd_score = accuracy_score(pred_gd.flatten(),y.flatten())\n\nprint(f\"GD accuracy score is {gd_score}\")\n\n\nGD accuracy score is 0.818"
  },
  {
    "objectID": "topics/optim_methods.html#model-1",
    "href": "topics/optim_methods.html#model-1",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nsgd_parameters, sgd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=32,\n    print_cost=False,\n    print_every=200\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-1",
    "href": "topics/optim_methods.html#predictions-1",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_sgd = predict_nn(X, sgd_parameters)\n\nsgd_score = accuracy_score(pred_sgd.flatten(),y.flatten())\n\nprint(f\"GD accuracy score is {sgd_score}\")\n\n\nGD accuracy score is 0.887"
  },
  {
    "objectID": "topics/optim_methods.html#model-2",
    "href": "topics/optim_methods.html#model-2",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nmomentum_parameters, momentum_costs = model(\n    X, y, layer_dims,\n    optimizer=\"momentum\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta = 0.9\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-2",
    "href": "topics/optim_methods.html#predictions-2",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_momentum = predict_nn(X, momentum_parameters)\n\nmomentum_score = accuracy_score(pred_momentum.flatten(),y.flatten())\n\nprint(f\"Momentum accuracy score is {momentum_score}\")\n\n\nMomentum accuracy score is 0.799"
  },
  {
    "objectID": "topics/optim_methods.html#model-3",
    "href": "topics/optim_methods.html#model-3",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nrmsprop_parameters, rmsprop_costs = model(\n    X, y, layer_dims,\n    optimizer=\"rmsprop\",\n    learning_rate=0.001,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta2 = 0.9,\n    epsilon = 1 * 10 ** (-8)\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-3",
    "href": "topics/optim_methods.html#predictions-3",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_rmsprop = predict_nn(X, rmsprop_parameters)\n\nrmsprop_score = accuracy_score(pred_rmsprop.flatten(),y.flatten())\n\nprint(f\"RMSProp accuracy score is {rmsprop_score}\")\n\n\nRMSProp accuracy score is 0.866"
  },
  {
    "objectID": "topics/optim_methods.html#model-4",
    "href": "topics/optim_methods.html#model-4",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nadam_parameters, adam_costs = model(\n    X, y, layer_dims,\n    optimizer=\"adam\",\n    learning_rate=0.001,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta1 = 0.9,\n    beta2 = 0.999,\n    epsilon = 1 * 10 ** (-8)\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-4",
    "href": "topics/optim_methods.html#predictions-4",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_adam = predict_nn(X, adam_parameters)\n\nadam_score = accuracy_score(pred_adam.flatten(),y.flatten())\n\nprint(f\"Adam accuracy score is {adam_score}\")\n\n\nAdam accuracy score is 0.867"
  },
  {
    "objectID": "topics/convolution.html",
    "href": "topics/convolution.html",
    "title": "Convolutional Neural Network (CNN) for Image Classification",
    "section": "",
    "text": "Build, train, and evaluate a Convolutional Neural Network using the Sign Language MNIST dataset.\n\nImport and Load Data\n\nImport necessary libraries, including TensorFlow and Keras layers.\nLoad the training and test datasets from CSV files.\n\n\n\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n\n\n\nShow the code\n\ntrain_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_train.csv\"))\n\ntest_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_test.csv\"))\n\n\n\n\nPreprocessing\n\nCreate a function to preprocess pixel values: normalize and reshape to (28, 28, 1).\nCreate a function to preprocess labels: remap classes to account for the missing letter ‘J’ and one-hot encode them.\nApply the preprocessing functions to the training and test sets.\n\n\n\nShow the code\n\ndef preprocess_data(df, img_height=28, img_width=28):\n  \n  processed_df = df / 255.0\n\n  processed_df = processed_df.values.reshape(-1, img_height, img_width, 1).copy()\n\n  return processed_df\n\n\ndef preprocess_labels(label_series, num_classes=24):\n    \"\"\"\n    Remaps labels to skip index 9 (J) and applies one-hot encoding.\n\n    Parameters:\n    - label_series: a pandas Series or 1D array of labels (originally 0–25, with 9 missing)\n    - num_classes: total number of actual classes (default 24)\n\n    Returns:\n    - One-hot encoded labels of shape (n_samples, num_classes)\n    \"\"\"\n    labels = np.array(label_series)\n\n    remapped_labels = np.array([l - 1 if l &gt; 9 else l for l in labels])\n\n    categorical_labels = to_categorical(remapped_labels, num_classes=num_classes)\n\n    return categorical_labels\n\n\n# Reverse the earlier remapping: add 1 to all labels ≥ 9\ndef reverse_remap(labels):\n    return [l + 1 if l &gt;= 9 else l for l in labels]\n\n\n\ndef show_predictions(x_data, y_true, y_pred, indices=None, n=6):\n    if indices is None:\n        indices = np.random.choice(len(x_data), n, replace=False)\n\n    plt.figure(figsize=(12, 6))\n    for i, idx in enumerate(indices):\n        plt.subplot(2, n // 2, i + 1)\n        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')\n        plt.title(f\"Pred: {y_pred[idx]}\\nTrue: {y_true[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nShow the code\n\n\n# Separate labels\ny_train = preprocess_labels(train_set['label'])\n\ny_test = preprocess_labels(test_set['label'])\n\n# Remove labels from the pixel data\nx_train = preprocess_data(train_set.drop('label', axis=1))\n\nx_test = preprocess_data(test_set.drop('label', axis=1))\n\n\n\n\nDefine CNN Model\n\nConstruct a Sequential model with:\n\nA Conv2D layer (32 filters, 3×3 kernel, ReLU, padding=‘same’).\nA MaxPooling2D layer (2×2).\nAnother Conv2D + MaxPooling2D block with 64 filters.\nA Flatten layer.\nA fully connected Dense layer with 128 units and ReLU activation.\nA Dropout layer with rate 0.3.\nA final Dense layer with 24 units and softmax activation.\n\nCompile the model using the Adam optimizer and categorical cross-entropy loss.\n\n\n\nShow the code\n\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(24, activation='softmax')  # 24 because of label remapping\n])\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\n\n\nTrain the Model\n\nFit the model on the training data for 5 epochs with batch size 128.\nUse the test data as validation during training.\n\n\n\nShow the code\n\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_test, y_test),\n    epochs=5,\n    batch_size=128,\n    verbose = 0\n)\n\n\n\n\nEvaluate the Model\n\nEvaluate the model on the test data.\nReport test accuracy.\n\n\n\nShow the code\nloss, accuracy = model.evaluate(x_test, y_test)\n\n\nShow the code\nprint(f\"Test Accuracy: {accuracy:.4f}\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Python workshopn\n\n\n\n\nIntroduction\nSingle layer\nOptimization method\nConvolution network"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction\nSingle layer\nOptimization method\nConvolution network"
  },
  {
    "objectID": "topics/intro.html",
    "href": "topics/intro.html",
    "title": "Intro",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom patsy import dmatrix\n\nfrom sklearn.linear_model import LinearRegression\n\nimport pygam\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\Wage.csv\"))\n\ny_vec = raw_df[\"wage\"].copy()\n\nx_vec = raw_df[[\"age\"]].copy()\n\n\n\n\nShow the code\n\ndef plot_predictions(pred_df, title):\n  pred_cols = [temp_name for temp_name in pred_df.columns.values if \"pred\" in temp_name]\n\n  plt.figure(figsize=(10, 6))\n  plt.scatter(pred_df[\"age\"], pred_df[\"wage\"], label=\"Actual Wage\", alpha = 0.7)\n\n  for temp_name in pred_cols:\n    plt.scatter(pred_df[\"age\"], pred_df[temp_name], label=temp_name, alpha = 0.7)\n#  plt.scatter(pred_df[\"age\"], pred_df[\"wage_pred\"], label=\"Predicted Wage\", alpha = 0.7)\n  plt.xlabel(\"Age\")\n  plt.ylabel(\"Wage\")\n  plt.title(title)\n  plt.legend()\n  plt.grid(True)\n  plt.show()\n\n\n\nPolynomial regression\n\nConstruct polynomial features:\n\nUsing the dmatrix function, create a feature matrix that includes polynomial terms up to the fourth degree: \\(\\text{age}, \\text{age}^2, \\text{age}^3, \\text{age}^4\\)\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec from the polynomial features.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\n\n\nShow the code\npoly_x_mat = dmatrix(\"age + I(age**2) + I(age**3) + I(age**4)\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(poly_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\npoly_pred = lin_reg.predict(poly_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": poly_pred}),\n                 \"Polynomial Regression\")\n\n\n\n\n\n\n\n\n\n\n\nStep functions\n\nDefine step function intervals (bins):\n\nUse np.percentile to determine cutoff points (knots) that divide age into quartiles.\n\nUse pd.cut to assign each age value to a bin.\n\nCreate a step function design matrix:\n\nUse the dmatrix function to encode the binned age values as categorical variables.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the step function representation of age.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\nHints:\n* Ensure that the step function bins do not have duplicate edges, which can be handled using duplicates='drop' in pd.cut.\n* You can use matplotlib.pyplot or seaborn to visualize the stepwise fitted function.\n* Since this is a piecewise constant model, expect a regression curve with horizontal segments rather than a smooth curve.\n\n\nShow the code\nknots = np.percentile(x_vec[\"age\"], [0, 15, 25, 50, 75,90, 100])\n\nx_vec_step = pd.cut(x_vec[\"age\"], bins=knots, labels=False,\n                    include_lowest=True,duplicates='drop')\n\nstep_x_mat = dmatrix(\"C(x_vec_step)\", x_vec_step)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(step_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nstep_pred = lin_reg.predict(step_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": step_pred}), \"Step Regression\")\n\n\n\n\n\n\n\n\n\n\n\nPiecewise polynomials\n\nDefine the knot location:\n\nSet a single knot at age = 50 to allow for a change in the polynomial relationship at this point.\n\nCreate piecewise polynomial terms:\n\n\nConstruct polynomial terms (age, age², age³) for the entire dataset.\nDefine separate squared and cubic terms for values below and above the knot to allow for discontinuity.\nUse np.maximum to ensure that terms are active only in their respective regions.\n\n\nCreate a design matrix:\n\nCombine all polynomial terms into a matrix that serves as input for the regression model.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the spline-transformed age values.\n\nMake predictions:\nUse the trained model to generate predictions for y_vec.\n\nHints:\n* B-splines create smooth, continuous fits by combining piecewise polynomial segments.\n* You can experiment with additional knots to see how the flexibility of the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curve.\n\n\nShow the code\nknot = 50\n\n# Manually create piecewise terms to enforce discontinuity at the knot\nx_less_knot = np.maximum(0, knot - x_vec)  # For x &lt; knot\nx_greater_knot = np.maximum(0, x_vec - knot)  # For x &gt;= knot\n\n# Combine the terms into a design matrix (manual spline basis)\nspline_x_mat = np.column_stack((x_vec, x_vec**2, x_vec**3, x_less_knot**2,\n                                x_greater_knot**2,x_less_knot**3, x_greater_knot**3))\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": spline_pred}),\n\"Piecewise Spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nSplines\n\nConstruct spline basis matrices:\n\nUse the dmatrix function to create a B-spline basis with knots at age = 25, 40, 60 and a polynomial degree of 3.\nUse the dmatrix function to create a natural spline basis with the same knots.\n\nFit linear regression models:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit one model using the B-spline basis and another using the natural spline basis.\n\nMake predictions:\nUse each trained model to generate predictions for y_vec.\n\nHints:\n* B-splines allow local flexibility, adjusting the curve within defined knots.\n* Natural splines impose additional constraints that make the curve behave more smoothly at the boundaries.\n* Try adjusting the number and position of knots to see how the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curves.\n\n\nShow the code\nspline_x_mat = dmatrix(\"bs(age, knots = [25,40,60], degree=3)\", x_vec)\n\nnatural_spline_x_mat = dmatrix(\"cr(age, knots = [25,40,60])\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(natural_spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nnatural_spline_pred = lin_reg.predict(natural_spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec,\n                               \"spline_pred\": spline_pred,\n                               \"natural_spline_pred\": natural_spline_pred}),\n                               \"Spline and Natural spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nGeneral Additive Models\n\n\nShow the code\nfrom pygam import LinearGAM, s, f\n\n# Encode categorical variable 'education'\nraw_df['education_code'] = raw_df['education'].astype('category').cat.codes\n\n# Features and response\nX = raw_df[['year', 'age', 'education_code']].values\n\ny = raw_df['wage'].values\n\n# Fit the GAM model\n\ngam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=8) + f(2));\n\ngam.fit(X, y);\n\n\n\n\nShow the code\n# Generate smooth predictions for each term\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot the effect of year\nyear_grid = np.linspace(raw_df['year'].min(), raw_df['year'].max(), 100)\nX_year = np.zeros((100, 3))\nX_year[:, 0] = year_grid\naxes[0].plot(year_grid, gam.partial_dependence(0, X=X_year), color='red');\naxes[0].set_title(r\"$f_1(\\mathrm{year})$\");\naxes[0].set_xlabel(\"year\");\naxes[0].set_ylabel(\"Effect\");\naxes[0].set_ylim(-30, 30);\n\n# Plot the effect of age\nage_grid = np.linspace(raw_df['age'].min(), raw_df['age'].max(), 100)\nX_age = np.zeros((100, 3))\nX_age[:, 1] = age_grid\naxes[1].plot(age_grid, gam.partial_dependence(1, X=X_age), color='red');\naxes[1].set_title(r\"$f_2(\\mathrm{age})$\");\naxes[1].set_xlabel(\"age\");\naxes[1].set_ylabel(\"Effect\");\naxes[1].set_ylim(-50, 40);\n\n# Plot the effect of education\neducation_levels = np.sort(raw_df['education_code'].unique())\nX_edu = np.zeros((len(education_levels), 3))\nX_edu[:, 2] = education_levels\naxes[2].bar(\n    raw_df['education'].astype('category').cat.categories,\n    gam.partial_dependence(2, X=X_edu).flatten(),\n    color='red'\n);\naxes[2].set_title(r\"$f_3(\\mathrm{education})$\");\naxes[2].set_xlabel(\"education\");\naxes[2].set_ylabel(\"Effect\");\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/python_workshop.html",
    "href": "topics/python_workshop.html",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "This hands-on workshop introduces the essential Python skills needed for deep learning. You’ll run Python code directly in your environment (e.g., RStudio with reticulate or Jupyter), and practice every concept along the way.\n\n\n\nWe will use a local Python environment via reticulate, which allows running Python code directly inside this Quarto document.\n\n\n\nWhy Python for Deep Learning\nSetting up your local Python environment\nReading local files using pandas\n\n\n\n\nRead a CSV file from your computer using pandas and display its first few rows.\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport os\n\n\n\n\n\n\nGet familiar with Python’s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n\n\nData types: int, float, bool, str\nLists and dictionaries\nif, for, and while statements\nWriting and calling functions\n\n\n\n\nCreate a dictionary of students and their exam scores. Write a function that:\n\nPrints each student’s average score\nReturns the name of the top student\n\nYou’ll use this data structure in the next section to explore NumPy arrays.\n\n\n\nShow the code\ndef calculate_averages(scores_dict):\n  \n  scores_df = pd.DataFrame(data=scores_dict.values(), index=scores_dict.keys()).T\n  \n  averages_df = scores_df.mean(axis = 0).sort_values(ascending=False)\n\n  return averages_df\n\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70]\n}\n\ncalculate_averages(student_scores)\n\n\nCarol    90.000000\nAlice    86.333333\nBob      79.333333\nDave     73.333333\ndtype: float64\n\n\n\n\n\n\nLearn to work with arrays and vectors using NumPy, Python’s core numerical library.\n\n\n\nCreating and manipulating NumPy arrays\nIndexing, slicing, shapes, reshaping\nElement-wise operations and broadcasting\nMatrix multiplication and axis-based operations\n\n\n\n\nUse the student-score data from the previous task:\n\nConvert it to a NumPy array\nCompute average scores using matrix multiplication\nPrint each student’s average based on the computed result\n\n\n\nstudent_names = list(student_scores.keys())\ngrade_lists = list(student_scores.values())  # list of lists, one per student\n\n# Step 3: Convert to NumPy array and transpose\n# Original shape: (n_students, n_exams) → Transpose to (n_exams, n_students)\ngrades_matrix = np.array(grade_lists).T\n\n# Step 4: Create a weight vector to compute averages (equal weights)\nn_exams = grades_matrix.shape[0]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\n\n# Step 5: Matrix multiplication → result shape: (n_students, 1)\naverages = grades_matrix.T @ weights  # (n_students, n_exams) x (n_exams, 1)\n\n# Step 6: Display results\nfor name, avg in zip(student_names, averages.flatten()):\n    print(f\"{name}: {avg:.2f}\")\n\nAlice: 86.33\nBob: 79.33\nCarol: 90.00\nDave: 73.33\n\n\n\n\n\n\nMost real-world deep learning tasks start with data in files. This section shows how to load and prepare it.\n\n\n\nLoad the planar_data.csv dataset\nSeparate features X and target Y\n\n\n\nShow the code\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\planar_data.csv\"))\n\nfeatures = [\"x_coord\",\"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\nY = raw_df[target].copy()\n\n\n\n\n\nLoad a planar dataset with columns x_coord, y_coord, and label:\n\nShow class distribution\nExtract features (X) and labels (Y)\nUse them to train and evaluate models\n\n\n\n\n\n\nWe’ll train two models on the same planar data:\n\nA logistic regression (linear decision boundary)\nA shallow neural network (non-linear decision boundary)\n\n\n\n\nBuilding models with Sequential\nAdding layers with Dense\nCompiling and training with fit\nComparing results across models\n\n\n\n\nUsing the prepared planar dataset:\n\nTrain a logistic regression model (1 layer)\nTrain a neural network with one hidden layer\nCompare their training accuracy\n\n\n\n\nShow the code\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n\n\n\nShow the code\nnn_model = Sequential([\n  Dense(10, activation='relu', input_shape=(2,)),\n  Dense(1, activation='sigmoid')\n])\n\n\nC:\\Users\\internet\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nShow the code\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\n\nnn_model.fit(X, Y, epochs=100, batch_size=32, verbose=0)\n\n\n&lt;keras.src.callbacks.history.History object at 0x000001A5AAF3AB10&gt;\n\n\nShow the code\n\nnn_acc = nn_model.evaluate(X, Y, verbose=0)[1]\n\n\n\n\nShow the code\nlog_reg_model = Sequential([\n  Dense(1, activation='sigmoid', input_shape=(2,))\n])\n\n\nC:\\Users\\internet\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nShow the code\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\n\nlog_reg_model.fit(X, Y, epochs=100, batch_size=32, verbose=0)\n\n\n&lt;keras.src.callbacks.history.History object at 0x000001A5AC104CD0&gt;\n\n\nShow the code\n\nlog_acc = log_reg_model.evaluate(X, Y, verbose=0)[1]\n\n\n\n\nShow the code\nprint(f\"Neural Network Accuracy: {nn_acc:.4f} vs Logistic Regression Accuracy: {log_acc:.4f}\")\n\n\nNeural Network Accuracy: 0.7800 vs Logistic Regression Accuracy: 0.5775"
  },
  {
    "objectID": "topics/python_workshop.html#introduction-environment-setup",
    "href": "topics/python_workshop.html#introduction-environment-setup",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "We will use a local Python environment via reticulate, which allows running Python code directly inside this Quarto document.\n\n\n\nWhy Python for Deep Learning\nSetting up your local Python environment\nReading local files using pandas\n\n\n\n\nRead a CSV file from your computer using pandas and display its first few rows.\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport os"
  },
  {
    "objectID": "topics/python_workshop.html#python-fundamentals",
    "href": "topics/python_workshop.html#python-fundamentals",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Get familiar with Python’s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n\n\nData types: int, float, bool, str\nLists and dictionaries\nif, for, and while statements\nWriting and calling functions\n\n\n\n\nCreate a dictionary of students and their exam scores. Write a function that:\n\nPrints each student’s average score\nReturns the name of the top student\n\nYou’ll use this data structure in the next section to explore NumPy arrays.\n\n\n\nShow the code\ndef calculate_averages(scores_dict):\n  \n  scores_df = pd.DataFrame(data=scores_dict.values(), index=scores_dict.keys()).T\n  \n  averages_df = scores_df.mean(axis = 0).sort_values(ascending=False)\n\n  return averages_df\n\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70]\n}\n\ncalculate_averages(student_scores)\n\n\nCarol    90.000000\nAlice    86.333333\nBob      79.333333\nDave     73.333333\ndtype: float64"
  },
  {
    "objectID": "topics/python_workshop.html#numerical-computing-with-numpy",
    "href": "topics/python_workshop.html#numerical-computing-with-numpy",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Learn to work with arrays and vectors using NumPy, Python’s core numerical library.\n\n\n\nCreating and manipulating NumPy arrays\nIndexing, slicing, shapes, reshaping\nElement-wise operations and broadcasting\nMatrix multiplication and axis-based operations\n\n\n\n\nUse the student-score data from the previous task:\n\nConvert it to a NumPy array\nCompute average scores using matrix multiplication\nPrint each student’s average based on the computed result\n\n\n\nstudent_names = list(student_scores.keys())\ngrade_lists = list(student_scores.values())  # list of lists, one per student\n\n# Step 3: Convert to NumPy array and transpose\n# Original shape: (n_students, n_exams) → Transpose to (n_exams, n_students)\ngrades_matrix = np.array(grade_lists).T\n\n# Step 4: Create a weight vector to compute averages (equal weights)\nn_exams = grades_matrix.shape[0]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\n\n# Step 5: Matrix multiplication → result shape: (n_students, 1)\naverages = grades_matrix.T @ weights  # (n_students, n_exams) x (n_exams, 1)\n\n# Step 6: Display results\nfor name, avg in zip(student_names, averages.flatten()):\n    print(f\"{name}: {avg:.2f}\")\n\nAlice: 86.33\nBob: 79.33\nCarol: 90.00\nDave: 73.33"
  },
  {
    "objectID": "topics/python_workshop.html#working-with-data",
    "href": "topics/python_workshop.html#working-with-data",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Most real-world deep learning tasks start with data in files. This section shows how to load and prepare it.\n\n\n\nLoad the planar_data.csv dataset\nSeparate features X and target Y\n\n\n\nShow the code\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\planar_data.csv\"))\n\nfeatures = [\"x_coord\",\"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\nY = raw_df[target].copy()\n\n\n\n\n\nLoad a planar dataset with columns x_coord, y_coord, and label:\n\nShow class distribution\nExtract features (X) and labels (Y)\nUse them to train and evaluate models"
  },
  {
    "objectID": "topics/python_workshop.html#neural-network-compared-to-logistic-regression",
    "href": "topics/python_workshop.html#neural-network-compared-to-logistic-regression",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "We’ll train two models on the same planar data:\n\nA logistic regression (linear decision boundary)\nA shallow neural network (non-linear decision boundary)\n\n\n\n\nBuilding models with Sequential\nAdding layers with Dense\nCompiling and training with fit\nComparing results across models\n\n\n\n\nUsing the prepared planar dataset:\n\nTrain a logistic regression model (1 layer)\nTrain a neural network with one hidden layer\nCompare their training accuracy\n\n\n\n\nShow the code\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n\n\n\nShow the code\nnn_model = Sequential([\n  Dense(10, activation='relu', input_shape=(2,)),\n  Dense(1, activation='sigmoid')\n])\n\n\nC:\\Users\\internet\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nShow the code\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\n\nnn_model.fit(X, Y, epochs=100, batch_size=32, verbose=0)\n\n\n&lt;keras.src.callbacks.history.History object at 0x000001A5AAF3AB10&gt;\n\n\nShow the code\n\nnn_acc = nn_model.evaluate(X, Y, verbose=0)[1]\n\n\n\n\nShow the code\nlog_reg_model = Sequential([\n  Dense(1, activation='sigmoid', input_shape=(2,))\n])\n\n\nC:\\Users\\internet\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nShow the code\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\n\nlog_reg_model.fit(X, Y, epochs=100, batch_size=32, verbose=0)\n\n\n&lt;keras.src.callbacks.history.History object at 0x000001A5AC104CD0&gt;\n\n\nShow the code\n\nlog_acc = log_reg_model.evaluate(X, Y, verbose=0)[1]\n\n\n\n\nShow the code\nprint(f\"Neural Network Accuracy: {nn_acc:.4f} vs Logistic Regression Accuracy: {log_acc:.4f}\")\n\n\nNeural Network Accuracy: 0.7800 vs Logistic Regression Accuracy: 0.5775"
  }
]