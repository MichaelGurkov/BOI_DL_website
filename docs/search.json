[
  {
    "objectID": "topics/single_layer_nn.html",
    "href": "topics/single_layer_nn.html",
    "title": "Single layer Neural Network",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os"
  },
  {
    "objectID": "topics/single_layer_nn.html#auxiliary-functions",
    "href": "topics/single_layer_nn.html#auxiliary-functions",
    "title": "Single layer Neural Network",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nNN training functions\n\n\nShow the code\ndef activate(Z, activation_function = \"tanh\"):\n  \n  if activation_function == \"tanh\":\n    \n    A = np.tanh(Z)\n  \n  elif activation_function == \"sigmoid\":\n    \n    A = 1 / (1 + np.exp(-Z))\n    \n  return(A)\n\n\ndef initialize_parameters(X_adj,num_hidden_layer_neurons, scale_const = 0.01):\n  np.random.seed(1)\n\n  n_features = X_adj.shape[1]  # Number of input features\n\n  W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n\n  b1 = np.zeros((num_hidden_layer_neurons,1))\n\n  W2 = np.random.randn(1,num_hidden_layer_neurons) * scale_const\n\n  b2 = np.zeros((1,1))\n\n  parameters = {\"W1\": W1, \"b1\": b1, \"W2\":W2, \"b2\":b2}\n\n  return parameters\n\ndef forward_propagation(parameters,X_adj):\n  \n  W1 = parameters[\"W1\"]\n  \n  b1 = parameters[\"b1\"]\n\n  W2 = parameters[\"W2\"]\n\n  b2 = parameters[\"b2\"]\n\n  Z1 = np.dot(W1,X_adj) + b1\n\n  A1 = activate(Z1)\n\n  Z2 = np.dot(W2,A1) + b2\n\n  A2 = activate(Z2,activation_function=\"sigmoid\")\n\n  forward_propagation_values = {\"Z1\":Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n\n  return forward_propagation_values\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n\n  p = X_adj.shape[1]\n\n  W1 = parameters[\"W1\"]\n\n  W2 = parameters[\"W2\"]\n\n  A1 = forward_propagation_values[\"A1\"]\n\n  A2 = forward_propagation_values[\"A2\"]\n\n  dZ2 = A2 - Y_adj\n\n  dW2 = np.dot(dZ2, A1.T) / p\n\n  db2 = np.sum(dZ2, axis=1, keepdims=True) / p\n\n  dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n\n  dW1 = np.dot(dZ1,X_adj.T) / p\n\n  db1 = np.sum(dZ1, axis=1, keepdims=True) / p\n\n  grads = {\"dW1\":dW1, \"db1\":db1, \"dW2\":dW2, \"db2\":db2}\n\n  return grads\n\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n  \n  W1 = parameters[\"W1\"]\n\n  b1 = parameters[\"b1\"]\n\n  W2 = parameters[\"W2\"]\n\n  b2 = parameters[\"b2\"]\n\n  \n  dW1 = grads[\"dW1\"]\n\n  db1 = grads[\"db1\"]\n\n  dW2 = grads[\"dW2\"]\n\n  db2 = grads[\"db2\"]\n\n  \n  W1 = W1 - learning_rate * dW1\n\n  b1 = b1 - learning_rate * db1\n\n  W2 = W2 - learning_rate * dW2\n\n  b2 = b2 - learning_rate * db2\n\n  \n  parameters = {\"W1\": W1, \"b1\": b1, \"W2\":W2, \"b2\":b2}\n\n  return parameters\n\n\n\nNeural network implementation\n\n\nShow the code\ndef train_neural_network(X,Y, num_iterations,num_hidden_layer_neurons = 4):\n    \n    # Initialize the model's parameters\n    # Loop:\n    #  - Implement forward propagation to get the predictions\n    #  - Implement backward propagation to get the gradients\n    #  - Update parameters (gradient descent)\n\n\n  X_adj = X.T.copy()\n\n  Y_adj = Y.values.reshape(1,-1).copy()\n\n  parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n  for interation in range(num_iterations):\n    \n    forward_propagation_values = forward_propagation(parameters,X_adj.copy())\n\n    grads = backward_propagation(parameters, forward_propagation_values, X_adj.copy(), Y_adj.copy())\n\n    parameters = update_parameters(parameters, grads)\n\n  return parameters\n\n\n\n\n\nAuxilary functions\n\n\nShow the code\ndef predict(nn_parameters, X, threshold = 0.5):\n\n  X_adj = X.T.copy()\n\n  forward_propagation_values = forward_propagation(nn_parameters, X_adj.copy())\n\n  y_pred = forward_propagation_values[\"A2\"]\n\n  y_pred = y_pred &gt; threshold\n\n  return y_pred.astype(int).ravel()"
  },
  {
    "objectID": "topics/intro.html",
    "href": "topics/intro.html",
    "title": "Intro",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom patsy import dmatrix\n\nfrom sklearn.linear_model import LinearRegression\n\nimport pygam\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\Wage.csv\"))\n\ny_vec = raw_df[\"wage\"].copy()\n\nx_vec = raw_df[[\"age\"]].copy()\n\n\n\n\nShow the code\n\ndef plot_predictions(pred_df, title):\n  pred_cols = [temp_name for temp_name in pred_df.columns.values if \"pred\" in temp_name]\n\n  plt.figure(figsize=(10, 6))\n  plt.scatter(pred_df[\"age\"], pred_df[\"wage\"], label=\"Actual Wage\", alpha = 0.7)\n\n  for temp_name in pred_cols:\n    plt.scatter(pred_df[\"age\"], pred_df[temp_name], label=temp_name, alpha = 0.7)\n#  plt.scatter(pred_df[\"age\"], pred_df[\"wage_pred\"], label=\"Predicted Wage\", alpha = 0.7)\n  plt.xlabel(\"Age\")\n  plt.ylabel(\"Wage\")\n  plt.title(title)\n  plt.legend()\n  plt.grid(True)\n  plt.show()\n\n\n\nPolynomial regression\n\nConstruct polynomial features:\n\nUsing the dmatrix function, create a feature matrix that includes polynomial terms up to the fourth degree: \\(\\text{age}, \\text{age}^2, \\text{age}^3, \\text{age}^4\\)\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec from the polynomial features.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\n\n\nShow the code\npoly_x_mat = dmatrix(\"age + I(age**2) + I(age**3) + I(age**4)\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(poly_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\npoly_pred = lin_reg.predict(poly_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": poly_pred}),\n                 \"Polynomial Regression\")\n\n\n\n\n\n\n\n\n\n\n\nStep functions\n\nDefine step function intervals (bins):\n\nUse np.percentile to determine cutoff points (knots) that divide age into quartiles.\n\nUse pd.cut to assign each age value to a bin.\n\nCreate a step function design matrix:\n\nUse the dmatrix function to encode the binned age values as categorical variables.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the step function representation of age.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\nHints:\n* Ensure that the step function bins do not have duplicate edges, which can be handled using duplicates='drop' in pd.cut.\n* You can use matplotlib.pyplot or seaborn to visualize the stepwise fitted function.\n* Since this is a piecewise constant model, expect a regression curve with horizontal segments rather than a smooth curve.\n\n\nShow the code\nknots = np.percentile(x_vec[\"age\"], [0, 15, 25, 50, 75,90, 100])\n\nx_vec_step = pd.cut(x_vec[\"age\"], bins=knots, labels=False,\n                    include_lowest=True,duplicates='drop')\n\nstep_x_mat = dmatrix(\"C(x_vec_step)\", x_vec_step)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(step_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nstep_pred = lin_reg.predict(step_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": step_pred}), \"Step Regression\")\n\n\n\n\n\n\n\n\n\n\n\nPiecewise polynomials\n\nDefine the knot location:\n\nSet a single knot at age = 50 to allow for a change in the polynomial relationship at this point.\n\nCreate piecewise polynomial terms:\n\n\nConstruct polynomial terms (age, age², age³) for the entire dataset.\nDefine separate squared and cubic terms for values below and above the knot to allow for discontinuity.\nUse np.maximum to ensure that terms are active only in their respective regions.\n\n\nCreate a design matrix:\n\nCombine all polynomial terms into a matrix that serves as input for the regression model.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the spline-transformed age values.\n\nMake predictions:\nUse the trained model to generate predictions for y_vec.\n\nHints:\n* B-splines create smooth, continuous fits by combining piecewise polynomial segments.\n* You can experiment with additional knots to see how the flexibility of the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curve.\n\n\nShow the code\nknot = 50\n\n# Manually create piecewise terms to enforce discontinuity at the knot\nx_less_knot = np.maximum(0, knot - x_vec)  # For x &lt; knot\nx_greater_knot = np.maximum(0, x_vec - knot)  # For x &gt;= knot\n\n# Combine the terms into a design matrix (manual spline basis)\nspline_x_mat = np.column_stack((x_vec, x_vec**2, x_vec**3, x_less_knot**2,\n                                x_greater_knot**2,x_less_knot**3, x_greater_knot**3))\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": spline_pred}),\n\"Piecewise Spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nSplines\n\nConstruct spline basis matrices:\n\nUse the dmatrix function to create a B-spline basis with knots at age = 25, 40, 60 and a polynomial degree of 3.\nUse the dmatrix function to create a natural spline basis with the same knots.\n\nFit linear regression models:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit one model using the B-spline basis and another using the natural spline basis.\n\nMake predictions:\nUse each trained model to generate predictions for y_vec.\n\nHints:\n* B-splines allow local flexibility, adjusting the curve within defined knots.\n* Natural splines impose additional constraints that make the curve behave more smoothly at the boundaries.\n* Try adjusting the number and position of knots to see how the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curves.\n\n\nShow the code\nspline_x_mat = dmatrix(\"bs(age, knots = [25,40,60], degree=3)\", x_vec)\n\nnatural_spline_x_mat = dmatrix(\"cr(age, knots = [25,40,60])\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(natural_spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nnatural_spline_pred = lin_reg.predict(natural_spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec,\n                               \"spline_pred\": spline_pred,\n                               \"natural_spline_pred\": natural_spline_pred}),\n                               \"Spline and Natural spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nGeneral Additive Models\n\n\nShow the code\nfrom pygam import LinearGAM, s, f\n\n# Encode categorical variable 'education'\nraw_df['education_code'] = raw_df['education'].astype('category').cat.codes\n\n# Features and response\nX = raw_df[['year', 'age', 'education_code']].values\n\ny = raw_df['wage'].values\n\n# Fit the GAM model\n\ngam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=8) + f(2));\n\ngam.fit(X, y);\n\n\n\n\nShow the code\n# Generate smooth predictions for each term\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot the effect of year\nyear_grid = np.linspace(raw_df['year'].min(), raw_df['year'].max(), 100)\nX_year = np.zeros((100, 3))\nX_year[:, 0] = year_grid\naxes[0].plot(year_grid, gam.partial_dependence(0, X=X_year), color='red');\naxes[0].set_title(r\"$f_1(\\mathrm{year})$\");\naxes[0].set_xlabel(\"year\");\naxes[0].set_ylabel(\"Effect\");\naxes[0].set_ylim(-30, 30);\n\n# Plot the effect of age\nage_grid = np.linspace(raw_df['age'].min(), raw_df['age'].max(), 100)\nX_age = np.zeros((100, 3))\nX_age[:, 1] = age_grid\naxes[1].plot(age_grid, gam.partial_dependence(1, X=X_age), color='red');\naxes[1].set_title(r\"$f_2(\\mathrm{age})$\");\naxes[1].set_xlabel(\"age\");\naxes[1].set_ylabel(\"Effect\");\naxes[1].set_ylim(-50, 40);\n\n# Plot the effect of education\neducation_levels = np.sort(raw_df['education_code'].unique())\nX_edu = np.zeros((len(education_levels), 3))\nX_edu[:, 2] = education_levels\naxes[2].bar(\n    raw_df['education'].astype('category').cat.categories,\n    gam.partial_dependence(2, X=X_edu).flatten(),\n    color='red'\n);\naxes[2].set_title(r\"$f_3(\\mathrm{education})$\");\naxes[2].set_xlabel(\"education\");\naxes[2].set_ylabel(\"Effect\");\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction\nSingle layer\nOptimization method\nConvolution network"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction\nSingle layer\nOptimization method\nConvolution network"
  },
  {
    "objectID": "topics/convolution.html",
    "href": "topics/convolution.html",
    "title": "Convolution Neural Network",
    "section": "",
    "text": "Import libraries and data\n\n\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n\n\n\nShow the code\n\ntrain_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_train.csv\"))\n\ntest_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_test.csv\"))\n\n\n\n\nShow the code\n\ndef preprocess_data(df, img_height=28, img_width=28):\n  \n  processed_df = df / 255.0\n\n  processed_df = processed_df.values.reshape(-1, img_height, img_width, 1).copy()\n\n  return processed_df\n\n\ndef preprocess_labels(label_series, num_classes=24):\n    \"\"\"\n    Remaps labels to skip index 9 (J) and applies one-hot encoding.\n\n    Parameters:\n    - label_series: a pandas Series or 1D array of labels (originally 0–25, with 9 missing)\n    - num_classes: total number of actual classes (default 24)\n\n    Returns:\n    - One-hot encoded labels of shape (n_samples, num_classes)\n    \"\"\"\n    labels = np.array(label_series)\n\n    remapped_labels = np.array([l - 1 if l &gt; 9 else l for l in labels])\n\n    categorical_labels = to_categorical(remapped_labels, num_classes=num_classes)\n\n    return categorical_labels\n\n\n# Reverse the earlier remapping: add 1 to all labels ≥ 9\ndef reverse_remap(labels):\n    return [l + 1 if l &gt;= 9 else l for l in labels]\n\n\n\ndef show_predictions(x_data, y_true, y_pred, indices=None, n=6):\n    if indices is None:\n        indices = np.random.choice(len(x_data), n, replace=False)\n\n    plt.figure(figsize=(12, 6))\n    for i, idx in enumerate(indices):\n        plt.subplot(2, n // 2, i + 1)\n        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')\n        plt.title(f\"Pred: {y_pred[idx]}\\nTrue: {y_true[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nPreprocess and define model\n\n\nShow the code\n\n\n# Separate labels\ny_train = preprocess_labels(train_set['label'])\n\ny_test = preprocess_labels(test_set['label'])\n\n# Remove labels from the pixel data\nx_train = preprocess_data(train_set.drop('label', axis=1))\n\nx_test = preprocess_data(test_set.drop('label', axis=1))\n\n\n\n\nShow the code\n\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(24, activation='softmax')  # 24 because of label remapping\n])\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\n\n\nFit model and evaluate\n\n\nShow the code\n\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_test, y_test),\n    epochs=5,\n    batch_size=128,\n    verbose = 0\n)\n\n\n\n\nShow the code\nloss, accuracy = model.evaluate(x_test, y_test)\n\n\nShow the code\nprint(f\"Test Accuracy: {accuracy:.4f}\")"
  },
  {
    "objectID": "topics/optim_methods.html",
    "href": "topics/optim_methods.html",
    "title": "Optimization methods",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\n\nfrom func_package.tests import (test_forward_propagation,\ntest_backward_propagation, test_model)\n\nfrom func_package.model import model\n\nfrom func_package.utils import predict_nn\n\nfrom sklearn.metrics import accuracy_score\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\ndata_file_path = os.path.expanduser('~') + \"\\\\Documents\\\\BOI_DL_website\\\\data\\\\cluster_moons.csv\"\n\nraw_data = pd.read_csv(data_file_path)\n\nX = raw_data.iloc[:,0:2].copy()\n\nX = X.to_numpy().T.copy()\n\ny = raw_data.iloc[:,2].copy()\n\ny = y.to_numpy().reshape(1, -1).copy()\n\n\nplt.scatter(X[0, :], X[1, :], c=y.flatten())\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Scatter Plot of Feature 1 and Feature 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nGradient descent\n\n\nlayer_dims = [2, 4, 1]\n\ngd_parameters, gd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200\n)\n\n\npred_gd = predict_nn(X, gd_parameters)\n\naccuracy_score(pred_gd.flatten(),y.flatten())\n\n0.818\n\n\n\n\nStochastic gradient descent\n\n\nsgd_parameters, sgd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=32,\n    print_cost=False,\n    print_every=200\n)\n\n\npred_sgd = predict_nn(X, sgd_parameters)\n\naccuracy_score(pred_sgd.flatten(),y.flatten())\n\n0.887\n\n\n\n\nMomentum\n\n\nmomentum_parameters, momentum_costs = model(\n    X, y, layer_dims,\n    optimizer=\"momentum\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta = 0.9\n)\n\n\npred_momentum = predict_nn(X, momentum_parameters)\n\naccuracy_score(pred_momentum.flatten(),y.flatten())\n\n0.799\n\n\n\n\nRMSProp\n\n\nrmsprop_parameters, rmsprop_costs = model(\n    X, y, layer_dims,\n    optimizer=\"rmsprop\",\n    learning_rate=0.001,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta2 = 0.9,\n    epsilon = 1 * 10 ** (-8)\n)\n\n\npred_rmsprop = predict_nn(X, rmsprop_parameters)\n\naccuracy_score(pred_rmsprop.flatten(),y.flatten())\n\n0.866\n\n\n\n\nAdam\n\n\nadam_parameters, adam_costs = model(\n    X, y, layer_dims,\n    optimizer=\"adam\",\n    learning_rate=0.001,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta1 = 0.9,\n    beta2 = 0.999,\n    epsilon = 1 * 10 ** (-8)\n)\n\n\npred_adam = predict_nn(X, adam_parameters)\n\naccuracy_score(pred_adam.flatten(),y.flatten())\n\n0.867"
  }
]