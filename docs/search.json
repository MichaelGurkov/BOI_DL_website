[
  {
    "objectID": "topics/optim_methods.html",
    "href": "topics/optim_methods.html",
    "title": "Optimization methods",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\n\nfrom func_package.tests import (test_forward_propagation,\ntest_backward_propagation, test_model)\n\nfrom func_package.model import model\n\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\nGradient descent\n\n\n# test_forward_propagation()\n# \n# test_backward_propagation()\n\n\ntest_model()\n\nCost after epoch 200: 0.069470\nCost after epoch 400: 0.013542\nCost after epoch 600: 0.005890\nCost after epoch 800: 0.003328\nCost after epoch 1000: 0.002145\nCost after epoch 1200: 0.001496\nCost after epoch 1400: 0.001099\nCost after epoch 1600: 0.000838\nCost after epoch 1800: 0.000658\nCost after epoch 2000: 0.000527\nCosts logged every 200 epochs: [0.06947043702309688, 0.013541968813219256, 0.005890344090831819, 0.0033282236458506715, 0.0021450450079864287, 0.0014956123749607152, 0.0010994132299183633, 0.000838496694324244, 0.0006575331781621329, 0.0005271635697681502]\nW1 shape: (4, 2)\nb1 shape: (4, 1)\nW2 shape: (1, 4)\nb2 shape: (1, 1)\nPredictions on training set: [[0 1 1 0]]\nTrue labels:             [[0 1 1 0]]\nTraining accuracy: 100.0%\n\n\n\n\n# from func_package.model import model\n# \n# from func_package.utils import (initialize_parameters,split_parameters,\n# get_minibatches, compute_cost)\n# \n# from func_package.forward_propagation import forward_propagation\n# \n# from func_package.backward_propagation import backward_propagation\n# \n# from func_package.optimization import (initialize_optimizer_state,\n# update_parameters)\n# \n# \n# X = np.array([[0, 0, 1, 1],\n#               [0, 1, 0, 1]])   # shape (2, 4)\n# Y = np.array([[0, 1, 1, 0]])     # shape (1, 4)\n# \n#   # 2) Define a 2‑layer network: 2 inputs → 4 hidden units → 1 output\n# layer_dims = [2, 4, 1]\n# \n# parameters, costs = model(\n#   X, Y, layer_dims,\n#   optimizer=\"adam\",\n#   learning_rate=0.01,      # you can try 0.01 or even 0.1 here\n#   num_epochs=2000,\n#   batch_size=None,\n#   print_cost=True,\n#   print_every=200,\n#   beta1=0.9,\n#   beta2=0.999,\n#   epsilon=1e-8\n# )\n\n\n\nStochastic gradient descent\n\n\nMomentum\n\n\nRMSProp\n\n\nAdam"
  }
]