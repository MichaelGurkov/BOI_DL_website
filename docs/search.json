[
  {
    "objectID": "topics/single_layer_nn_regression.html",
    "href": "topics/single_layer_nn_regression.html",
    "title": "Single layer Neural Network for Regression",
    "section": "",
    "text": "Implement a simple neural network from scratch and compare its performance to linear regression on a 2D dataset.\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os"
  },
  {
    "objectID": "topics/single_layer_nn_regression.html#auxiliary-functions",
    "href": "topics/single_layer_nn_regression.html#auxiliary-functions",
    "title": "Single layer Neural Network for Regression",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nImplement Training Functions\n\nDefine helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.\n\n\n\nShow the activation function code\nimport numpy as np\n\n\ndef activate(Z, activation_function=\"tanh\"):\n    \"\"\"\n    Apply an activation function elementwise.\n    \"\"\"\n    if activation_function == \"tanh\":\n        return np.tanh(Z)  # squashes values to [-1, 1]\n    elif activation_function == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-Z))  # squashes values to [0, 1]\n    else:\n        raise ValueError(\"activation_function must be 'tanh' or 'sigmoid'.\")\n\n\n\n\nShow the parameters initialization code\nimport numpy as np\n\n\ndef initialize_parameters(X, num_hidden_layer_neurons, scale_const=0.01, seed=1):\n    \"\"\"\n    Initialize weights and biases for a single hidden-layer network.\n    \"\"\"\n    np.random.seed(seed)\n\n    n_features = X.shape[1]  # number of input features\n\n    # Small random weights help avoid saturation of activations at start\n    W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n    b1 = np.zeros((num_hidden_layer_neurons, 1))\n    W2 = np.random.randn(1, num_hidden_layer_neurons) * scale_const\n    b2 = np.zeros((1, 1))\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\n\n\nForward and backward propagation in a single-hidden-layer neural network. The forward pass takes inputs \\(X\\) through weights \\(W^{[1]}, W^{[2]}\\) and biases \\(b^{[1]}, b^{[2]}\\) to produce pre-activations \\(Z^{[1]}, Z^{[2]}\\), activations \\(A^{[1]}, A^{[2]}\\), and final output \\(y\\). The backward pass computes gradients of activations, pre-activations, weights, and biases \\((dA, dZ, dW, db)\\) from the output layer back to the input layer for parameter updates.\n\n\n\n\nShow the forward propagation code\nimport numpy as np\n\ndef forward_propagation(parameters, X_adj):\n    \"\"\"\n    Perform one forward pass (regression: linear output).\n    \"\"\"\n    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n\n    Z1 = np.dot(W1, X_adj) + b1      # hidden layer affine transform\n    A1 = activate(Z1, \"tanh\")        # hidden nonlinearity\n    Z2 = np.dot(W2, A1) + b2         # output layer affine transform\n    A2 = Z2                          # linear output for regression\n\n    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n\n\n\n\nShow the backward propagation code\nimport numpy as np\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n    \"\"\"\n    Compute gradients for parameters using backpropagation (MSE loss, linear output).\n    \"\"\"\n    N = X_adj.shape[1]  # number of samples\n\n    W2 = parameters[\"W2\"]\n    A1, A2 = forward_propagation_values[\"A1\"], forward_propagation_values[\"A2\"]\n\n    # Output layer: for MSE with linear output, dZ2 = A2 - Y\n    dZ2 = A2 - Y_adj\n    dW2 = np.dot(dZ2, A1.T) / N\n    db2 = np.sum(dZ2, axis=1, keepdims=True) / N\n\n    # Hidden layer\n    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2)\n    dW1 = np.dot(dZ1, X_adj.T) / N\n    db1 = np.sum(dZ1, axis=1, keepdims=True) / N\n\n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n\n\n\n\nShow the parameters update code\nimport numpy as np\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n    \"\"\"\n    Update parameters using gradient descent.\n    \"\"\"\n    # subtract learning_rate * gradient for each parameter\n    W1 = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n    b1 = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n    W2 = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n    b2 = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\nCreate a wrapper function train_neural_network to run the training loop.\n\n\n\nShow the neural network training code\ndef train_neural_network(X, Y, num_iterations, num_hidden_layer_neurons=4):\n    \"\"\"\n    Trains a simple 1-hidden-layer neural network using gradient descent.\n    \"\"\"\n    # Transpose X so columns are examples, reshape Y to row vector\n    X_adj = X.T.copy()                         \n    Y_adj = Y.values.reshape(1, -1).copy()     \n\n    # Initialize weights and biases\n    parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n    for iteration in range(num_iterations):\n        # Forward pass\n        forward_values = forward_propagation(parameters, X_adj.copy())\n\n        # Backward pass\n        grads = backward_propagation(parameters, forward_values, X_adj.copy(), Y_adj.copy())\n\n        # Parameter update\n        parameters = update_parameters(parameters, grads)\n\n    return parameters\n\n\n\n\nImplement Prediction Function\n\nCreate a predict function that runs forward propagation and returns continuous outputs.\n\n\n\nShow the code\ndef predict(nn_parameters, X):\n    \"\"\"\n    Generates continuous predictions from a trained neural network (regression).\n    \"\"\"\n    # Transpose X so columns are examples\n    X_adj = X.T.copy()\n\n    # Forward pass to get output layer values\n    forward_values = forward_propagation(nn_parameters, X_adj.copy())\n\n    # For regression, output is linear (A2)\n    return forward_values[\"A2\"].ravel()"
  },
  {
    "objectID": "topics/single_layer_nn_regression.html#derivation-mean-squared-error-mse-with-linear-output",
    "href": "topics/single_layer_nn_regression.html#derivation-mean-squared-error-mse-with-linear-output",
    "title": "Single layer Neural Network for Regression",
    "section": "Derivation: Mean Squared Error (MSE) with Linear Output",
    "text": "Derivation: Mean Squared Error (MSE) with Linear Output\nWith a linear output \\(A^{[2]} = Z^{[2]}\\) and labels \\(Y\\),\n\\[\nL = \\frac{1}{2N}\\,\\lVert A^{[2]} - Y \\rVert_F^{2}\n\\]\nThen \\[\n\\frac{\\partial L}{\\partial Z^{[2]}} = \\frac{1}{N}\\,(A^{[2]} - Y)\n\\]\nGradients: \\[\ndW^{[2]} = \\frac{1}{N}\\,(A^{[2]} - Y)\\,A^{[1]T}, \\qquad\ndb^{[2]} = \\frac{1}{N}\\sum_{i=1}^N (A^{[2]} - Y)_{:,i}\n\\]\nFor a hidden tanh layer: \\[\ndZ^{[1]} = W^{[2]T}\\,(A^{[2]} - Y)\\,\\odot\\,\\big(1 - A^{[1]}\\odot A^{[1]}\\big)\n\\]\n\\[\ndW^{[1]} = \\frac{1}{N}\\, dZ^{[1]} X^T, \\qquad\ndb^{[1]} = \\frac{1}{N}\\sum_{i=1}^N dZ^{[1]}_{:,i}\n\\]\nCode correspondence\n\ndZ2 = A2 - Y_adj\ndW2 = np.dot(dZ2, A1.T) / N\ndb2 = np.sum(dZ2, axis=1, keepdims=True) / N\ndZ1 = np.dot(W2.T, dZ2) * (1 - A1**2)\ndW1 = np.dot(dZ1, X_adj.T) / N\ndb1 = np.sum(dZ1, axis=1, keepdims=True) / N"
  },
  {
    "objectID": "topics/python_workshop.html",
    "href": "topics/python_workshop.html",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "This hands-on workshop introduces the essential Python skills needed for deep learning. You’ll run Python code directly in your environment (e.g., RStudio with reticulate or Jupyter), and practice every concept along the way.\n\n\n\nWe will use a local Python environment via reticulate, which allows running Python code directly inside this Quarto document.\n\n\n\nWhy Python for Deep Learning\nSetting up your local Python environment\nReading local files using pandas\n\n\n\n\nRead a CSV file from your computer using pandas and display its first few rows.\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport os\n\n\n\n\n\n\nGet familiar with Python’s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n\n\nLists and dictionaries\nif, for, and while statements\nWriting and calling functions\n\n\n\n\nConcept:\n- Lists are ordered collections. Each item has a position called an index (starting from 0).\n- You can get a single element using list[index] (e.g., scores[0] → first element).\n- You can get a slice using list[start:stop], which returns elements from start up to but not including stop.\n- Dictionaries store key–value pairs. You look up a value by its key (like a label).\n- You can add new keys or update existing ones using dict[key] = value.\n- Keys must be unique; values can be any data type.\n\n# A list of exam scores for Alice\nscores_alice = [88, 92, 79]\n\n# Indexing: position 0 is the first score\nprint(\"First score:\", scores_alice[0])\n\nFirst score: 88\n\n# Slicing: [:2] means start at index 0, stop before index 2 → positions 0 and 1\nprint(\"First two scores:\", scores_alice[:2])\n\nFirst two scores: [88, 92]\n\n# A dictionary mapping student names to their list of scores\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70],\n}\n\n# Lookup: use the key (student's name) to get the list of scores\nprint(\"Carol's scores:\", student_scores[\"Carol\"])\n\nCarol's scores: [90, 85, 95]\n\n# Add a new student by assigning to a new key\nstudent_scores[\"Eve\"] = [85, 88, 91]\n\n# Update an existing student's scores (overwrites the old list)\nstudent_scores[\"Bob\"] = [78, 84, 82]\n\n# Get all keys (student names) and values (lists of scores)\nprint(\"Students:\", list(student_scores.keys()))\n\nStudents: ['Alice', 'Bob', 'Carol', 'Dave', 'Eve']\n\nprint(\"Sample scores:\", list(student_scores.values())[:2])  # [:2] → first two values\n\nSample scores: [[88, 92, 79], [78, 84, 82]]\n\n\n\n\n\nConcept:\n- if / elif / else lets the program choose actions based on conditions.\n- for loops iterate over items in a collection, letting you process each element in turn.\nWe’ll use the student_scores dictionary from the previous section.\n\n# Example 1: if / elif / else\nx = 87\nif x &gt;= 90:\n    grade = \"A\"\nelif x &gt;= 80:\n    grade = \"B\"\nelse:\n    grade = \"C or below\"\nprint(\"Grade bucket:\", grade)\n\nGrade bucket: B\n\n# Example 2: for loop over a student's scores\n# Reusing the student_scores dictionary from earlier\nscores_alice = student_scores[\"Alice\"]  # a list of Alice's scores\ntotal = 0\nfor s in scores_alice:\n    total = total + s\naverage = total / len(scores_alice)\nprint(\"Alice's average score:\", average)\n\nAlice's average score: 86.33333333333333\n\n\n\n\n\nConcept:\n- A function is a reusable block of code that takes inputs (parameters) and can return an output.\n- Use def function_name(parameters): to define it.\n- Use return to send a result back to the caller.\n- You can reuse loops and calculations inside a function so you don’t repeat the same code.\n\ndef average_score(scores):\n    \"\"\"\n    Calculate the average from a list of scores.\n    \"\"\"\n    total = 0\n    for s in scores:\n        total = total + s\n    return total / len(scores)\n\n# Example: Calculate averages for all students\nfor name in student_scores:  # loops over keys (student names)\n    avg = average_score(student_scores[name])\n    print(f\"{name}: {avg:.2f}\")\n\nAlice: 86.33\nBob: 81.33\nCarol: 90.00\nDave: 73.33\nEve: 88.00\n\n\n\n\n\n\nConcept:\nNumPy arrays are like lists but optimized for fast mathematical operations.\n- 1D array → like a row of numbers.\n- 2D array → like a table (rows × columns).\n- .shape tells you the size of the array.\n\n# 1D array: vector of exam scores\nscores_1d = np.array([88, 92, 79])\nprint(\"1D array:\", scores_1d)\n\n1D array: [88 92 79]\n\nprint(\"Shape:\", scores_1d.shape)  # (3,) → 3 elements in 1 dimension\n\nShape: (3,)\n\n# 2D array: scores for two students across three exams\nscores_2d = np.array([\n    [88, 92, 79],  # student 1\n    [75, 83, 80]   # student 2\n])\nprint(\"\\n2D array:\\n\", scores_2d)\n\n\n2D array:\n [[88 92 79]\n [75 83 80]]\n\nprint(\"Shape:\", scores_2d.shape)  # (2, 3) → 2 rows, 3 columns\n\nShape: (2, 3)\n\n# Now convert student_scores dictionary values into a 2D array\ngrades_matrix = np.array(list(student_scores.values()))\nprint(\"\\nGrades matrix:\\n\", grades_matrix)\n\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)  # rows = students, columns = exams\n\nShape: (5, 3)\n\n\n\n\nConcept:\n- Indexing works like Python lists: array[row_index, col_index] (0-based).\n- Slicing lets you select a range: start:stop returns elements from start up to (but not including) stop.\n- You can slice rows, columns, or both.\n\n# Example array for reference\nprint(\"Grades matrix:\\n\", grades_matrix)\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\n# Get the score of the first student in the first exam\nprint(\"\\nFirst student's first exam score:\", grades_matrix[0, 0])\n\n\nFirst student's first exam score: 88\n\n# Get all exam scores for the second student (row index 1)\nprint(\"Second student's scores:\", grades_matrix[1, :])\n\nSecond student's scores: [78 84 82]\n\n# Get all scores for the third exam (column index 2)\nprint(\"Scores in third exam:\", grades_matrix[:, 2])\n\nScores in third exam: [79 82 95 70 91]\n\n# Slice: first two students' scores\nprint(\"First two students' scores:\\n\", grades_matrix[0:2, :])\n\nFirst two students' scores:\n [[88 92 79]\n [78 84 82]]\n\n# Slice: first two exams for all students\nprint(\"First two exams for all students:\\n\", grades_matrix[:, 0:2])\n\nFirst two exams for all students:\n [[88 92]\n [78 84]\n [90 85]\n [72 78]\n [85 88]]\n\n\n\n\n\nConcept:\n- Element-wise operations apply a calculation to each element of an array.\n- Broadcasting lets NumPy apply operations between arrays of different shapes by “stretching” one to match the other (without copying data).\n- This is much faster and cleaner than using Python loops.\n\n# Add 5 points to every score (element-wise addition)\nprint(\"Original:\\n\", grades_matrix)\n\nOriginal:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"\\n+5 to every score:\\n\", grades_matrix + 5)\n\n\n+5 to every score:\n [[ 93  97  84]\n [ 83  89  87]\n [ 95  90 100]\n [ 77  83  75]\n [ 90  93  96]]\n\n# Multiply all scores by 1.1 to simulate a 10% bonus\nprint(\"\\n10% bonus:\\n\", grades_matrix * 1.1)\n\n\n10% bonus:\n [[ 96.8 101.2  86.9]\n [ 85.8  92.4  90.2]\n [ 99.   93.5 104.5]\n [ 79.2  85.8  77. ]\n [ 93.5  96.8 100.1]]\n\n# Broadcasting: subtract the minimum score in each column (exam) from that column\nmin_scores_per_exam = grades_matrix.min(axis=0)  # shape: (n_exams,)\nprint(\"\\nMinimum scores per exam:\", min_scores_per_exam)\n\n\nMinimum scores per exam: [72 78 70]\n\nadjusted = grades_matrix - min_scores_per_exam  # broadcasting happens here\nprint(\"\\nScores adjusted by exam minimum:\\n\", adjusted)\n\n\nScores adjusted by exam minimum:\n [[16 14  9]\n [ 6  6 12]\n [18  7 25]\n [ 0  0  0]\n [13 10 21]]\n\n\n\n\n\nConcept:\n- Matrix multiplication (np.dot) combines rows and columns, often used in deep learning layers to combine inputs with weights.\n- Axis-based operations let you apply functions (mean, sum, etc.) across rows or columns:\n\naxis=0 → operate down columns (across rows)\naxis=1 → operate across columns (per row)\n\nExtra notes: - np.ones(shape) creates an array of ones with the given shape.\n- Here we use it for equal weights when averaging scores: each exam gets the same weight.\n- .flatten() converts a multi-dimensional array into a 1D array.\n- After matrix multiplication, the result might be shape (n_students, 1); flattening makes it easier to print and work with.\n\n\n\n“Matrix multiplication producing a column vector, then flattening to a 1D array. Here, each row of the grades matrix is multiplied by the weights vector to produce a single average score per student.”\n\n\n\n# Example: equal-weight average across exams (axis=1 → per student)\navg_scores_axis = grades_matrix.mean(axis=1)\nprint(\"Average score per student (axis=1):\", [f\"{x:.2f}\" for x in avg_scores_axis])\n\nAverage score per student (axis=1): ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Example: average score per exam (axis=0 → per exam)\navg_scores_exam = grades_matrix.mean(axis=0)\nprint(\"Average score per exam (axis=0):\", [f\"{x:.2f}\" for x in avg_scores_exam])\n\nAverage score per exam (axis=0): ['82.60', '85.40', '83.40']\n\n# Using matrix multiplication to compute averages\nn_exams = grades_matrix.shape[1]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\naverages_via_dot = np.dot(grades_matrix, weights).flatten()\nprint(\"\\nAverages via matrix multiplication:\",\n      [f\"{x:.2f}\" for x in averages_via_dot])\n\n\nAverages via matrix multiplication: ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Weighted sum example: suppose exams have weights 0.5, 0.3, 0.2\nexam_weights = np.array([0.5, 0.3, 0.2]).reshape(-1, 1)  # shape: (n_exams, 1)\nweighted_scores = np.dot(grades_matrix, exam_weights).flatten()\nprint(\"\\nWeighted average per student:\",\n      [f\"{x:.2f}\" for x in weighted_scores])\n\n\nWeighted average per student: ['87.40', '80.60', '89.50', '73.40', '87.10']\n\n\n\n\n\nConcept:\n- A NumPy array is efficient for numerical operations but has no column or row labels — you must remember indexes yourself.\n- A pandas DataFrame wraps a NumPy array with labels (row and column names), allowing:\n\nEasier indexing by name (df[\"Math\"]) instead of position.\nMixed data types in one table (numbers, text, dates).\nBuilt-in data inspection methods (.head(), .info(), .describe()).\n\nKey point: Deep learning libraries often use NumPy arrays internally, but pandas is more convenient for data cleaning and exploration.\n\n# Our NumPy grades_matrix (from Section 3)\nprint(\"NumPy array:\\n\", grades_matrix)\n\nNumPy array:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)\n\nShape: (5, 3)\n\n# Convert to DataFrame with labels\nexam_names = [\"Exam 1\", \"Exam 2\", \"Exam 3\"]\nstudent_names = list(student_scores.keys())\ngrades_df = pd.DataFrame(grades_matrix, index=student_names, columns=exam_names)\n\nprint(\"\\nDataFrame:\\n\", grades_df)\n\n\nDataFrame:\n        Exam 1  Exam 2  Exam 3\nAlice      88      92      79\nBob        78      84      82\nCarol      90      85      95\nDave       72      78      70\nEve        85      88      91\n\n# Accessing data\nprint(\"\\nScore of Carol in Exam 2 (by labels):\", grades_df.loc[\"Carol\", \"Exam 2\"])\n\n\nScore of Carol in Exam 2 (by labels): 85\n\nprint(\"Score of Carol in Exam 2 (by position):\", grades_matrix[2, 1])\n\nScore of Carol in Exam 2 (by position): 85\n\n# Quick stats for each exam\nprint(\"\\nExam averages:\\n\", grades_df.mean().round(2))\n\n\nExam averages:\n Exam 1    82.6\nExam 2    85.4\nExam 3    83.4\ndtype: float64\n\n\n\n\n\n\nConcept:\nIn real projects, we often load datasets from CSV or Excel files.\nPandas DataFrames are perfect for this stage because they:\n\nRead files directly into a labeled table.\nMake it easy to explore and summarize the data.\nAllow quick selection of features (X) and target labels (y) for model training.\n\nWe’ll load a planar dataset with two numeric features (x_coord, y_coord) and a binary label (label).\n\n# Load into DataFrame\nraw_df = pd.read_csv(data_path_here)\n\n# Inspect\nprint(\"Shape:\", raw_df.shape)\n\nShape: (400, 3)\n\nprint(\"\\nFirst 5 rows:\\n\", raw_df.head())\n\n\nFirst 5 rows:\n     x_coord   y_coord  label\n0  1.204442  3.576114      0\n1  0.158710 -1.482171      0\n2  0.095247 -1.279955      0\n3  0.349178 -2.064380      0\n4  0.694150  2.889109      0\n\n# Separate features and target\nfeatures = [\"x_coord\", \"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\ny = raw_df[target].copy()\n\nprint(\"\\nFeatures sample:\\n\", X.head())\n\n\nFeatures sample:\n     x_coord   y_coord\n0  1.204442  3.576114\n1  0.158710 -1.482171\n2  0.095247 -1.279955\n3  0.349178 -2.064380\n4  0.694150  2.889109\n\nprint(\"\\nLabels sample:\\n\", y.head())\n\n\nLabels sample:\n 0    0\n1    0\n2    0\n3    0\n4    0\nName: label, dtype: int64\n\n\n\n\nConcept:\nA scatter plot lets us see how the two features (x_coord, y_coord) relate to the class label.\nIf classes are not linearly separable, a simple logistic regression will likely underperform compared to a neural network.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\n\n# Bright, high-contrast colors\ncolors = {0: \"orange\", 1: \"teal\"}\n\n# Scatter plot\nfor label_value in sorted(y.unique()):\n    subset = X[y == label_value]\n    plt.scatter(\n        subset[\"x_coord\"], subset[\"y_coord\"],\n        c=colors[label_value],\n        edgecolor=\"k\",\n        s=50,\n        label=f\"Class {label_value}\"\n    )\n\nplt.xlabel(\"x_coord\")\nplt.ylabel(\"y_coord\")\nplt.title(\"Planar Dataset by Label\")\nplt.legend(title=\"Label\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nConcept:\nWe’ll train two models on the planar dataset:\n\nLogistic Regression — a single-layer model that produces a linear decision boundary.\nShallow Neural Network — one hidden layer that can model non-linear boundaries.\n\nThis comparison shows why neural networks can outperform linear models on complex patterns.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Logistic Regression model: single Dense layer\nlog_reg_model = Sequential([\n    Dense(1, activation='sigmoid', input_shape=(2,))\n])\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\nlog_reg_history = log_reg_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nlog_acc = log_reg_model.evaluate(X, y, verbose=0)[1]\n\n\nprint(f\"Logistic Regression Accuracy: {log_acc:.2f} \\n \")\n\nLogistic Regression Accuracy: 0.47 \n \n\n\n\n# Neural Network model: one hidden layer\nnn_model = Sequential([\n    Dense(10, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\nnn_history = nn_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nnn_acc = nn_model.evaluate(X, y, verbose=0)[1]\n\nprint(f\"Neural Network Accuracy:     {nn_acc:.2f}\")\n\nNeural Network Accuracy:     0.66\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract loss values\nlog_loss = log_reg_history.history['loss']\nnn_loss = nn_history.history['loss']\n\n# Extract accuracy values\nlog_acc_hist = log_reg_history.history['accuracy']\nnn_acc_hist = nn_history.history['accuracy']\n\nepochs_range = range(1, len(log_loss) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, log_loss, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_loss, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, log_acc_hist, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_acc_hist, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTo better understand how each model separates the two classes, we will define an auxiliary plotting function called plot_decision_boundary.\nThis function will: 1. Create a grid over the feature space. 2. Use the model to predict the class for each point in the grid. 3. Display the predicted regions as a colored background. 4. Overlay the actual data points on top.\nAfter defining this function, we will call it for both the logistic regression and neural network models to visually compare their decision boundaries.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y, title):\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[\"x_coord\"].min() - 1, X[\"x_coord\"].max() + 1\n    y_min, y_max = X[\"y_coord\"].min() - 1, X[\"y_coord\"].max() + 1\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 300),\n        np.linspace(y_min, y_max, 300)\n    )\n    \n    # Predict over the grid\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(grid_points, verbose=0)\n    Z = (Z &gt; 0.5).astype(int).reshape(xx.shape)\n\n    # Plot contour and points\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Oranges, alpha=0.3)\n    \n    colors = {0: \"orange\", 1: \"teal\"}\n    for label_value in sorted(y.unique()):\n        subset = X[y == label_value]\n        plt.scatter(\n            subset[\"x_coord\"], subset[\"y_coord\"],\n            c=colors[label_value],\n            edgecolor=\"k\",\n            s=50,\n            label=f\"Class {label_value}\"\n        )\n    plt.xlabel(\"x_coord\")\n    plt.ylabel(\"y_coord\")\n    plt.title(title)\n    plt.legend()\n\n\n# Plot for both models side by side\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplot_decision_boundary(log_reg_model, X, y, \"Logistic Regression Decision Boundary\")\n\nplt.subplot(1, 2, 2)\nplot_decision_boundary(nn_model, X, y, \"Neural Network Decision Boundary\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/python_workshop.html#introduction-environment-setup",
    "href": "topics/python_workshop.html#introduction-environment-setup",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "We will use a local Python environment via reticulate, which allows running Python code directly inside this Quarto document.\n\n\n\nWhy Python for Deep Learning\nSetting up your local Python environment\nReading local files using pandas\n\n\n\n\nRead a CSV file from your computer using pandas and display its first few rows.\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport os"
  },
  {
    "objectID": "topics/python_workshop.html#python-fundamentals",
    "href": "topics/python_workshop.html#python-fundamentals",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Get familiar with Python’s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n\n\nLists and dictionaries\nif, for, and while statements\nWriting and calling functions\n\n\n\n\nConcept:\n- Lists are ordered collections. Each item has a position called an index (starting from 0).\n- You can get a single element using list[index] (e.g., scores[0] → first element).\n- You can get a slice using list[start:stop], which returns elements from start up to but not including stop.\n- Dictionaries store key–value pairs. You look up a value by its key (like a label).\n- You can add new keys or update existing ones using dict[key] = value.\n- Keys must be unique; values can be any data type.\n\n# A list of exam scores for Alice\nscores_alice = [88, 92, 79]\n\n# Indexing: position 0 is the first score\nprint(\"First score:\", scores_alice[0])\n\nFirst score: 88\n\n# Slicing: [:2] means start at index 0, stop before index 2 → positions 0 and 1\nprint(\"First two scores:\", scores_alice[:2])\n\nFirst two scores: [88, 92]\n\n# A dictionary mapping student names to their list of scores\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70],\n}\n\n# Lookup: use the key (student's name) to get the list of scores\nprint(\"Carol's scores:\", student_scores[\"Carol\"])\n\nCarol's scores: [90, 85, 95]\n\n# Add a new student by assigning to a new key\nstudent_scores[\"Eve\"] = [85, 88, 91]\n\n# Update an existing student's scores (overwrites the old list)\nstudent_scores[\"Bob\"] = [78, 84, 82]\n\n# Get all keys (student names) and values (lists of scores)\nprint(\"Students:\", list(student_scores.keys()))\n\nStudents: ['Alice', 'Bob', 'Carol', 'Dave', 'Eve']\n\nprint(\"Sample scores:\", list(student_scores.values())[:2])  # [:2] → first two values\n\nSample scores: [[88, 92, 79], [78, 84, 82]]\n\n\n\n\n\nConcept:\n- if / elif / else lets the program choose actions based on conditions.\n- for loops iterate over items in a collection, letting you process each element in turn.\nWe’ll use the student_scores dictionary from the previous section.\n\n# Example 1: if / elif / else\nx = 87\nif x &gt;= 90:\n    grade = \"A\"\nelif x &gt;= 80:\n    grade = \"B\"\nelse:\n    grade = \"C or below\"\nprint(\"Grade bucket:\", grade)\n\nGrade bucket: B\n\n# Example 2: for loop over a student's scores\n# Reusing the student_scores dictionary from earlier\nscores_alice = student_scores[\"Alice\"]  # a list of Alice's scores\ntotal = 0\nfor s in scores_alice:\n    total = total + s\naverage = total / len(scores_alice)\nprint(\"Alice's average score:\", average)\n\nAlice's average score: 86.33333333333333\n\n\n\n\n\nConcept:\n- A function is a reusable block of code that takes inputs (parameters) and can return an output.\n- Use def function_name(parameters): to define it.\n- Use return to send a result back to the caller.\n- You can reuse loops and calculations inside a function so you don’t repeat the same code.\n\ndef average_score(scores):\n    \"\"\"\n    Calculate the average from a list of scores.\n    \"\"\"\n    total = 0\n    for s in scores:\n        total = total + s\n    return total / len(scores)\n\n# Example: Calculate averages for all students\nfor name in student_scores:  # loops over keys (student names)\n    avg = average_score(student_scores[name])\n    print(f\"{name}: {avg:.2f}\")\n\nAlice: 86.33\nBob: 81.33\nCarol: 90.00\nDave: 73.33\nEve: 88.00"
  },
  {
    "objectID": "topics/python_workshop.html#numerical-computing-with-numpy",
    "href": "topics/python_workshop.html#numerical-computing-with-numpy",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Concept:\nNumPy arrays are like lists but optimized for fast mathematical operations.\n- 1D array → like a row of numbers.\n- 2D array → like a table (rows × columns).\n- .shape tells you the size of the array.\n\n# 1D array: vector of exam scores\nscores_1d = np.array([88, 92, 79])\nprint(\"1D array:\", scores_1d)\n\n1D array: [88 92 79]\n\nprint(\"Shape:\", scores_1d.shape)  # (3,) → 3 elements in 1 dimension\n\nShape: (3,)\n\n# 2D array: scores for two students across three exams\nscores_2d = np.array([\n    [88, 92, 79],  # student 1\n    [75, 83, 80]   # student 2\n])\nprint(\"\\n2D array:\\n\", scores_2d)\n\n\n2D array:\n [[88 92 79]\n [75 83 80]]\n\nprint(\"Shape:\", scores_2d.shape)  # (2, 3) → 2 rows, 3 columns\n\nShape: (2, 3)\n\n# Now convert student_scores dictionary values into a 2D array\ngrades_matrix = np.array(list(student_scores.values()))\nprint(\"\\nGrades matrix:\\n\", grades_matrix)\n\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)  # rows = students, columns = exams\n\nShape: (5, 3)\n\n\n\n\nConcept:\n- Indexing works like Python lists: array[row_index, col_index] (0-based).\n- Slicing lets you select a range: start:stop returns elements from start up to (but not including) stop.\n- You can slice rows, columns, or both.\n\n# Example array for reference\nprint(\"Grades matrix:\\n\", grades_matrix)\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\n# Get the score of the first student in the first exam\nprint(\"\\nFirst student's first exam score:\", grades_matrix[0, 0])\n\n\nFirst student's first exam score: 88\n\n# Get all exam scores for the second student (row index 1)\nprint(\"Second student's scores:\", grades_matrix[1, :])\n\nSecond student's scores: [78 84 82]\n\n# Get all scores for the third exam (column index 2)\nprint(\"Scores in third exam:\", grades_matrix[:, 2])\n\nScores in third exam: [79 82 95 70 91]\n\n# Slice: first two students' scores\nprint(\"First two students' scores:\\n\", grades_matrix[0:2, :])\n\nFirst two students' scores:\n [[88 92 79]\n [78 84 82]]\n\n# Slice: first two exams for all students\nprint(\"First two exams for all students:\\n\", grades_matrix[:, 0:2])\n\nFirst two exams for all students:\n [[88 92]\n [78 84]\n [90 85]\n [72 78]\n [85 88]]\n\n\n\n\n\nConcept:\n- Element-wise operations apply a calculation to each element of an array.\n- Broadcasting lets NumPy apply operations between arrays of different shapes by “stretching” one to match the other (without copying data).\n- This is much faster and cleaner than using Python loops.\n\n# Add 5 points to every score (element-wise addition)\nprint(\"Original:\\n\", grades_matrix)\n\nOriginal:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"\\n+5 to every score:\\n\", grades_matrix + 5)\n\n\n+5 to every score:\n [[ 93  97  84]\n [ 83  89  87]\n [ 95  90 100]\n [ 77  83  75]\n [ 90  93  96]]\n\n# Multiply all scores by 1.1 to simulate a 10% bonus\nprint(\"\\n10% bonus:\\n\", grades_matrix * 1.1)\n\n\n10% bonus:\n [[ 96.8 101.2  86.9]\n [ 85.8  92.4  90.2]\n [ 99.   93.5 104.5]\n [ 79.2  85.8  77. ]\n [ 93.5  96.8 100.1]]\n\n# Broadcasting: subtract the minimum score in each column (exam) from that column\nmin_scores_per_exam = grades_matrix.min(axis=0)  # shape: (n_exams,)\nprint(\"\\nMinimum scores per exam:\", min_scores_per_exam)\n\n\nMinimum scores per exam: [72 78 70]\n\nadjusted = grades_matrix - min_scores_per_exam  # broadcasting happens here\nprint(\"\\nScores adjusted by exam minimum:\\n\", adjusted)\n\n\nScores adjusted by exam minimum:\n [[16 14  9]\n [ 6  6 12]\n [18  7 25]\n [ 0  0  0]\n [13 10 21]]\n\n\n\n\n\nConcept:\n- Matrix multiplication (np.dot) combines rows and columns, often used in deep learning layers to combine inputs with weights.\n- Axis-based operations let you apply functions (mean, sum, etc.) across rows or columns:\n\naxis=0 → operate down columns (across rows)\naxis=1 → operate across columns (per row)\n\nExtra notes: - np.ones(shape) creates an array of ones with the given shape.\n- Here we use it for equal weights when averaging scores: each exam gets the same weight.\n- .flatten() converts a multi-dimensional array into a 1D array.\n- After matrix multiplication, the result might be shape (n_students, 1); flattening makes it easier to print and work with.\n\n\n\n“Matrix multiplication producing a column vector, then flattening to a 1D array. Here, each row of the grades matrix is multiplied by the weights vector to produce a single average score per student.”\n\n\n\n# Example: equal-weight average across exams (axis=1 → per student)\navg_scores_axis = grades_matrix.mean(axis=1)\nprint(\"Average score per student (axis=1):\", [f\"{x:.2f}\" for x in avg_scores_axis])\n\nAverage score per student (axis=1): ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Example: average score per exam (axis=0 → per exam)\navg_scores_exam = grades_matrix.mean(axis=0)\nprint(\"Average score per exam (axis=0):\", [f\"{x:.2f}\" for x in avg_scores_exam])\n\nAverage score per exam (axis=0): ['82.60', '85.40', '83.40']\n\n# Using matrix multiplication to compute averages\nn_exams = grades_matrix.shape[1]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\naverages_via_dot = np.dot(grades_matrix, weights).flatten()\nprint(\"\\nAverages via matrix multiplication:\",\n      [f\"{x:.2f}\" for x in averages_via_dot])\n\n\nAverages via matrix multiplication: ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Weighted sum example: suppose exams have weights 0.5, 0.3, 0.2\nexam_weights = np.array([0.5, 0.3, 0.2]).reshape(-1, 1)  # shape: (n_exams, 1)\nweighted_scores = np.dot(grades_matrix, exam_weights).flatten()\nprint(\"\\nWeighted average per student:\",\n      [f\"{x:.2f}\" for x in weighted_scores])\n\n\nWeighted average per student: ['87.40', '80.60', '89.50', '73.40', '87.10']\n\n\n\n\n\nConcept:\n- A NumPy array is efficient for numerical operations but has no column or row labels — you must remember indexes yourself.\n- A pandas DataFrame wraps a NumPy array with labels (row and column names), allowing:\n\nEasier indexing by name (df[\"Math\"]) instead of position.\nMixed data types in one table (numbers, text, dates).\nBuilt-in data inspection methods (.head(), .info(), .describe()).\n\nKey point: Deep learning libraries often use NumPy arrays internally, but pandas is more convenient for data cleaning and exploration.\n\n# Our NumPy grades_matrix (from Section 3)\nprint(\"NumPy array:\\n\", grades_matrix)\n\nNumPy array:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)\n\nShape: (5, 3)\n\n# Convert to DataFrame with labels\nexam_names = [\"Exam 1\", \"Exam 2\", \"Exam 3\"]\nstudent_names = list(student_scores.keys())\ngrades_df = pd.DataFrame(grades_matrix, index=student_names, columns=exam_names)\n\nprint(\"\\nDataFrame:\\n\", grades_df)\n\n\nDataFrame:\n        Exam 1  Exam 2  Exam 3\nAlice      88      92      79\nBob        78      84      82\nCarol      90      85      95\nDave       72      78      70\nEve        85      88      91\n\n# Accessing data\nprint(\"\\nScore of Carol in Exam 2 (by labels):\", grades_df.loc[\"Carol\", \"Exam 2\"])\n\n\nScore of Carol in Exam 2 (by labels): 85\n\nprint(\"Score of Carol in Exam 2 (by position):\", grades_matrix[2, 1])\n\nScore of Carol in Exam 2 (by position): 85\n\n# Quick stats for each exam\nprint(\"\\nExam averages:\\n\", grades_df.mean().round(2))\n\n\nExam averages:\n Exam 1    82.6\nExam 2    85.4\nExam 3    83.4\ndtype: float64"
  },
  {
    "objectID": "topics/python_workshop.html#working-with-data",
    "href": "topics/python_workshop.html#working-with-data",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Concept:\nIn real projects, we often load datasets from CSV or Excel files.\nPandas DataFrames are perfect for this stage because they:\n\nRead files directly into a labeled table.\nMake it easy to explore and summarize the data.\nAllow quick selection of features (X) and target labels (y) for model training.\n\nWe’ll load a planar dataset with two numeric features (x_coord, y_coord) and a binary label (label).\n\n# Load into DataFrame\nraw_df = pd.read_csv(data_path_here)\n\n# Inspect\nprint(\"Shape:\", raw_df.shape)\n\nShape: (400, 3)\n\nprint(\"\\nFirst 5 rows:\\n\", raw_df.head())\n\n\nFirst 5 rows:\n     x_coord   y_coord  label\n0  1.204442  3.576114      0\n1  0.158710 -1.482171      0\n2  0.095247 -1.279955      0\n3  0.349178 -2.064380      0\n4  0.694150  2.889109      0\n\n# Separate features and target\nfeatures = [\"x_coord\", \"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\ny = raw_df[target].copy()\n\nprint(\"\\nFeatures sample:\\n\", X.head())\n\n\nFeatures sample:\n     x_coord   y_coord\n0  1.204442  3.576114\n1  0.158710 -1.482171\n2  0.095247 -1.279955\n3  0.349178 -2.064380\n4  0.694150  2.889109\n\nprint(\"\\nLabels sample:\\n\", y.head())\n\n\nLabels sample:\n 0    0\n1    0\n2    0\n3    0\n4    0\nName: label, dtype: int64\n\n\n\n\nConcept:\nA scatter plot lets us see how the two features (x_coord, y_coord) relate to the class label.\nIf classes are not linearly separable, a simple logistic regression will likely underperform compared to a neural network.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\n\n# Bright, high-contrast colors\ncolors = {0: \"orange\", 1: \"teal\"}\n\n# Scatter plot\nfor label_value in sorted(y.unique()):\n    subset = X[y == label_value]\n    plt.scatter(\n        subset[\"x_coord\"], subset[\"y_coord\"],\n        c=colors[label_value],\n        edgecolor=\"k\",\n        s=50,\n        label=f\"Class {label_value}\"\n    )\n\nplt.xlabel(\"x_coord\")\nplt.ylabel(\"y_coord\")\nplt.title(\"Planar Dataset by Label\")\nplt.legend(title=\"Label\")\nplt.show()"
  },
  {
    "objectID": "topics/python_workshop.html#logistic-regression-vs-neural-network",
    "href": "topics/python_workshop.html#logistic-regression-vs-neural-network",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Concept:\nWe’ll train two models on the planar dataset:\n\nLogistic Regression — a single-layer model that produces a linear decision boundary.\nShallow Neural Network — one hidden layer that can model non-linear boundaries.\n\nThis comparison shows why neural networks can outperform linear models on complex patterns.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Logistic Regression model: single Dense layer\nlog_reg_model = Sequential([\n    Dense(1, activation='sigmoid', input_shape=(2,))\n])\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\nlog_reg_history = log_reg_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nlog_acc = log_reg_model.evaluate(X, y, verbose=0)[1]\n\n\nprint(f\"Logistic Regression Accuracy: {log_acc:.2f} \\n \")\n\nLogistic Regression Accuracy: 0.47 \n \n\n\n\n# Neural Network model: one hidden layer\nnn_model = Sequential([\n    Dense(10, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\nnn_history = nn_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nnn_acc = nn_model.evaluate(X, y, verbose=0)[1]\n\nprint(f\"Neural Network Accuracy:     {nn_acc:.2f}\")\n\nNeural Network Accuracy:     0.66\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract loss values\nlog_loss = log_reg_history.history['loss']\nnn_loss = nn_history.history['loss']\n\n# Extract accuracy values\nlog_acc_hist = log_reg_history.history['accuracy']\nnn_acc_hist = nn_history.history['accuracy']\n\nepochs_range = range(1, len(log_loss) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, log_loss, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_loss, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, log_acc_hist, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_acc_hist, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTo better understand how each model separates the two classes, we will define an auxiliary plotting function called plot_decision_boundary.\nThis function will: 1. Create a grid over the feature space. 2. Use the model to predict the class for each point in the grid. 3. Display the predicted regions as a colored background. 4. Overlay the actual data points on top.\nAfter defining this function, we will call it for both the logistic regression and neural network models to visually compare their decision boundaries.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y, title):\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[\"x_coord\"].min() - 1, X[\"x_coord\"].max() + 1\n    y_min, y_max = X[\"y_coord\"].min() - 1, X[\"y_coord\"].max() + 1\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 300),\n        np.linspace(y_min, y_max, 300)\n    )\n    \n    # Predict over the grid\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(grid_points, verbose=0)\n    Z = (Z &gt; 0.5).astype(int).reshape(xx.shape)\n\n    # Plot contour and points\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Oranges, alpha=0.3)\n    \n    colors = {0: \"orange\", 1: \"teal\"}\n    for label_value in sorted(y.unique()):\n        subset = X[y == label_value]\n        plt.scatter(\n            subset[\"x_coord\"], subset[\"y_coord\"],\n            c=colors[label_value],\n            edgecolor=\"k\",\n            s=50,\n            label=f\"Class {label_value}\"\n        )\n    plt.xlabel(\"x_coord\")\n    plt.ylabel(\"y_coord\")\n    plt.title(title)\n    plt.legend()\n\n\n# Plot for both models side by side\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplot_decision_boundary(log_reg_model, X, y, \"Logistic Regression Decision Boundary\")\n\nplt.subplot(1, 2, 2)\nplot_decision_boundary(nn_model, X, y, \"Neural Network Decision Boundary\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/intro.html",
    "href": "topics/intro.html",
    "title": "Moving beyond linearity",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom patsy import dmatrix\n\nfrom sklearn.linear_model import LinearRegression\n\nimport pygam\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\Wage.csv\"))\n\ny_vec = raw_df[\"wage\"].copy()\n\nx_vec = raw_df[[\"age\"]].copy()\n\n\n\n\nShow the code\n\ndef plot_predictions(pred_df, title):\n  pred_cols = [temp_name for temp_name in pred_df.columns.values if \"pred\" in temp_name]\n\n  plt.figure(figsize=(10, 6))\n  plt.scatter(pred_df[\"age\"], pred_df[\"wage\"], label=\"Actual Wage\", alpha = 0.7)\n\n  for temp_name in pred_cols:\n    plt.scatter(pred_df[\"age\"], pred_df[temp_name], label=temp_name, alpha = 0.7)\n#  plt.scatter(pred_df[\"age\"], pred_df[\"wage_pred\"], label=\"Predicted Wage\", alpha = 0.7)\n  plt.xlabel(\"Age\")\n  plt.ylabel(\"Wage\")\n  plt.title(title)\n  plt.legend()\n  plt.grid(True)\n  plt.show()\n\n\n\nPolynomial regression\n\nConstruct polynomial features:\n\nUsing the dmatrix function, create a feature matrix that includes polynomial terms up to the fourth degree: \\(\\text{age}, \\text{age}^2, \\text{age}^3, \\text{age}^4\\)\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec from the polynomial features.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\n\n\nShow the code\npoly_x_mat = dmatrix(\"age + I(age**2) + I(age**3) + I(age**4)\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(poly_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\npoly_pred = lin_reg.predict(poly_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": poly_pred}),\n                 \"Polynomial Regression\")\n\n\n\n\n\n\n\n\n\n\n\nStep functions\n\nDefine step function intervals (bins):\n\nUse np.percentile to determine cutoff points (knots) that divide age into quartiles.\n\nUse pd.cut to assign each age value to a bin.\n\nCreate a step function design matrix:\n\nUse the dmatrix function to encode the binned age values as categorical variables.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the step function representation of age.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\nHints:\n* Ensure that the step function bins do not have duplicate edges, which can be handled using duplicates='drop' in pd.cut.\n* You can use matplotlib.pyplot or seaborn to visualize the stepwise fitted function.\n* Since this is a piecewise constant model, expect a regression curve with horizontal segments rather than a smooth curve.\n\n\nShow the code\nknots = np.percentile(x_vec[\"age\"], [0, 15, 25, 50, 75,90, 100])\n\nx_vec_step = pd.cut(x_vec[\"age\"], bins=knots, labels=False,\n                    include_lowest=True,duplicates='drop')\n\nstep_x_mat = dmatrix(\"C(x_vec_step)\", x_vec_step)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(step_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nstep_pred = lin_reg.predict(step_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": step_pred}), \"Step Regression\")\n\n\n\n\n\n\n\n\n\n\n\nPiecewise polynomials\n\nDefine the knot location:\n\nSet a single knot at age = 50 to allow for a change in the polynomial relationship at this point.\n\nCreate piecewise polynomial terms:\n\n\nConstruct polynomial terms (age, age², age³) for the entire dataset.\nDefine separate squared and cubic terms for values below and above the knot to allow for discontinuity.\nUse np.maximum to ensure that terms are active only in their respective regions.\n\n\nCreate a design matrix:\n\nCombine all polynomial terms into a matrix that serves as input for the regression model.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the spline-transformed age values.\n\nMake predictions:\nUse the trained model to generate predictions for y_vec.\n\nHints:\n* B-splines create smooth, continuous fits by combining piecewise polynomial segments.\n* You can experiment with additional knots to see how the flexibility of the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curve.\n\n\nShow the code\nknot = 50\n\n# Manually create piecewise terms to enforce discontinuity at the knot\nx_less_knot = np.maximum(0, knot - x_vec)  # For x &lt; knot\nx_greater_knot = np.maximum(0, x_vec - knot)  # For x &gt;= knot\n\n# Combine the terms into a design matrix (manual spline basis)\nspline_x_mat = np.column_stack((x_vec, x_vec**2, x_vec**3, x_less_knot**2,\n                                x_greater_knot**2,x_less_knot**3, x_greater_knot**3))\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": spline_pred}),\n\"Piecewise Spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nSplines\n\nConstruct spline basis matrices:\n\nUse the dmatrix function to create a B-spline basis with knots at age = 25, 40, 60 and a polynomial degree of 3.\nUse the dmatrix function to create a natural spline basis with the same knots.\n\nFit linear regression models:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit one model using the B-spline basis and another using the natural spline basis.\n\nMake predictions:\nUse each trained model to generate predictions for y_vec.\n\nHints:\n* B-splines allow local flexibility, adjusting the curve within defined knots.\n* Natural splines impose additional constraints that make the curve behave more smoothly at the boundaries.\n* Try adjusting the number and position of knots to see how the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curves.\n\n\nShow the code\nspline_x_mat = dmatrix(\"bs(age, knots = [25,40,60], degree=3)\", x_vec)\n\nnatural_spline_x_mat = dmatrix(\"cr(age, knots = [25,40,60])\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(natural_spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nnatural_spline_pred = lin_reg.predict(natural_spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec,\n                               \"spline_pred\": spline_pred,\n                               \"natural_spline_pred\": natural_spline_pred}),\n                               \"Spline and Natural spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nGeneral Additive Models\n\nPrepare features\n\nKeep year and age as numeric.\n\nEncode education as a categorical variable.\n\nSpecify the model\n\nUse smooth terms s() for year and age.\n\nUse f() for the categorical factor education.\n\nFit the model\n\nFit a LinearGAM to the training data.\n\n\n\n\nShow the code\nfrom pygam import LinearGAM, s, f\n\n# Encode categorical variable 'education'\nraw_df['education_code'] = raw_df['education'].astype('category').cat.codes\n\n# Features and response\nX = raw_df[['year', 'age', 'education_code']].values\n\ny = raw_df['wage'].values\n\n# Fit the GAM model\n\ngam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=8) + f(2));\n\ngam.fit(X, y);\n\n\n\nVisualize the results\n\nPlot partial dependence for each term:\n\nsmooth curve for year\n\nsmooth curve for age\n\nbar chart for education\n\n\n\n\n\nShow the code\n# Generate smooth predictions for each term\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot the effect of year\nyear_grid = np.linspace(raw_df['year'].min(), raw_df['year'].max(), 100)\nX_year = np.zeros((100, 3))\nX_year[:, 0] = year_grid\naxes[0].plot(year_grid, gam.partial_dependence(0, X=X_year), color='red');\naxes[0].set_title(r\"$f_1(\\mathrm{year})$\");\naxes[0].set_xlabel(\"year\");\naxes[0].set_ylabel(\"Effect\");\naxes[0].set_ylim(-30, 30);\n\n# Plot the effect of age\nage_grid = np.linspace(raw_df['age'].min(), raw_df['age'].max(), 100)\nX_age = np.zeros((100, 3))\nX_age[:, 1] = age_grid\naxes[1].plot(age_grid, gam.partial_dependence(1, X=X_age), color='red');\naxes[1].set_title(r\"$f_2(\\mathrm{age})$\");\naxes[1].set_xlabel(\"age\");\naxes[1].set_ylabel(\"Effect\");\naxes[1].set_ylim(-50, 40);\n\n# Plot the effect of education\neducation_levels = np.sort(raw_df['education_code'].unique())\nX_edu = np.zeros((len(education_levels), 3))\nX_edu[:, 2] = education_levels\naxes[2].bar(\n    raw_df['education'].astype('category').cat.categories,\n    gam.partial_dependence(2, X=X_edu).flatten(),\n    color='red'\n);\naxes[2].set_title(r\"$f_3(\\mathrm{education})$\");\naxes[2].set_xlabel(\"education\");\naxes[2].set_ylabel(\"Effect\");\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Python workshop\n\n\n\n\nMoving beyond linearity\nSingle layer\n\nSingle layer – classification\nSingle layer – regression\n\nOptimization method\nConvolution network"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Moving beyond linearity\nSingle layer\n\nSingle layer – classification\nSingle layer – regression\n\nOptimization method\nConvolution network"
  },
  {
    "objectID": "topics/convolution.html",
    "href": "topics/convolution.html",
    "title": "Convolutional Neural Network (CNN) for Image Classification",
    "section": "",
    "text": "Build, train, and evaluate a Convolutional Neural Network using the Sign Language MNIST dataset.\n\nImport and Load Data\n\nImport necessary libraries, including TensorFlow and Keras layers.\nLoad the training and test datasets from CSV files.\n\n\n\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.models import Sequential\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n\n\n\nShow the code\n\ntrain_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_train.csv\"))\n\ntest_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_test.csv\"))\n\n\n\n\nPreprocessing\n\nCreate a function to preprocess pixel values: normalize and reshape to (28, 28, 1).\nCreate a function to preprocess labels: remap classes to account for the missing letter ‘J’ and one-hot encode them.\nApply the preprocessing functions to the training and test sets.\n\n\n\nShow the code\n\ndef preprocess_data(df, img_height=28, img_width=28):\n  \n  processed_df = df / 255.0\n\n  processed_df = processed_df.values.reshape(-1, img_height, img_width, 1).copy()\n\n  return processed_df\n\n\ndef preprocess_labels(label_series, num_classes=24):\n    \"\"\"\n    Remaps labels to skip index 9 (J) and applies one-hot encoding.\n\n    Parameters:\n    - label_series: a pandas Series or 1D array of labels (originally 0–25, with 9 missing)\n    - num_classes: total number of actual classes (default 24)\n\n    Returns:\n    - One-hot encoded labels of shape (n_samples, num_classes)\n    \"\"\"\n    labels = np.array(label_series)\n\n    remapped_labels = np.array([l - 1 if l &gt; 9 else l for l in labels])\n\n    categorical_labels = to_categorical(remapped_labels, num_classes=num_classes)\n\n    return categorical_labels\n\n\n# Reverse the earlier remapping: add 1 to all labels ≥ 9\ndef reverse_remap(labels):\n    return [l + 1 if l &gt;= 9 else l for l in labels]\n\n\n\ndef show_predictions(x_data, y_true, y_pred, indices=None, n=6):\n    if indices is None:\n        indices = np.random.choice(len(x_data), n, replace=False)\n\n    plt.figure(figsize=(12, 6))\n    for i, idx in enumerate(indices):\n        plt.subplot(2, n // 2, i + 1)\n        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')\n        plt.title(f\"Pred: {y_pred[idx]}\\nTrue: {y_true[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nShow the code\n\n\n# Separate labels\ny_train = preprocess_labels(train_set['label'])\n\ny_test = preprocess_labels(test_set['label'])\n\n# Remove labels from the pixel data\nx_train = preprocess_data(train_set.drop('label', axis=1))\n\nx_test = preprocess_data(test_set.drop('label', axis=1))\n\n\n\n\nDefine CNN Model\n\nConstruct a Sequential model with:\n\nA Conv2D layer (32 filters, 3×3 kernel, ReLU, padding=‘same’).\nA MaxPooling2D layer (2×2).\nAnother Conv2D + MaxPooling2D block with 64 filters.\nA Flatten layer.\nA fully connected Dense layer with 128 units and ReLU activation.\nA Dropout layer with rate 0.3.\nA final Dense layer with 24 units and softmax activation.\n\nCompile the model using the Adam optimizer and categorical cross-entropy loss.\n\n\n\nShow the code\n\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(24, activation='softmax')  # 24 because of label remapping\n])\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\n\n\nTrain the Model\n\nFit the model on the training data for 5 epochs with batch size 128.\nUse the test data as validation during training.\n\n\n\nShow the code\n\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_test, y_test),\n    epochs=5,\n    batch_size=128,\n    verbose = 0\n)\n\n\n\n\nEvaluate the Model\n\nEvaluate the model on the test data.\nReport test accuracy.\n\n\n\nShow the code\nloss, accuracy = model.evaluate(x_test, y_test)\n\n\nShow the code\nprint(f\"Test Accuracy: {accuracy:.4f}\")"
  },
  {
    "objectID": "topics/optim_methods.html",
    "href": "topics/optim_methods.html",
    "title": "Optimization Methods in Neural Networks",
    "section": "",
    "text": "Implement and evaluate different optimization strategies for training a simple neural network"
  },
  {
    "objectID": "topics/optim_methods.html#model",
    "href": "topics/optim_methods.html#model",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nlayer_dims = [2, 4, 1]\n\ngd_parameters, gd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions",
    "href": "topics/optim_methods.html#predictions",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_gd = predict_nn(X, gd_parameters)\n\ngd_score = accuracy_score(pred_gd.flatten(),y.flatten())\n\nprint(f\"GD accuracy score is {gd_score}\")\n\n\nGD accuracy score is 0.818"
  },
  {
    "objectID": "topics/optim_methods.html#model-1",
    "href": "topics/optim_methods.html#model-1",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nsgd_parameters, sgd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=32,\n    print_cost=False,\n    print_every=200\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-1",
    "href": "topics/optim_methods.html#predictions-1",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_sgd = predict_nn(X, sgd_parameters)\n\nsgd_score = accuracy_score(pred_sgd.flatten(),y.flatten())\n\nprint(f\"GD accuracy score is {sgd_score}\")\n\n\nGD accuracy score is 0.887"
  },
  {
    "objectID": "topics/optim_methods.html#model-2",
    "href": "topics/optim_methods.html#model-2",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nmomentum_parameters, momentum_costs = model(\n    X, y, layer_dims,\n    optimizer=\"momentum\",\n    learning_rate=0.01,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta = 0.9\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-2",
    "href": "topics/optim_methods.html#predictions-2",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_momentum = predict_nn(X, momentum_parameters)\n\nmomentum_score = accuracy_score(pred_momentum.flatten(),y.flatten())\n\nprint(f\"Momentum accuracy score is {momentum_score}\")\n\n\nMomentum accuracy score is 0.799"
  },
  {
    "objectID": "topics/optim_methods.html#model-3",
    "href": "topics/optim_methods.html#model-3",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nrmsprop_parameters, rmsprop_costs = model(\n    X, y, layer_dims,\n    optimizer=\"rmsprop\",\n    learning_rate=0.001,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta2 = 0.9,\n    epsilon = 1 * 10 ** (-8)\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-3",
    "href": "topics/optim_methods.html#predictions-3",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_rmsprop = predict_nn(X, rmsprop_parameters)\n\nrmsprop_score = accuracy_score(pred_rmsprop.flatten(),y.flatten())\n\nprint(f\"RMSProp accuracy score is {rmsprop_score}\")\n\n\nRMSProp accuracy score is 0.866"
  },
  {
    "objectID": "topics/optim_methods.html#model-4",
    "href": "topics/optim_methods.html#model-4",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nadam_parameters, adam_costs = model(\n    X, y, layer_dims,\n    optimizer=\"adam\",\n    learning_rate=0.001,\n    num_epochs=2000,\n    batch_size=None,\n    print_cost=False,\n    print_every=200,\n    beta1 = 0.9,\n    beta2 = 0.999,\n    epsilon = 1 * 10 ** (-8)\n)"
  },
  {
    "objectID": "topics/optim_methods.html#predictions-4",
    "href": "topics/optim_methods.html#predictions-4",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_adam = predict_nn(X, adam_parameters)\n\nadam_score = accuracy_score(pred_adam.flatten(),y.flatten())\n\nprint(f\"Adam accuracy score is {adam_score}\")\n\n\nAdam accuracy score is 0.867"
  },
  {
    "objectID": "topics/single_layer_nn_classification.html",
    "href": "topics/single_layer_nn_classification.html",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "",
    "text": "Implement a simple neural network from scratch and compare its performance to logistic regression on a 2D dataset.\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os"
  },
  {
    "objectID": "topics/single_layer_nn_classification.html#auxiliary-functions",
    "href": "topics/single_layer_nn_classification.html#auxiliary-functions",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nImplement Training Functions\n\nDefine helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.\n\n\n\nShow the activation function code\nimport numpy as np\n\n\ndef activate(Z, activation_function=\"tanh\"):\n    \"\"\"\n    Apply an activation function elementwise.\n    \"\"\"\n    if activation_function == \"tanh\":\n        return np.tanh(Z)  # squashes values to [-1, 1]\n    elif activation_function == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-Z))  # squashes values to [0, 1]\n    else:\n        raise ValueError(\"activation_function must be 'tanh' or 'sigmoid'.\")\n\n\n\n\nShow the parameters initialization code\nimport numpy as np\n\n\ndef initialize_parameters(X, num_hidden_layer_neurons, scale_const=0.01, seed=1):\n    \"\"\"\n    Initialize weights and biases for a single hidden-layer network.\n    \"\"\"\n    np.random.seed(seed)\n\n    n_features = X.shape[1]  # number of input features\n\n    # Small random weights help avoid saturation of activations at start\n    W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n    b1 = np.zeros((num_hidden_layer_neurons, 1))\n    W2 = np.random.randn(1, num_hidden_layer_neurons) * scale_const\n    b2 = np.zeros((1, 1))\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\n\n\nForward and backward propagation in a single-hidden-layer neural network. The forward pass takes inputs \\(X\\) through weights \\(W^{[1]}, W^{[2]}\\) and biases \\(b^{[1]}, b^{[2]}\\) to produce pre-activations \\(Z^{[1]}, Z^{[2]}\\), activations \\(A^{[1]}, A^{[2]}\\), and final output \\(y\\). The backward pass computes gradients of activations, pre-activations, weights, and biases \\((dA, dZ, dW, db)\\) from the output layer back to the input layer for parameter updates.\n\n\n\n\nShow the forward propagation code\nimport numpy as np\n\n\ndef forward_propagation(parameters, X_adj):\n    \"\"\"\n    Perform one forward pass.\n    \"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n\n    Z1 = np.dot(W1, X_adj) + b1  # linear transform: hidden layer\n    A1 = activate(Z1, \"tanh\")    # non-linear activation for hidden layer\n    Z2 = np.dot(W2, A1) + b2     # linear transform: output layer\n    A2 = activate(Z2, \"sigmoid\") # probability output for binary classification\n\n    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n\n\n\n\nShow the backward propagation code\nimport numpy as np\n\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n    \"\"\"\n    Compute gradients for parameters using backpropagation.\n    \"\"\"\n    N = X_adj.shape[1]  # number of samples\n\n    W2 = parameters[\"W2\"]\n    A1, A2 = forward_propagation_values[\"A1\"], forward_propagation_values[\"A2\"]\n\n    dZ2 = A2 - Y_adj                      # derivative of loss w.r.t. Z2 (sigmoid+BCE Loss)\n    dW2 = np.dot(dZ2, A1.T) / N           # gradient for W2\n    db2 = np.sum(dZ2, axis=1, keepdims=True) / N  # gradient for b2\n\n    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2) # backprop through tanh: derivative is 1 - A1^2\n    dW1 = np.dot(dZ1, X_adj.T) / N        # gradient for W1\n    db1 = np.sum(dZ1, axis=1, keepdims=True) / N  # gradient for b1\n\n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n\n\n\n\nShow the parameters update code\nimport numpy as np\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n    \"\"\"\n    Update parameters using gradient descent.\n    \"\"\"\n    # subtract learning_rate * gradient for each parameter\n    W1 = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n    b1 = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n    W2 = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n    b2 = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\nCreate a wrapper function train_neural_network to run the training loop.\n\n\n\nShow the neural network training code\ndef train_neural_network(X, Y, num_iterations, num_hidden_layer_neurons=4):\n    \"\"\"\n    Trains a simple 1-hidden-layer neural network using gradient descent.\n    \"\"\"\n    # Transpose X so columns are examples, reshape Y to row vector\n    X_adj = X.T.copy()                         \n    Y_adj = Y.values.reshape(1, -1).copy()     \n\n    # Initialize weights and biases\n    parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n    for iteration in range(num_iterations):\n        # Forward pass\n        forward_values = forward_propagation(parameters, X_adj.copy())\n\n        # Backward pass\n        grads = backward_propagation(parameters, forward_values, X_adj.copy(), Y_adj.copy())\n\n        # Parameter update\n        parameters = update_parameters(parameters, grads)\n\n    return parameters\n\n\n\n\nImplement Prediction Function\n\nCreate a predict function that runs forward propagation and thresholds outputs.\n\n\n\nShow the code\ndef predict(nn_parameters, X, threshold=0.5):\n    \"\"\"\n    Generates binary predictions from a trained neural network.\n    \"\"\"\n    # Transpose X so columns are examples\n    X_adj = X.T.copy()\n\n    # Forward pass to get output layer activations\n    forward_values = forward_propagation(nn_parameters, X_adj.copy())\n\n    # Output probabilities from sigmoid\n    y_pred = forward_values[\"A2\"]\n\n    # Apply threshold to get class labels {0,1}\n    y_pred = y_pred &gt; threshold\n\n    return y_pred.astype(int).ravel()  # flatten to 1D array"
  },
  {
    "objectID": "topics/single_layer_nn_classification.html#derivation-sigmoid-binary-cross-entropy",
    "href": "topics/single_layer_nn_classification.html#derivation-sigmoid-binary-cross-entropy",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "Derivation: Sigmoid + Binary Cross-Entropy",
    "text": "Derivation: Sigmoid + Binary Cross-Entropy\nWe can show that using a sigmoid activation in the output layer with binary cross-entropy (BCE) loss leads to a very simple gradient formula.\nGoal \\[\n\\frac{\\partial L}{\\partial z} = a - y\n\\]\nSetup For a single example with label \\(y \\in \\{0,1\\}\\): \\[\nz = W^{[2]} a^{[1]} + b^{[2]}, \\qquad a = \\sigma(z) = \\frac{1}{1+e^{-z}}\n\\] The BCE loss for this example is: \\[\n\\ell(a,y) = -\\big[y \\log a + (1-y)\\log(1-a)\\big]\n\\] For a batch of \\(N\\) examples: \\[\nL = \\frac{1}{N}\\sum_{i=1}^N \\ell\\!\\big(a^{(i)}, y^{(i)}\\big)\n\\]\nStep 1 (loss wrt \\(a\\)) \\[\n\\frac{\\partial \\ell}{\\partial a}\n= -\\!\\left(\\frac{y}{a} - \\frac{1-y}{1-a}\\right)\n= \\frac{a - y}{a(1-a)}\n\\]\nStep 2 (sigmoid derivative) \\[\n\\frac{\\partial a}{\\partial z} = a(1-a)\n\\]\nStep 3 (chain rule) \\[\n\\frac{\\partial \\ell}{\\partial z}\n= \\frac{\\partial \\ell}{\\partial a}\\cdot\\frac{\\partial a}{\\partial z}\n= \\frac{a - y}{a(1-a)} \\cdot a(1-a)\n= a - y\n\\]\nBatch form \\[\n\\frac{\\partial L}{\\partial Z} = \\frac{1}{N}(A - Y)\n\\]\nCode correspondence\n\ndZ2 = A2 - Y_adj\n\ndW2 = np.dot(dZ2, A1.T) / N\n\ndb2 = np.sum(dZ2, axis=1, keepdims=True) / N"
  }
]