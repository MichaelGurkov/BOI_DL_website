[
  {
    "objectID": "topics/transformers/fine_tuning.html",
    "href": "topics/transformers/fine_tuning.html",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "",
    "text": "Show the code\n\n#!pip install -q transformers datasets accelerate\n\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast\nfrom collections import Counter\n\ndataset = load_dataset(\"tweet_eval\", \"sentiment\")\n\nN_train = 2000\nN_val   = 500\n\ndataset[\"train\"] = dataset[\"train\"].select(range(N_train))\n\ndataset[\"validation\"] = dataset[\"validation\"].select(range(N_val))\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#dataset-tweeteval-sentiment",
    "href": "topics/transformers/fine_tuning.html#dataset-tweeteval-sentiment",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Dataset: TweetEval Sentiment",
    "text": "Dataset: TweetEval Sentiment\nFor this tutorial we use the TweetEval Sentiment dataset, a benchmark collection of tweets labeled for sentiment analysis. The dataset was introduced as part of the TweetEval benchmark, which unifies several Twitter NLP tasks under a common framework.\nThe sentiment subset contains tweets annotated with three classes:\n\n0 — Negative\n1 — Neutral\n2 — Positive\n\nTweetEval is well-suited for fine-tuning transformer models because:\n\nTweets contain slang, emojis, irregular grammar, abbreviations, and sarcasm.\nThese characteristics make the domain challenging for general pre-trained models.\nFine-tuning allows BERT to adapt from formal text (BooksCorpus, Wikipedia) to the informal, noisy style of Twitter.\n\n\nDataset Size (Before Downsampling)\nThe full dataset includes approximately:\n\n45,000 tweets for training\n\n12,000 tweets for validation\n\n2,000 tweets for testing\n\nSince full training can be slow during development, this notebook uses a downsampled version"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#tokenization",
    "href": "topics/transformers/fine_tuning.html#tokenization",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Tokenization",
    "text": "Tokenization\nBefore we can feed text into BERT, we must convert each tweet into the numerical format that the model expects. BERT does not work directly with raw strings—it operates on token IDs and attention masks. The tokenizer performs this conversion and applies several important preprocessing steps.\nBERT uses a WordPiece tokenizer, which breaks text into subword units. This allows the model to handle noisy and informal language often found in tweets, including slang, abbreviations, hashtags, and even misspellings. If a word is not in the vocabulary, it is decomposed into smaller subwords that BERT can still interpret meaningfully.\nDuring tokenization, the tokenizer also:\n\nAdds special tokens such as [CLS] (classification token) and [SEP] (separator).\nTruncates sequences so they fit within a fixed maximum length (64 tokens in this tutorial).\nPads shorter sequences so all inputs are the same length.\nBuilds an attention mask, which tells BERT which tokens are real and which are padding.\n\nAfter tokenization, each example in the dataset contains:\n\ninput_ids: the numerical tokens representing the text\n\nattention_mask: indicators showing which positions should be attended to\n\nlabel: the sentiment class\n\nWe apply the tokenizer to the entire dataset using the map() function and then reformat the output so it can be used directly with PyTorch.\n\n\nShow the code\ndef tokenize_batch(batch):\n    return tokenizer(\n        batch[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=64\n    )\n\ntokenized_dataset = dataset.map(tokenize_batch, batched=True)\n\n\nShow the code\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n\ntokenized_dataset.set_format(\n    type=\"torch\",\n    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n)\n\ntokenized_dataset[\"train\"][0]"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#preparing-dataloaders",
    "href": "topics/transformers/fine_tuning.html#preparing-dataloaders",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Preparing DataLoaders",
    "text": "Preparing DataLoaders\nAfter tokenization, the dataset contains all the components BERT needs—input_ids, attention_mask, and label. The next step is to prepare these examples for efficient training.\nNeural networks in PyTorch expect data to be provided through DataLoaders, which handle batching, shuffling, and iteration over the dataset. This is especially important for transformer models, where training is computationally intensive and must be performed in batches that fit into GPU memory.\nWe construct two DataLoaders:\n\nTraining DataLoader\nShuffles the data at every epoch to prevent the model from learning order-specific patterns and to encourage better generalization.\nValidation DataLoader\nDoes not shuffle the data and is used to evaluate model performance after training without introducing randomness.\n\nEach batch produced by the DataLoader contains:\n\na tensor of tokenized tweets (input_ids)\na tensor of attention masks (attention_mask)\na tensor of labels\n\nThese will be passed directly into BERT during training and evaluation. Creating DataLoaders ensures that the model receives input in a structured, optimized, and reproducible format.\n\n\nShow the code\n# Creating DataLoaders for training and validation\n\nfrom torch.utils.data import DataLoader\n\nbatch_size = 32\n\ntrain_loader = DataLoader(\n    tokenized_dataset[\"train\"],\n    batch_size=batch_size,\n    shuffle=True\n)\n\nval_loader = DataLoader(\n    tokenized_dataset[\"validation\"],\n    batch_size=batch_size,\n    shuffle=False\n)"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#baseline-model-no-fine-tuning",
    "href": "topics/transformers/fine_tuning.html#baseline-model-no-fine-tuning",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Baseline Model (No Fine-Tuning)",
    "text": "Baseline Model (No Fine-Tuning)\nBefore fine-tuning BERT, it is important to establish a baseline performance level. This baseline shows how well a pre-trained model performs without any adaptation to the domain—in this case, Twitter sentiment.\nAlthough BERT is trained on large general-purpose corpora (BooksCorpus and Wikipedia), it has never been exposed to the specific linguistic style of tweets. Tweets differ dramatically from formal text: they are short, noisy, packed with slang, emojis, abbreviations, hashtags, irony, and sarcasm. As a result, a pre-trained BERT model typically struggles when applied directly to this dataset.\nTo measure this effect, we freeze all of BERT’s encoder layers so that:\n\nthe transformer parameters remain unchanged,\nonly the classification head exists (untrained),\nthe model effectively performs zero-shot sentiment classification.\n\nEvaluating this model provides a realistic lower bound:\nHow well does BERT understand tweet sentiment before training?\nThis baseline accuracy serves as a reference point for comparing the improvements introduced by partial and full fine-tuning in later steps.\n\n\nShow the code\n# Baseline Model (No Fine-Tuning)\n\nfrom transformers import BertForSequenceClassification\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load a pre-trained BERT classifier\nbaseline_model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=3\n).to(device)\n\n# Freeze all encoder layers → no fine-tuning\nfor param in baseline_model.bert.parameters():\n    param.requires_grad = False\n\n# Evaluation function\ndef evaluate(model, data_loader):\n    model.eval()\n    correct, total = 0, 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            ids = batch[\"input_ids\"].to(device)\n            mask = batch[\"attention_mask\"].to(device)\n            y = batch[\"label\"].to(device)\n\n            logits = model(ids, attention_mask=mask).logits\n            preds = torch.argmax(logits, dim=1)\n\n            correct += (preds == y).sum().item()\n            total += y.size(0)\n\n    return correct / total"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#partial-fine-tuning",
    "href": "topics/transformers/fine_tuning.html#partial-fine-tuning",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Partial Fine-Tuning",
    "text": "Partial Fine-Tuning\nThe baseline model demonstrates how poorly a pre-trained BERT performs when it is not adapted to the TweetEval domain. To improve performance while still keeping computational cost low, we apply partial fine-tuning.\nBERT consists of 12 transformer encoder layers stacked on top of one another. The lower layers typically learn general linguistic features such as token identity, morphology, and short-range dependencies. The upper layers capture more task-specific information, including sentiment cues and semantic relationships.\nIn partial fine-tuning, we freeze the lower encoder layers (layers 0–8) and train only the upper layers (layers 9–11) together with the classification head. This approach:\n\nreduces the number of trainable parameters,\nspeeds up training,\nlowers VRAM requirements,\nreduces risk of overfitting on small datasets,\nstill allows BERT to learn important domain-specific patterns from tweets.\n\nPartial fine-tuning typically yields a large improvement in accuracy compared to the baseline, while still being efficient enough to run quickly in environments like Google Colab.\nThe next code block implements partial fine-tuning for one epoch and evaluates its performance.\n\n\nShow the code\n# Partial Fine-Tuning (train upper layers only)\n\nfrom transformers import BertForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load a fresh BERT model\npartial_model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=3\n).to(device)\n\n# Freeze lower BERT layers: 0–8\nfor layer_idx in range(9):\n    for param in partial_model.bert.encoder.layer[layer_idx].parameters():\n        param.requires_grad = False\n\n# Train only layers 9, 10, 11 + classifier head\ntrainable_params = [p for p in partial_model.parameters() if p.requires_grad]\n\n\noptimizer = AdamW(trainable_params, lr=2e-5)\n\n# One training epoch\npartial_model.train()\n\n\nShow the code\nfor batch in train_loader:\n    optimizer.zero_grad()\n\n    ids = batch[\"input_ids\"].to(device)\n    mask = batch[\"attention_mask\"].to(device)\n    y = batch[\"label\"].to(device)\n\n    outputs = partial_model(ids, attention_mask=mask, labels=y)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#full-fine-tuning",
    "href": "topics/transformers/fine_tuning.html#full-fine-tuning",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Full Fine-Tuning",
    "text": "Full Fine-Tuning\nPartial fine-tuning provides a substantial performance boost, but it still restricts learning to only the top few transformer layers. To give the model maximum flexibility and allow it to fully adapt to the characteristics of Twitter language, we now perform full fine-tuning, where all BERT parameters are updated.\nFull fine-tuning trains:\n\nall 12 transformer layers,\nthe attention mechanisms inside each layer,\nthe intermediate feedforward networks,\nlayer normalization parameters,\nand the final classification head.\n\nThis approach generally achieves the highest accuracy because the model can adjust every level of linguistic representation, from low-level token embeddings to high-level semantic patterns. As a result, the model becomes better at interpreting informal grammar, emojis, irony, abbreviations, and the overall noisy structure common in tweets.\nThe trade-offs are:\n\nhigher computational cost,\nlonger training time,\nincreased risk of overfitting on very small datasets.\n\nIn this exercise, the training set is large enough and the model is already well regularized from pretraining, so full fine-tuning typically yields the best results.\nThe following code block performs one epoch of full fine-tuning and reports the resulting accuracy.\n\n\nShow the code\n# Full Fine-Tuning (train ALL layers)\n\nfrom transformers import BertForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load a fresh BERT model for full fine-tuning\nfull_model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=3\n).to(device)\n\n# Unfreeze ALL parameters → full end-to-end training\nfor param in full_model.parameters():\n    param.requires_grad = True\n\n# Count total trainable parameters\ntrainable_params = [p for p in full_model.parameters() if p.requires_grad]\n\n\noptimizer = AdamW(trainable_params, lr=2e-5)\n\n# One training epoch\nfull_model.train()\n\n\nShow the code\nfor batch in train_loader:\n    optimizer.zero_grad()\n\n    ids = batch[\"input_ids\"].to(device)\n    mask = batch[\"attention_mask\"].to(device)\n    y = batch[\"label\"].to(device)\n\n    outputs = full_model(ids, attention_mask=mask, labels=y)\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#comparing-the-three-models",
    "href": "topics/transformers/fine_tuning.html#comparing-the-three-models",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Comparing the Three Models",
    "text": "Comparing the Three Models\nNow that we have trained all three versions of the model—baseline, partial fine-tuning, and full fine-tuning—we can compare their performance directly. This comparison highlights how each level of adaptation improves the model’s ability to understand tweet sentiment.\n\nWhy Comparison Matters\nEvaluating all three models side by side reveals:\n\nThe impact of domain mismatch:\nThe baseline model performs poorly because raw BERT has never seen the structure of tweets, which often include emojis, slang, abbreviations, and informal grammar.\nThe benefit of partial fine-tuning:\nTraining only the top transformer layers allows the model to adjust its higher-level semantic representations, significantly improving accuracy without updating the entire network.\nThe advantages of full fine-tuning:\nUpdating all layers gives the model maximal flexibility to adapt to the tweet domain, typically yielding the highest performance.\n\n\n\nWhat We Expect to See\n\nBaseline accuracy: very low\n\nPartial fine-tuning: large improvement\n\nFull fine-tuning: best performance overall\n\nThis comparison provides the clearest demonstration of why fine-tuning is essential in modern NLP workflows. The next code block prints all three accuracies side by side.\n\n\nShow the code\nimport pandas as pd\n\nbaseline_acc = evaluate(baseline_model, val_loader)\n\npartial_acc = evaluate(partial_model, val_loader)\n\nfull_acc = evaluate(full_model, val_loader)\n\n# Collect metrics into a DataFrame\nresults_df = pd.DataFrame({\n    \"Model\": [\n        \"Baseline (no fine-tuning)\",\n        \"Partial fine-tuning\",\n        \"Full fine-tuning\"\n    ],\n    \"Accuracy\": [\n        round(baseline_acc, 4),\n        round(partial_acc, 4),\n        round(full_acc, 4)\n    ]\n})\n\nprint(results_df)\n\n\n                       Model  Accuracy\n0  Baseline (no fine-tuning)     0.436\n1        Partial fine-tuning     0.558\n2           Full fine-tuning     0.624"
  },
  {
    "objectID": "topics/transformers/fine_tuning.html#qualitative-comparison",
    "href": "topics/transformers/fine_tuning.html#qualitative-comparison",
    "title": "Fine-Tuning BERT on TweetEval Sentiment",
    "section": "Qualitative Comparison",
    "text": "Qualitative Comparison\nAccuracy scores and confusion matrices give a numerical summary of model performance, but they do not show how the models behave on individual examples. To understand the practical differences between the baseline, partial fine-tuning, and full fine-tuning, we compare their predictions on actual tweets.\nThis qualitative analysis is important because tweets often contain:\n\nsarcasm\n\nemojis\n\ninformal grammar\n\nabbreviations\n\nambiguous sentiment cues\n\nThese characteristics make sentiment classification difficult for a model that has not been adapted to Twitter-specific language patterns.\n\nWhat We Look For\nBy examining a small set of tweets and the predictions from all three models, we can observe:\n\nThe baseline model may misinterpret emojis or fail to detect sarcasm.\n\nThe partial fine-tuning model improves substantially, especially on clearer sentiment cues.\n\nThe full fine-tuning model typically gives the most reliable predictions and handles noisy input better.\n\nThis side-by-side comparison makes the benefits of fine-tuning immediately visible and intuitive.\nThe next code block selects random tweets and prints the predictions from all three models alongside the true labels.\n\n\nShow the code\n# Qualitative Comparison: Predictions on Sample Tweets\n\nimport random\nimport torch\n\nlabel_names = dataset[\"train\"].features[\"label\"].names\n\n# Select 10 random validation examples\nindices = random.sample(range(len(dataset[\"validation\"])), 10)\n\nprint(\"\\n=== QUALITATIVE MODEL COMPARISON ===\\n\")\n\n\n\n=== QUALITATIVE MODEL COMPARISON ===\n\n\nShow the code\nfor idx in indices:\n    text = dataset[\"validation\"][idx][\"text\"]\n    true_label = label_names[dataset[\"validation\"][idx][\"label\"]]\n\n    # Tokenize a single tweet\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=64\n    ).to(device)\n\n    # Predictions from each model\n    baseline_pred = label_names[baseline_model(**inputs).logits.argmax(dim=1).item()]\n    partial_pred  = label_names[partial_model(**inputs).logits.argmax(dim=1).item()]\n    full_pred     = label_names[full_model(**inputs).logits.argmax(dim=1).item()]\n\n    print(f\"Tweet: {text}\")\n    print(f\"True label:       {true_label}\")\n    print(f\"Baseline:         {baseline_pred}\")\n    print(f\"Partial-tuned:    {partial_pred}\")\n    print(f\"Full-tuned:       {full_pred}\")\n    print(\"-\" * 80)\n\n\nTweet: \"Paul Dunne shares 3rd-round lead at the Open. ... Wait, who?!?!\nTrue label:       neutral\nBaseline:         neutral\nPartial-tuned:    neutral\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: \"If I'm off from work again tomorrow, I'm spending the entire day catching up on The Walking Dead.\"\nTrue label:       neutral\nBaseline:         positive\nPartial-tuned:    neutral\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: @user if you're interested... Mansbridge's Harper interview. Justin tomorrow, Mulcair Wednesday.\nTrue label:       positive\nBaseline:         neutral\nPartial-tuned:    positive\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: PSSA patron & Doctors star Ian Kelsey will give a TV workshop @ the studio on Sat 24 Nov. 10 places open to non-members\nTrue label:       neutral\nBaseline:         neutral\nPartial-tuned:    neutral\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: #RedSox Saltalamacchia is the 4th catcher to hit 16+ HR in back-to-back seasons for BOS. Others: Carlton Fisk\\u002c Rich Gedman\\u002c Jason Varitek.\nTrue label:       neutral\nBaseline:         neutral\nPartial-tuned:    neutral\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: I'm thinking it's either some solo shit or that Kendrick collab\nTrue label:       negative\nBaseline:         positive\nPartial-tuned:    neutral\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: @user Please refrain from interupting Dan's rants on Dana White with a stupid ass Sunday Countdown commercial...     k? thanks!\nTrue label:       negative\nBaseline:         neutral\nPartial-tuned:    positive\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: @user Sorry to hear that! If things change before Feb, lemme know! I'll buy you a coffee and squee about Kim and Briana at you. *lol*\"\nTrue label:       positive\nBaseline:         positive\nPartial-tuned:    positive\nFull-tuned:       positive\n--------------------------------------------------------------------------------\nTweet: \"J.Cole &gt; Kendrick Lamar, biased opinion Kendrick is growing on me and the February album if true will be lit\"\nTrue label:       positive\nBaseline:         positive\nPartial-tuned:    neutral\nFull-tuned:       neutral\n--------------------------------------------------------------------------------\nTweet: Coming December 11th &amp; December 12th LIVE on PARTY103! We celebrate the Epic 100th Episode of Curtis &amp; Craigs...\nTrue label:       positive\nBaseline:         neutral\nPartial-tuned:    positive\nFull-tuned:       positive\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "topics/single_layer/regression.html",
    "href": "topics/single_layer/regression.html",
    "title": "Single layer Neural Network for Regression",
    "section": "",
    "text": "Implement a simple neural network from scratch and compare its performance to linear regression on a 2D dataset.\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/single_layer/regression.html#auxiliary-functions",
    "href": "topics/single_layer/regression.html#auxiliary-functions",
    "title": "Single layer Neural Network for Regression",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nImplement Training Functions\n\nDefine helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.\n\n\n\nShow the activation function code\nimport numpy as np\n\n\ndef activate(Z, activation_function=\"tanh\"):\n    \"\"\"\n    Apply an activation function elementwise.\n    \"\"\"\n    if activation_function == \"tanh\":\n        return np.tanh(Z)  # squashes values to [-1, 1]\n    elif activation_function == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-Z))  # squashes values to [0, 1]\n    else:\n        raise ValueError(\"activation_function must be 'tanh' or 'sigmoid'.\")\n\n\n\n\nShow the parameters initialization code\nimport numpy as np\n\n\ndef initialize_parameters(X, num_hidden_layer_neurons, scale_const=0.01, seed=1):\n    \"\"\"\n    Initialize weights and biases for a single hidden-layer network.\n    \"\"\"\n    np.random.seed(seed)\n\n    n_features = X.shape[1]  # number of input features\n\n    # Small random weights help avoid saturation of activations at start\n    W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n    b1 = np.zeros((num_hidden_layer_neurons, 1))\n    W2 = np.random.randn(1, num_hidden_layer_neurons) * scale_const\n    b2 = np.zeros((1, 1))\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\n\n\nForward and backward propagation in a single-hidden-layer neural network. The forward pass takes inputs \\(X\\) through weights \\(W^{[1]}, W^{[2]}\\) and biases \\(b^{[1]}, b^{[2]}\\) to produce pre-activations \\(Z^{[1]}, Z^{[2]}\\), activations \\(A^{[1]}, A^{[2]}\\), and final output \\(y\\). The backward pass computes gradients of activations, pre-activations, weights, and biases \\((dA, dZ, dW, db)\\) from the output layer back to the input layer for parameter updates.\n\n\n\n\nShow the forward propagation code\nimport numpy as np\n\ndef forward_propagation(parameters, X_adj):\n    \"\"\"\n    Perform one forward pass (regression: linear output).\n    \"\"\"\n    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n\n    Z1 = np.dot(W1, X_adj) + b1      # hidden layer affine transform\n    A1 = activate(Z1, \"tanh\")        # hidden nonlinearity\n    Z2 = np.dot(W2, A1) + b2         # output layer affine transform\n    A2 = Z2                          # linear output for regression\n\n    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n\n\n\n\nShow the backward propagation code\nimport numpy as np\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n    \"\"\"\n    Compute gradients for parameters using backpropagation (MSE loss, linear output).\n    \"\"\"\n    N = X_adj.shape[1]  # number of samples\n\n    W2 = parameters[\"W2\"]\n    A1, A2 = forward_propagation_values[\"A1\"], forward_propagation_values[\"A2\"]\n\n    # Output layer: for MSE with linear output, dZ2 = A2 - Y\n    dZ2 = A2 - Y_adj\n    dW2 = np.dot(dZ2, A1.T) / N\n    db2 = np.sum(dZ2, axis=1, keepdims=True) / N\n\n    # Hidden layer\n    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2)\n    dW1 = np.dot(dZ1, X_adj.T) / N\n    db1 = np.sum(dZ1, axis=1, keepdims=True) / N\n\n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n\n\n\n\nShow the parameters update code\nimport numpy as np\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n    \"\"\"\n    Update parameters using gradient descent.\n    \"\"\"\n    # subtract learning_rate * gradient for each parameter\n    W1 = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n    b1 = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n    W2 = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n    b2 = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\nCreate a wrapper function train_neural_network to run the training loop.\n\n\n\nShow the neural network training code\ndef train_neural_network(X, Y, num_iterations, num_hidden_layer_neurons=4):\n    \"\"\"\n    Trains a simple 1-hidden-layer neural network using gradient descent.\n    \"\"\"\n    # Transpose X so columns are examples, reshape Y to row vector\n    X_adj = X.T.copy()                         \n    Y_adj = Y.values.reshape(1, -1).copy()     \n\n    # Initialize weights and biases\n    parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n    for iteration in range(num_iterations):\n        # Forward pass\n        forward_values = forward_propagation(parameters, X_adj.copy())\n\n        # Backward pass\n        grads = backward_propagation(parameters, forward_values, X_adj.copy(), Y_adj.copy())\n\n        # Parameter update\n        parameters = update_parameters(parameters, grads)\n\n    return parameters\n\n\n\n\nImplement Prediction Function\n\nCreate a predict function that runs forward propagation and returns continuous outputs.\n\n\n\nShow the code\ndef predict(nn_parameters, X):\n    \"\"\"\n    Generates continuous predictions from a trained neural network (regression).\n    \"\"\"\n    # Transpose X so columns are examples\n    X_adj = X.T.copy()\n\n    # Forward pass to get output layer values\n    forward_values = forward_propagation(nn_parameters, X_adj.copy())\n\n    # For regression, output is linear (A2)\n    return forward_values[\"A2\"].ravel()",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/rnn/rnn_cpi.html",
    "href": "topics/rnn/rnn_cpi.html",
    "title": "CNN for Univariate Time Series Forecasting",
    "section": "",
    "text": "In this tutorial we build a 1-D Convolutional Neural Network (CNN) to forecast a single time series (univariate forecasting).\nThe goal is to show the full workflow:\n\nImport and visualize a CPI series.\nPreprocess the data (optional log/scale/difference).\nTurn the series into sliding windows for a CNN.\nBuild and train a simple 1-D CNN model.\nEvaluate the model on a hold-out test set.\n\n\n\nShow the code\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n\n\nImport data\nIn this example we use a CPI dataset stored locally in cpi_series.csv. From this file we select a single series (item_id == “Seasonal_1”) and split it into a training part (first 80% of observations) and a test part (last 20%). Before building any model, it is important to look at the raw series. Here we convert the timestamp column to a proper date type and plot the selected CPI series over time to get a sense of its level, trend and volatility.\n\n\nShow the code\n\ndata_path = os.path.join(\n    os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n    \"data\\\\cpi_series.csv\"\n)\n\ncpi_data = pd.read_csv(data_path)\n\ntemp_df = cpi_data.loc[cpi_data[\"item_id\"] == \"Seasonal_1\",[\"timestamp\",\"target\"]].copy()\n\nseries = temp_df['target'].values\n\ntrain_size = int(len(series) * 0.8)\ntrain_series_raw = series[:train_size]\ntest_series_raw  = series[train_size:]\n\n\n\n\nShow the code\ntemp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'])\n\nplt.figure(figsize=(12, 6))\nplt.plot(temp_df['timestamp'], temp_df['target'])\nplt.xlabel('Timestamp')\nplt.ylabel('Target')\nplt.title('Target over Timestamp in temp_df')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAuxiliary Functions\nTo keep the workflow clean, we separate the major tasks into helper functions:\n\nPreprocessing functions – optional log-transform, differencing, and scaling.\n\nWindow creation – converting a raw series into fixed-length input sequences.\n\nModel construction – building a 1-D CNN for forecasting.\n\nPlotting utilities – visualizing predictions.\n\nThis modular structure makes the notebook easier to maintain and allows you to reuse components across different experiments.\n\n\nShow the code\n\ndef preprocess_data(\n    train_series,\n    test_series,\n    scale=True,\n    log_transform=False,\n    diff_transform=False\n):\n    train = train_series.astype(float).copy()\n    test  = test_series.astype(float).copy()\n\n    objects = {\n        \"scaler\": None,\n        \"log_shift\": None,\n        \"diff_last_value\": None\n    }\n\n    # 1. Log transform\n    if log_transform:\n        min_val = min(train.min(), test.min())\n        shift = 1 - min_val if min_val &lt;= 0 else 0\n        objects[\"log_shift\"] = shift\n        train = np.log(train + shift)\n        test  = np.log(test + shift)\n\n    # 2. Differencing\n    if diff_transform:\n        objects[\"diff_last_value\"] = train[-1]\n        train = np.diff(train)\n        test  = np.diff(test)\n\n    # 3. Scaling\n    if scale:\n        scaler = StandardScaler()\n        scaler.fit(train.reshape(-1, 1))\n        train = scaler.transform(train.reshape(-1, 1)).flatten()\n        test  = scaler.transform(test.reshape(-1, 1)).flatten()\n        objects[\"scaler\"] = scaler\n\n    return train, test, objects\n\ndef invert_predictions(y_pred, y_true, objects):\n    y_pred_inv = y_pred.copy().astype(float)\n    y_true_inv = y_true.copy().astype(float)\n\n    # 1. Invert scaling\n    scaler = objects[\"scaler\"]\n    if scaler is not None:\n        y_pred_inv = scaler.inverse_transform(y_pred_inv.reshape(-1, 1)).flatten()\n        y_true_inv = scaler.inverse_transform(y_true_inv.reshape(-1, 1)).flatten()\n\n    # 2. Invert differencing\n    if objects[\"diff_last_value\"] is not None:\n        last_val = objects[\"diff_last_value\"]\n        y_pred_inv = np.cumsum(np.insert(y_pred_inv, 0, last_val))\n        y_true_inv = np.cumsum(np.insert(y_true_inv, 0, last_val))\n\n    # 3. Invert log transform\n    if objects[\"log_shift\"] is not None:\n        shift = objects[\"log_shift\"]\n        y_pred_inv = np.exp(y_pred_inv) - shift\n        y_true_inv = np.exp(y_true_inv) - shift\n\n    return y_pred_inv, y_true_inv\n\n\n\n\nBuilding the CNN Model\nA 1-D Convolutional Neural Network can extract short-range temporal patterns from a time series in much the same way that a 2-D CNN extracts spatial features from images.\nThe idea is simple: - The Conv1D layers learn local patterns across the last ( W ) observations (trend shapes, small cycles, short spikes). - MaxPooling reduces noise and compresses the learned features. - A Dense layer maps these learned features into a forecast for the next value.\nThe model defined below contains: 1. Two initial convolutional layers with ReLU activation. 2. A max-pooling layer. 3. A deeper convolutional layer. 4. A flattening step followed by a dense hidden layer. 5. A final dense output neuron producing a single next-step forecast.\n\n\nShow the code\n\n\ndef build_cnn_model(W, \n                    filters1=32, filters2=64,\n                    kernel_size=3,\n                    dense_units=64):\n    \n    model = Sequential([\n        Conv1D(filters=filters1, kernel_size=kernel_size, activation='relu', input_shape=(W, 1)),\n        Conv1D(filters=filters1, kernel_size=kernel_size, activation='relu'),\n        MaxPooling1D(pool_size=2),\n\n        Conv1D(filters=filters2, kernel_size=kernel_size, activation='relu'),\n        Flatten(),\n\n        Dense(dense_units, activation='relu'),\n        Dense(1)\n    ])\n\n    model.compile(\n        loss='mse',\n        optimizer='adam',\n        metrics=['mae']\n    )\n\n    return model\n\ndef train_model(model, X_train, y_train, epochs=50, batch_size=16):\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.1,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=1\n    )\n    return history\n\n\nWhy we need windowing for CNNs\nA CNN expects a fixed-width input segment, similar to how images have fixed dimensions. For a time series, the equivalent is a sliding window of length W:\nThe window is the input, containing the last W observations.\nThe target is the next observation immediately following the window.\nThis converts a single long series into many supervised examples.\n\n\nShow the code\n\n\ndef make_windows_train(train_processed, W):\n    X_train = []\n    y_train = []\n\n    n = len(train_processed)\n\n    # Slide a window of length W across the training set\n    for i in range(n - W):\n        X_train.append(train_processed[i : i + W])\n        y_train.append(train_processed[i + W])\n\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    # Reshape for CNN: (samples, W, 1)\n    X_train = X_train.reshape(-1, W, 1)\n\n    return X_train, y_train\nimport numpy as np\n\ndef make_windows_test(train_processed, test_processed, W):\n    X_test = []\n    y_test = []\n\n    # Take the last W values from the training set\n    last_train_window = train_processed[-W:]\n\n    # Total test length\n    n_test = len(test_processed)\n\n    # Create rolling windows over the test set\n    for i in range(n_test):\n        if i == 0:\n            # First test window uses only train history\n            window = last_train_window\n        else:\n            # Next windows slide forward into test_processed\n            # Combine tail of last train values with part of test\n            window = np.concatenate([\n                last_train_window[i:],       # decreasing slice of train\n                test_processed[:i]           # increasing slice of test\n            ])\n\n        # Ensure window is exactly length W\n        window = window[-W:]\n\n        X_test.append(window)\n        y_test.append(test_processed[i])\n\n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n\n    # Reshape for CNN: (samples, W, 1)\n    X_test = X_test.reshape(-1, W, 1)\n\n    return X_test, y_test\n\n\n\n\nShow the code\n\ndef plot_forecast(y_true_inv, y_pred_inv):\n    plt.figure(figsize=(12, 5))\n    plt.plot(y_true_inv, label=\"Actual\", linewidth=2)\n    plt.plot(y_pred_inv, label=\"Predicted\", alpha=0.8)\n\n    plt.title(\"CNN Forecast on Original Scale\")\n    plt.xlabel(\"Test Index\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nPreprocessing and Window Construction\nBefore training the CNN, we must prepare the series so the model receives inputs in the correct format.\nThis step performs three tasks:\n\nSplit the raw data into training and test segments.\n\nApply preprocessing (log-transform, scaling, optional differencing).\n\nConvert the processed series into sliding windows of length ( W ), which form the inputs for the CNN.\n\nThe window length ( W ) controls how much historical information the model sees for each prediction.\nIn this example the model uses the past 36 observations to predict the next one.\n\n\nShow the code\nW = 36\n\n# 1. Split raw series\nseries = temp_df['target'].values\ntrain_size = int(len(series) * 0.8)\ntrain_series_raw = series[:train_size]\ntest_series_raw  = series[train_size:]\n\n# 2. Preprocess\ntrain_p, test_p, objects = preprocess_data(train_series_raw, test_series_raw,\n                                           log_transform=True,diff_transform=False,scale=True)\n\n# 3. Create windows\nX_train, y_train = make_windows_train(train_p, W)\nX_test,  y_test  = make_windows_test(train_p, test_p, W)\n\n\n\n\nFitting the CNN Model\nWith the data prepared and the network architecture defined, we can now train the CNN.\nTraining consists of two steps:\n\nBuild the model with the chosen window size ( W ).\n\nFit the model on the windowed training data.\n\nThe training call uses: - validation_split=0.1 to monitor performance on unseen data during training, - epochs=50 and batch_size=16 as reasonable defaults for a small univariate series.\n\n\nShow the code\n# 4. Build model\nmodel = build_cnn_model(W)\n\n# 5. Train\nhistory = train_model(model, X_train, y_train, epochs=50, batch_size=16)\n\n\n\n\nEvaluation and Forecast Visualization\nOnce the CNN is trained, we can use it to generate predictions for the entire test set.\nSince the model was trained on preprocessed data, the predictions must be converted back to the original scale to make them interpretable.\nThe evaluation workflow consists of:\n\nPredicting on the test windows.\n\nInverting all preprocessing steps (scaling, log-transform, differencing).\n\nPlotting predicted vs. actual values on the original scale.\n\n\n\nShow the code\n# 6. Predict\ny_pred = model.predict(X_test).flatten()\n\n\nShow the code\n\n# 7. Invert preprocessing\ny_pred_inv, y_test_inv = invert_predictions(y_pred, y_test, objects)\n\n\n\n\nShow the code\n# 8. Plot\nplot_forecast(y_test_inv, y_pred_inv)",
    "crumbs": [
      "Seasonal data"
    ]
  },
  {
    "objectID": "topics/python_workshop/python_workshop.html",
    "href": "topics/python_workshop/python_workshop.html",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "This hands-on workshop introduces the essential Python skills needed for deep learning. You’ll run Python code in Google Colab.\n\n\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport os\n\n\n\n\nGet familiar with Python’s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n\n\nLists and dictionaries\nif, for, and while statements\nWriting and calling functions\n\n\n\n\nConcept:\n- Lists are ordered collections. Each item has a position called an index (starting from 0).\n- You can get a single element using list[index] (e.g., scores[0] → first element).\n- You can get a slice using list[start:stop], which returns elements from start up to but not including stop.\n- Dictionaries store key–value pairs. You look up a value by its key (like a label).\n- You can add new keys or update existing ones using dict[key] = value.\n- Keys must be unique; values can be any data type.\n\n# A list of exam scores for Alice\nscores_alice = [88, 92, 79]\n\n# Indexing: position 0 is the first score\nprint(\"First score:\", scores_alice[0])\n\nFirst score: 88\n\n# Slicing: [:2] means start at index 0, stop before index 2 → positions 0 and 1\nprint(\"First two scores:\", scores_alice[:2])\n\nFirst two scores: [88, 92]\n\n# A dictionary mapping student names to their list of scores\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70],\n}\n\n# Lookup: use the key (student's name) to get the list of scores\nprint(\"Carol's scores:\", student_scores[\"Carol\"])\n\nCarol's scores: [90, 85, 95]\n\n# Add a new student by assigning to a new key\nstudent_scores[\"Eve\"] = [85, 88, 91]\n\n# Update an existing student's scores (overwrites the old list)\nstudent_scores[\"Bob\"] = [78, 84, 82]\n\n# Get all keys (student names) and values (lists of scores)\nprint(\"Students:\", list(student_scores.keys()))\n\nStudents: ['Alice', 'Bob', 'Carol', 'Dave', 'Eve']\n\nprint(\"Sample scores:\", list(student_scores.values())[:2])  # [:2] → first two values\n\nSample scores: [[88, 92, 79], [78, 84, 82]]\n\n\n\n\n\nConcept:\n- if / elif / else lets the program choose actions based on conditions.\n- for loops iterate over items in a collection, letting you process each element in turn.\nWe’ll use the student_scores dictionary from the previous section.\n\n# Example 1: if / elif / else\nx = 87\nif x &gt;= 90:\n    grade = \"A\"\nelif x &gt;= 80:\n    grade = \"B\"\nelse:\n    grade = \"C or below\"\nprint(\"Grade bucket:\", grade)\n\nGrade bucket: B\n\n# Example 2: for loop over a student's scores\n# Reusing the student_scores dictionary from earlier\nscores_alice = student_scores[\"Alice\"]  # a list of Alice's scores\ntotal = 0\nfor s in scores_alice:\n    total = total + s\naverage = total / len(scores_alice)\nprint(\"Alice's average score:\", average)\n\nAlice's average score: 86.33333333333333\n\n\n\n\n\nConcept:\n- A function is a reusable block of code that takes inputs (parameters) and can return an output.\n- Use def function_name(parameters): to define it.\n- Use return to send a result back to the caller.\n- You can reuse loops and calculations inside a function so you don’t repeat the same code.\n\ndef average_score(scores):\n    \"\"\"\n    Calculate the average from a list of scores.\n    \"\"\"\n    total = 0\n    for s in scores:\n        total = total + s\n    return total / len(scores)\n\n# Example: Calculate averages for all students\nfor name in student_scores:  # loops over keys (student names)\n    avg = average_score(student_scores[name])\n    print(f\"{name}: {avg:.2f}\")\n\nAlice: 86.33\nBob: 81.33\nCarol: 90.00\nDave: 73.33\nEve: 88.00\n\n\n\n\n\n\nConcept:\nNumPy arrays are like lists but optimized for fast mathematical operations.\n- 1D array → like a row of numbers.\n- 2D array → like a table (rows × columns).\n- .shape tells you the size of the array.\n\n# 1D array: vector of exam scores\nscores_1d = np.array([88, 92, 79])\nprint(\"1D array:\", scores_1d)\n\n1D array: [88 92 79]\n\nprint(\"Shape:\", scores_1d.shape)  # (3,) → 3 elements in 1 dimension\n\nShape: (3,)\n\n# 2D array: scores for two students across three exams\nscores_2d = np.array([\n    [88, 92, 79],  # student 1\n    [75, 83, 80]   # student 2\n])\nprint(\"\\n2D array:\\n\", scores_2d)\n\n\n2D array:\n [[88 92 79]\n [75 83 80]]\n\nprint(\"Shape:\", scores_2d.shape)  # (2, 3) → 2 rows, 3 columns\n\nShape: (2, 3)\n\n# Now convert student_scores dictionary values into a 2D array\ngrades_matrix = np.array(list(student_scores.values()))\nprint(\"\\nGrades matrix:\\n\", grades_matrix)\n\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)  # rows = students, columns = exams\n\nShape: (5, 3)\n\n\n\n\nConcept:\n- Indexing works like Python lists: array[row_index, col_index] (0-based).\n- Slicing lets you select a range: start:stop returns elements from start up to (but not including) stop.\n- You can slice rows, columns, or both.\n\n# Example array for reference\nprint(\"Grades matrix:\\n\", grades_matrix)\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\n# Get the score of the first student in the first exam\nprint(\"\\nFirst student's first exam score:\", grades_matrix[0, 0])\n\n\nFirst student's first exam score: 88\n\n# Get all exam scores for the second student (row index 1)\nprint(\"Second student's scores:\", grades_matrix[1, :])\n\nSecond student's scores: [78 84 82]\n\n# Get all scores for the third exam (column index 2)\nprint(\"Scores in third exam:\", grades_matrix[:, 2])\n\nScores in third exam: [79 82 95 70 91]\n\n# Slice: first two students' scores\nprint(\"First two students' scores:\\n\", grades_matrix[0:2, :])\n\nFirst two students' scores:\n [[88 92 79]\n [78 84 82]]\n\n# Slice: first two exams for all students\nprint(\"First two exams for all students:\\n\", grades_matrix[:, 0:2])\n\nFirst two exams for all students:\n [[88 92]\n [78 84]\n [90 85]\n [72 78]\n [85 88]]\n\n\n\n\n\nConcept:\n- Element-wise operations apply a calculation to each element of an array.\n- Broadcasting lets NumPy apply operations between arrays of different shapes by “stretching” one to match the other (without copying data).\n- This is much faster and cleaner than using Python loops.\n\n# Add 5 points to every score (element-wise addition)\nprint(\"Original:\\n\", grades_matrix)\n\nOriginal:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"\\n+5 to every score:\\n\", grades_matrix + 5)\n\n\n+5 to every score:\n [[ 93  97  84]\n [ 83  89  87]\n [ 95  90 100]\n [ 77  83  75]\n [ 90  93  96]]\n\n# Multiply all scores by 1.1 to simulate a 10% bonus\nprint(\"\\n10% bonus:\\n\", grades_matrix * 1.1)\n\n\n10% bonus:\n [[ 96.8 101.2  86.9]\n [ 85.8  92.4  90.2]\n [ 99.   93.5 104.5]\n [ 79.2  85.8  77. ]\n [ 93.5  96.8 100.1]]\n\n# Broadcasting: subtract the minimum score in each column (exam) from that column\nmin_scores_per_exam = grades_matrix.min(axis=0)  # shape: (n_exams,)\nprint(\"\\nMinimum scores per exam:\", min_scores_per_exam)\n\n\nMinimum scores per exam: [72 78 70]\n\nadjusted = grades_matrix - min_scores_per_exam  # broadcasting happens here\nprint(\"\\nScores adjusted by exam minimum:\\n\", adjusted)\n\n\nScores adjusted by exam minimum:\n [[16 14  9]\n [ 6  6 12]\n [18  7 25]\n [ 0  0  0]\n [13 10 21]]\n\n\n\n\n\nConcept:\n- Matrix multiplication (np.dot) combines rows and columns, often used in deep learning layers to combine inputs with weights.\n- Axis-based operations let you apply functions (mean, sum, etc.) across rows or columns:\n\naxis=0 → operate down columns (across rows)\naxis=1 → operate across columns (per row)\n\nExtra notes: - np.ones(shape) creates an array of ones with the given shape.\n- Here we use it for equal weights when averaging scores: each exam gets the same weight.\n- .flatten() converts a multi-dimensional array into a 1D array.\n- After matrix multiplication, the result might be shape (n_students, 1); flattening makes it easier to print and work with.\n\n\n\nMatrix multiplication producing a column vector, then flattening to a 1D array. Here, each row of the grades matrix is multiplied by the weights vector to produce a single average score per student.\n\n\n\n# Example: equal-weight average across exams (axis=1 → per student)\navg_scores_axis = grades_matrix.mean(axis=1)\nprint(\"Average score per student (axis=1):\", [f\"{x:.2f}\" for x in avg_scores_axis])\n\nAverage score per student (axis=1): ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Example: average score per exam (axis=0 → per exam)\navg_scores_exam = grades_matrix.mean(axis=0)\nprint(\"Average score per exam (axis=0):\", [f\"{x:.2f}\" for x in avg_scores_exam])\n\nAverage score per exam (axis=0): ['82.60', '85.40', '83.40']\n\n# Using matrix multiplication to compute averages\nn_exams = grades_matrix.shape[1]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\naverages_via_dot = np.dot(grades_matrix, weights).flatten()\nprint(\"\\nAverages via matrix multiplication:\",\n      [f\"{x:.2f}\" for x in averages_via_dot])\n\n\nAverages via matrix multiplication: ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Weighted sum example: suppose exams have weights 0.5, 0.3, 0.2\nexam_weights = np.array([0.5, 0.3, 0.2]).reshape(-1, 1)  # shape: (n_exams, 1)\nweighted_scores = np.dot(grades_matrix, exam_weights).flatten()\nprint(\"\\nWeighted average per student:\",\n      [f\"{x:.2f}\" for x in weighted_scores])\n\n\nWeighted average per student: ['87.40', '80.60', '89.50', '73.40', '87.10']\n\n\n\n\n\nConcept:\n- A NumPy array is efficient for numerical operations but has no column or row labels — you must remember indexes yourself.\n- A pandas DataFrame wraps a NumPy array with labels (row and column names), allowing:\n\nEasier indexing by name (df[\"Math\"]) instead of position.\nMixed data types in one table (numbers, text, dates).\nBuilt-in data inspection methods (.head(), .info(), .describe()).\n\nKey point: Deep learning libraries often use NumPy arrays internally, but pandas is more convenient for data cleaning and exploration.\n\n# Our NumPy grades_matrix (from Section 3)\nprint(\"NumPy array:\\n\", grades_matrix)\n\nNumPy array:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)\n\nShape: (5, 3)\n\n# Convert to DataFrame with labels\nexam_names = [\"Exam 1\", \"Exam 2\", \"Exam 3\"]\nstudent_names = list(student_scores.keys())\ngrades_df = pd.DataFrame(grades_matrix, index=student_names, columns=exam_names)\n\nprint(\"\\nDataFrame:\\n\", grades_df)\n\n\nDataFrame:\n        Exam 1  Exam 2  Exam 3\nAlice      88      92      79\nBob        78      84      82\nCarol      90      85      95\nDave       72      78      70\nEve        85      88      91\n\n# Accessing data\nprint(\"\\nScore of Carol in Exam 2 (by labels):\", grades_df.loc[\"Carol\", \"Exam 2\"])\n\n\nScore of Carol in Exam 2 (by labels): 85\n\nprint(\"Score of Carol in Exam 2 (by position):\", grades_matrix[2, 1])\n\nScore of Carol in Exam 2 (by position): 85\n\n# Quick stats for each exam\nprint(\"\\nExam averages:\\n\", grades_df.mean().round(2))\n\n\nExam averages:\n Exam 1    82.6\nExam 2    85.4\nExam 3    83.4\ndtype: float64\n\n\n\n\n\n\nConcept:\nIn real projects, we often load datasets from CSV or Excel files.\nPandas DataFrames are perfect for this stage because they:\n\nRead files directly into a labeled table.\nMake it easy to explore and summarize the data.\nAllow quick selection of features (X) and target labels (y) for model training.\n\nWe’ll load a planar dataset with two numeric features (x_coord, y_coord) and a binary label (label).\n\n# Load into DataFrame\nraw_df = pd.read_csv(data_path_here)\n\n# Inspect\nprint(\"Shape:\", raw_df.shape)\n\nShape: (400, 3)\n\nprint(\"\\nFirst 5 rows:\\n\", raw_df.head())\n\n\nFirst 5 rows:\n     x_coord   y_coord  label\n0  1.204442  3.576114      0\n1  0.158710 -1.482171      0\n2  0.095247 -1.279955      0\n3  0.349178 -2.064380      0\n4  0.694150  2.889109      0\n\n# Separate features and target\nfeatures = [\"x_coord\", \"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\ny = raw_df[target].copy()\n\nprint(\"\\nFeatures sample:\\n\", X.head())\n\n\nFeatures sample:\n     x_coord   y_coord\n0  1.204442  3.576114\n1  0.158710 -1.482171\n2  0.095247 -1.279955\n3  0.349178 -2.064380\n4  0.694150  2.889109\n\nprint(\"\\nLabels sample:\\n\", y.head())\n\n\nLabels sample:\n 0    0\n1    0\n2    0\n3    0\n4    0\nName: label, dtype: int64\n\n\n\n\nConcept:\nA scatter plot lets us see how the two features (x_coord, y_coord) relate to the class label.\nIf classes are not linearly separable, a simple logistic regression will likely underperform compared to a neural network.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\n\n# Bright, high-contrast colors\ncolors = {0: \"orange\", 1: \"teal\"}\n\n# Scatter plot\nfor label_value in sorted(y.unique()):\n    subset = X[y == label_value]\n    plt.scatter(\n        subset[\"x_coord\"], subset[\"y_coord\"],\n        c=colors[label_value],\n        edgecolor=\"k\",\n        s=50,\n        label=f\"Class {label_value}\"\n    )\n\nplt.xlabel(\"x_coord\")\nplt.ylabel(\"y_coord\")\nplt.title(\"Planar Dataset by Label\")\nplt.legend(title=\"Label\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nConcept:\nWe’ll train two models on the planar dataset:\n\nLogistic Regression — a single-layer model that produces a linear decision boundary.\nShallow Neural Network — one hidden layer that can model non-linear boundaries.\n\n\n\n\n\n\nThis comparison shows why neural networks can outperform linear models on complex patterns.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Logistic Regression model: single Dense layer\nlog_reg_model = Sequential([\n    Dense(1, activation='sigmoid', input_shape=(2,))\n])\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\nlog_reg_history = log_reg_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nlog_acc = log_reg_model.evaluate(X, y, verbose=0)[1]\n\n\nprint(f\"Logistic Regression Accuracy: {log_acc:.2f} \\n \")\n\nLogistic Regression Accuracy: 0.46 \n \n\n\n\n# Neural Network model: one hidden layer\nnn_model = Sequential([\n    Dense(10, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\nnn_history = nn_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nnn_acc = nn_model.evaluate(X, y, verbose=0)[1]\n\nprint(f\"Neural Network Accuracy:     {nn_acc:.2f}\")\n\nNeural Network Accuracy:     0.70\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract loss values\nlog_loss = log_reg_history.history['loss']\nnn_loss = nn_history.history['loss']\n\n# Extract accuracy values\nlog_acc_hist = log_reg_history.history['accuracy']\nnn_acc_hist = nn_history.history['accuracy']\n\nepochs_range = range(1, len(log_loss) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, log_loss, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_loss, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, log_acc_hist, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_acc_hist, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTo better understand how each model separates the two classes, we will define an auxiliary plotting function called plot_decision_boundary.\nThis function will: 1. Create a grid over the feature space. 2. Use the model to predict the class for each point in the grid. 3. Display the predicted regions as a colored background. 4. Overlay the actual data points on top.\nAfter defining this function, we will call it for both the logistic regression and neural network models to visually compare their decision boundaries.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y, title):\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[\"x_coord\"].min() - 1, X[\"x_coord\"].max() + 1\n    y_min, y_max = X[\"y_coord\"].min() - 1, X[\"y_coord\"].max() + 1\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 300),\n        np.linspace(y_min, y_max, 300)\n    )\n    \n    # Predict over the grid\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(grid_points, verbose=0)\n    Z = (Z &gt; 0.5).astype(int).reshape(xx.shape)\n\n    # Plot contour and points\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Oranges, alpha=0.3)\n    \n    colors = {0: \"orange\", 1: \"teal\"}\n    for label_value in sorted(y.unique()):\n        subset = X[y == label_value]\n        plt.scatter(\n            subset[\"x_coord\"], subset[\"y_coord\"],\n            c=colors[label_value],\n            edgecolor=\"k\",\n            s=50,\n            label=f\"Class {label_value}\"\n        )\n    plt.xlabel(\"x_coord\")\n    plt.ylabel(\"y_coord\")\n    plt.title(title)\n    plt.legend()\n\n\n# Plot for both models side by side\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplot_decision_boundary(log_reg_model, X, y, \"Logistic Regression Decision Boundary\")\n\nplt.subplot(1, 2, 2)\nplot_decision_boundary(nn_model, X, y, \"Neural Network Decision Boundary\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/python_workshop/python_workshop.html#python-fundamentals",
    "href": "topics/python_workshop/python_workshop.html#python-fundamentals",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Get familiar with Python’s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n\n\nLists and dictionaries\nif, for, and while statements\nWriting and calling functions\n\n\n\n\nConcept:\n- Lists are ordered collections. Each item has a position called an index (starting from 0).\n- You can get a single element using list[index] (e.g., scores[0] → first element).\n- You can get a slice using list[start:stop], which returns elements from start up to but not including stop.\n- Dictionaries store key–value pairs. You look up a value by its key (like a label).\n- You can add new keys or update existing ones using dict[key] = value.\n- Keys must be unique; values can be any data type.\n\n# A list of exam scores for Alice\nscores_alice = [88, 92, 79]\n\n# Indexing: position 0 is the first score\nprint(\"First score:\", scores_alice[0])\n\nFirst score: 88\n\n# Slicing: [:2] means start at index 0, stop before index 2 → positions 0 and 1\nprint(\"First two scores:\", scores_alice[:2])\n\nFirst two scores: [88, 92]\n\n# A dictionary mapping student names to their list of scores\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70],\n}\n\n# Lookup: use the key (student's name) to get the list of scores\nprint(\"Carol's scores:\", student_scores[\"Carol\"])\n\nCarol's scores: [90, 85, 95]\n\n# Add a new student by assigning to a new key\nstudent_scores[\"Eve\"] = [85, 88, 91]\n\n# Update an existing student's scores (overwrites the old list)\nstudent_scores[\"Bob\"] = [78, 84, 82]\n\n# Get all keys (student names) and values (lists of scores)\nprint(\"Students:\", list(student_scores.keys()))\n\nStudents: ['Alice', 'Bob', 'Carol', 'Dave', 'Eve']\n\nprint(\"Sample scores:\", list(student_scores.values())[:2])  # [:2] → first two values\n\nSample scores: [[88, 92, 79], [78, 84, 82]]\n\n\n\n\n\nConcept:\n- if / elif / else lets the program choose actions based on conditions.\n- for loops iterate over items in a collection, letting you process each element in turn.\nWe’ll use the student_scores dictionary from the previous section.\n\n# Example 1: if / elif / else\nx = 87\nif x &gt;= 90:\n    grade = \"A\"\nelif x &gt;= 80:\n    grade = \"B\"\nelse:\n    grade = \"C or below\"\nprint(\"Grade bucket:\", grade)\n\nGrade bucket: B\n\n# Example 2: for loop over a student's scores\n# Reusing the student_scores dictionary from earlier\nscores_alice = student_scores[\"Alice\"]  # a list of Alice's scores\ntotal = 0\nfor s in scores_alice:\n    total = total + s\naverage = total / len(scores_alice)\nprint(\"Alice's average score:\", average)\n\nAlice's average score: 86.33333333333333\n\n\n\n\n\nConcept:\n- A function is a reusable block of code that takes inputs (parameters) and can return an output.\n- Use def function_name(parameters): to define it.\n- Use return to send a result back to the caller.\n- You can reuse loops and calculations inside a function so you don’t repeat the same code.\n\ndef average_score(scores):\n    \"\"\"\n    Calculate the average from a list of scores.\n    \"\"\"\n    total = 0\n    for s in scores:\n        total = total + s\n    return total / len(scores)\n\n# Example: Calculate averages for all students\nfor name in student_scores:  # loops over keys (student names)\n    avg = average_score(student_scores[name])\n    print(f\"{name}: {avg:.2f}\")\n\nAlice: 86.33\nBob: 81.33\nCarol: 90.00\nDave: 73.33\nEve: 88.00"
  },
  {
    "objectID": "topics/python_workshop/python_workshop.html#numerical-computing-with-numpy",
    "href": "topics/python_workshop/python_workshop.html#numerical-computing-with-numpy",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Concept:\nNumPy arrays are like lists but optimized for fast mathematical operations.\n- 1D array → like a row of numbers.\n- 2D array → like a table (rows × columns).\n- .shape tells you the size of the array.\n\n# 1D array: vector of exam scores\nscores_1d = np.array([88, 92, 79])\nprint(\"1D array:\", scores_1d)\n\n1D array: [88 92 79]\n\nprint(\"Shape:\", scores_1d.shape)  # (3,) → 3 elements in 1 dimension\n\nShape: (3,)\n\n# 2D array: scores for two students across three exams\nscores_2d = np.array([\n    [88, 92, 79],  # student 1\n    [75, 83, 80]   # student 2\n])\nprint(\"\\n2D array:\\n\", scores_2d)\n\n\n2D array:\n [[88 92 79]\n [75 83 80]]\n\nprint(\"Shape:\", scores_2d.shape)  # (2, 3) → 2 rows, 3 columns\n\nShape: (2, 3)\n\n# Now convert student_scores dictionary values into a 2D array\ngrades_matrix = np.array(list(student_scores.values()))\nprint(\"\\nGrades matrix:\\n\", grades_matrix)\n\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)  # rows = students, columns = exams\n\nShape: (5, 3)\n\n\n\n\nConcept:\n- Indexing works like Python lists: array[row_index, col_index] (0-based).\n- Slicing lets you select a range: start:stop returns elements from start up to (but not including) stop.\n- You can slice rows, columns, or both.\n\n# Example array for reference\nprint(\"Grades matrix:\\n\", grades_matrix)\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\n# Get the score of the first student in the first exam\nprint(\"\\nFirst student's first exam score:\", grades_matrix[0, 0])\n\n\nFirst student's first exam score: 88\n\n# Get all exam scores for the second student (row index 1)\nprint(\"Second student's scores:\", grades_matrix[1, :])\n\nSecond student's scores: [78 84 82]\n\n# Get all scores for the third exam (column index 2)\nprint(\"Scores in third exam:\", grades_matrix[:, 2])\n\nScores in third exam: [79 82 95 70 91]\n\n# Slice: first two students' scores\nprint(\"First two students' scores:\\n\", grades_matrix[0:2, :])\n\nFirst two students' scores:\n [[88 92 79]\n [78 84 82]]\n\n# Slice: first two exams for all students\nprint(\"First two exams for all students:\\n\", grades_matrix[:, 0:2])\n\nFirst two exams for all students:\n [[88 92]\n [78 84]\n [90 85]\n [72 78]\n [85 88]]\n\n\n\n\n\nConcept:\n- Element-wise operations apply a calculation to each element of an array.\n- Broadcasting lets NumPy apply operations between arrays of different shapes by “stretching” one to match the other (without copying data).\n- This is much faster and cleaner than using Python loops.\n\n# Add 5 points to every score (element-wise addition)\nprint(\"Original:\\n\", grades_matrix)\n\nOriginal:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"\\n+5 to every score:\\n\", grades_matrix + 5)\n\n\n+5 to every score:\n [[ 93  97  84]\n [ 83  89  87]\n [ 95  90 100]\n [ 77  83  75]\n [ 90  93  96]]\n\n# Multiply all scores by 1.1 to simulate a 10% bonus\nprint(\"\\n10% bonus:\\n\", grades_matrix * 1.1)\n\n\n10% bonus:\n [[ 96.8 101.2  86.9]\n [ 85.8  92.4  90.2]\n [ 99.   93.5 104.5]\n [ 79.2  85.8  77. ]\n [ 93.5  96.8 100.1]]\n\n# Broadcasting: subtract the minimum score in each column (exam) from that column\nmin_scores_per_exam = grades_matrix.min(axis=0)  # shape: (n_exams,)\nprint(\"\\nMinimum scores per exam:\", min_scores_per_exam)\n\n\nMinimum scores per exam: [72 78 70]\n\nadjusted = grades_matrix - min_scores_per_exam  # broadcasting happens here\nprint(\"\\nScores adjusted by exam minimum:\\n\", adjusted)\n\n\nScores adjusted by exam minimum:\n [[16 14  9]\n [ 6  6 12]\n [18  7 25]\n [ 0  0  0]\n [13 10 21]]\n\n\n\n\n\nConcept:\n- Matrix multiplication (np.dot) combines rows and columns, often used in deep learning layers to combine inputs with weights.\n- Axis-based operations let you apply functions (mean, sum, etc.) across rows or columns:\n\naxis=0 → operate down columns (across rows)\naxis=1 → operate across columns (per row)\n\nExtra notes: - np.ones(shape) creates an array of ones with the given shape.\n- Here we use it for equal weights when averaging scores: each exam gets the same weight.\n- .flatten() converts a multi-dimensional array into a 1D array.\n- After matrix multiplication, the result might be shape (n_students, 1); flattening makes it easier to print and work with.\n\n\n\nMatrix multiplication producing a column vector, then flattening to a 1D array. Here, each row of the grades matrix is multiplied by the weights vector to produce a single average score per student.\n\n\n\n# Example: equal-weight average across exams (axis=1 → per student)\navg_scores_axis = grades_matrix.mean(axis=1)\nprint(\"Average score per student (axis=1):\", [f\"{x:.2f}\" for x in avg_scores_axis])\n\nAverage score per student (axis=1): ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Example: average score per exam (axis=0 → per exam)\navg_scores_exam = grades_matrix.mean(axis=0)\nprint(\"Average score per exam (axis=0):\", [f\"{x:.2f}\" for x in avg_scores_exam])\n\nAverage score per exam (axis=0): ['82.60', '85.40', '83.40']\n\n# Using matrix multiplication to compute averages\nn_exams = grades_matrix.shape[1]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\naverages_via_dot = np.dot(grades_matrix, weights).flatten()\nprint(\"\\nAverages via matrix multiplication:\",\n      [f\"{x:.2f}\" for x in averages_via_dot])\n\n\nAverages via matrix multiplication: ['86.33', '81.33', '90.00', '73.33', '88.00']\n\n# Weighted sum example: suppose exams have weights 0.5, 0.3, 0.2\nexam_weights = np.array([0.5, 0.3, 0.2]).reshape(-1, 1)  # shape: (n_exams, 1)\nweighted_scores = np.dot(grades_matrix, exam_weights).flatten()\nprint(\"\\nWeighted average per student:\",\n      [f\"{x:.2f}\" for x in weighted_scores])\n\n\nWeighted average per student: ['87.40', '80.60', '89.50', '73.40', '87.10']\n\n\n\n\n\nConcept:\n- A NumPy array is efficient for numerical operations but has no column or row labels — you must remember indexes yourself.\n- A pandas DataFrame wraps a NumPy array with labels (row and column names), allowing:\n\nEasier indexing by name (df[\"Math\"]) instead of position.\nMixed data types in one table (numbers, text, dates).\nBuilt-in data inspection methods (.head(), .info(), .describe()).\n\nKey point: Deep learning libraries often use NumPy arrays internally, but pandas is more convenient for data cleaning and exploration.\n\n# Our NumPy grades_matrix (from Section 3)\nprint(\"NumPy array:\\n\", grades_matrix)\n\nNumPy array:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n\nprint(\"Shape:\", grades_matrix.shape)\n\nShape: (5, 3)\n\n# Convert to DataFrame with labels\nexam_names = [\"Exam 1\", \"Exam 2\", \"Exam 3\"]\nstudent_names = list(student_scores.keys())\ngrades_df = pd.DataFrame(grades_matrix, index=student_names, columns=exam_names)\n\nprint(\"\\nDataFrame:\\n\", grades_df)\n\n\nDataFrame:\n        Exam 1  Exam 2  Exam 3\nAlice      88      92      79\nBob        78      84      82\nCarol      90      85      95\nDave       72      78      70\nEve        85      88      91\n\n# Accessing data\nprint(\"\\nScore of Carol in Exam 2 (by labels):\", grades_df.loc[\"Carol\", \"Exam 2\"])\n\n\nScore of Carol in Exam 2 (by labels): 85\n\nprint(\"Score of Carol in Exam 2 (by position):\", grades_matrix[2, 1])\n\nScore of Carol in Exam 2 (by position): 85\n\n# Quick stats for each exam\nprint(\"\\nExam averages:\\n\", grades_df.mean().round(2))\n\n\nExam averages:\n Exam 1    82.6\nExam 2    85.4\nExam 3    83.4\ndtype: float64"
  },
  {
    "objectID": "topics/python_workshop/python_workshop.html#working-with-data",
    "href": "topics/python_workshop/python_workshop.html#working-with-data",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Concept:\nIn real projects, we often load datasets from CSV or Excel files.\nPandas DataFrames are perfect for this stage because they:\n\nRead files directly into a labeled table.\nMake it easy to explore and summarize the data.\nAllow quick selection of features (X) and target labels (y) for model training.\n\nWe’ll load a planar dataset with two numeric features (x_coord, y_coord) and a binary label (label).\n\n# Load into DataFrame\nraw_df = pd.read_csv(data_path_here)\n\n# Inspect\nprint(\"Shape:\", raw_df.shape)\n\nShape: (400, 3)\n\nprint(\"\\nFirst 5 rows:\\n\", raw_df.head())\n\n\nFirst 5 rows:\n     x_coord   y_coord  label\n0  1.204442  3.576114      0\n1  0.158710 -1.482171      0\n2  0.095247 -1.279955      0\n3  0.349178 -2.064380      0\n4  0.694150  2.889109      0\n\n# Separate features and target\nfeatures = [\"x_coord\", \"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\ny = raw_df[target].copy()\n\nprint(\"\\nFeatures sample:\\n\", X.head())\n\n\nFeatures sample:\n     x_coord   y_coord\n0  1.204442  3.576114\n1  0.158710 -1.482171\n2  0.095247 -1.279955\n3  0.349178 -2.064380\n4  0.694150  2.889109\n\nprint(\"\\nLabels sample:\\n\", y.head())\n\n\nLabels sample:\n 0    0\n1    0\n2    0\n3    0\n4    0\nName: label, dtype: int64\n\n\n\n\nConcept:\nA scatter plot lets us see how the two features (x_coord, y_coord) relate to the class label.\nIf classes are not linearly separable, a simple logistic regression will likely underperform compared to a neural network.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\n\n# Bright, high-contrast colors\ncolors = {0: \"orange\", 1: \"teal\"}\n\n# Scatter plot\nfor label_value in sorted(y.unique()):\n    subset = X[y == label_value]\n    plt.scatter(\n        subset[\"x_coord\"], subset[\"y_coord\"],\n        c=colors[label_value],\n        edgecolor=\"k\",\n        s=50,\n        label=f\"Class {label_value}\"\n    )\n\nplt.xlabel(\"x_coord\")\nplt.ylabel(\"y_coord\")\nplt.title(\"Planar Dataset by Label\")\nplt.legend(title=\"Label\")\nplt.show()"
  },
  {
    "objectID": "topics/python_workshop/python_workshop.html#logistic-regression-vs-neural-network",
    "href": "topics/python_workshop/python_workshop.html#logistic-regression-vs-neural-network",
    "title": "Python for Deep Learning",
    "section": "",
    "text": "Concept:\nWe’ll train two models on the planar dataset:\n\nLogistic Regression — a single-layer model that produces a linear decision boundary.\nShallow Neural Network — one hidden layer that can model non-linear boundaries.\n\n\n\n\n\n\nThis comparison shows why neural networks can outperform linear models on complex patterns.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Logistic Regression model: single Dense layer\nlog_reg_model = Sequential([\n    Dense(1, activation='sigmoid', input_shape=(2,))\n])\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\nlog_reg_history = log_reg_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nlog_acc = log_reg_model.evaluate(X, y, verbose=0)[1]\n\n\nprint(f\"Logistic Regression Accuracy: {log_acc:.2f} \\n \")\n\nLogistic Regression Accuracy: 0.46 \n \n\n\n\n# Neural Network model: one hidden layer\nnn_model = Sequential([\n    Dense(10, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\nnn_history = nn_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nnn_acc = nn_model.evaluate(X, y, verbose=0)[1]\n\nprint(f\"Neural Network Accuracy:     {nn_acc:.2f}\")\n\nNeural Network Accuracy:     0.70\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract loss values\nlog_loss = log_reg_history.history['loss']\nnn_loss = nn_history.history['loss']\n\n# Extract accuracy values\nlog_acc_hist = log_reg_history.history['accuracy']\nnn_acc_hist = nn_history.history['accuracy']\n\nepochs_range = range(1, len(log_loss) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, log_loss, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_loss, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, log_acc_hist, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_acc_hist, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTo better understand how each model separates the two classes, we will define an auxiliary plotting function called plot_decision_boundary.\nThis function will: 1. Create a grid over the feature space. 2. Use the model to predict the class for each point in the grid. 3. Display the predicted regions as a colored background. 4. Overlay the actual data points on top.\nAfter defining this function, we will call it for both the logistic regression and neural network models to visually compare their decision boundaries.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y, title):\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[\"x_coord\"].min() - 1, X[\"x_coord\"].max() + 1\n    y_min, y_max = X[\"y_coord\"].min() - 1, X[\"y_coord\"].max() + 1\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 300),\n        np.linspace(y_min, y_max, 300)\n    )\n    \n    # Predict over the grid\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(grid_points, verbose=0)\n    Z = (Z &gt; 0.5).astype(int).reshape(xx.shape)\n\n    # Plot contour and points\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Oranges, alpha=0.3)\n    \n    colors = {0: \"orange\", 1: \"teal\"}\n    for label_value in sorted(y.unique()):\n        subset = X[y == label_value]\n        plt.scatter(\n            subset[\"x_coord\"], subset[\"y_coord\"],\n            c=colors[label_value],\n            edgecolor=\"k\",\n            s=50,\n            label=f\"Class {label_value}\"\n        )\n    plt.xlabel(\"x_coord\")\n    plt.ylabel(\"y_coord\")\n    plt.title(title)\n    plt.legend()\n\n\n# Plot for both models side by side\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplot_decision_boundary(log_reg_model, X, y, \"Logistic Regression Decision Boundary\")\n\nplt.subplot(1, 2, 2)\nplot_decision_boundary(nn_model, X, y, \"Neural Network Decision Boundary\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html",
    "href": "topics/multi_layer/multi_layer_credit_score.html",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "",
    "text": "In this tutorial we’ll build and evaluate a multi-layer neural network (MLP) to predict credit default risk (SeriousDlqin2yrs, binary 0/1).\nWe will:\n\nSplit the data with stratification to keep the class balance.\nStandardize numeric features (important for MLP stability).\nDefine a small MLP using ReLU hidden layers and a sigmoid output.\nTrain with binary cross-entropy and report accuracy, AUC, precision, recall, and F1.\nDiscuss practical knobs: epochs, batch size, and early stopping.\n\n\n\n\nHidden layers: ReLU activations learn non-linear decision boundaries efficiently.\n\nOutput layer: Sigmoid maps predictions to probabilities in ([0,1]) for binary classification.\n\nLoss: Binary cross-entropy aligns with Bernoulli likelihood and probabilistic outputs.\n\nMetrics: Accuracy can be misleading with imbalance; AUC, precision, recall, F1 add nuance.\n\n\n\n\n\nTarget: SeriousDlqin2yrs — whether serious delinquency occurred within 2 years (0/1).\nFeatures: 10 standardized numeric predictors (after cleaning and dropping the ID column).\n\n\n\nShow the code\n# Core data handling\nimport os                           # file paths\nimport numpy as np                  # numeric arrays, vectorized ops\nimport pandas as pd                 # dataframes, CSV I/O\n\n# Model selection & preprocessing\nfrom sklearn.model_selection import train_test_split   # train/test split with stratify\nfrom sklearn.preprocessing import StandardScaler       # feature standardization (fit on train only!)\n\n# Deep learning (Keras/TensorFlow)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential                # simple stack model\nfrom tensorflow.keras.layers import Dense, Input       # fully connected layers\nfrom tensorflow.keras import metrics                   # ready-made metrics: AUC, Precision, Recall, F1",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html#overview",
    "href": "topics/multi_layer/multi_layer_credit_score.html#overview",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "",
    "text": "In this tutorial we’ll build and evaluate a multi-layer neural network (MLP) to predict credit default risk (SeriousDlqin2yrs, binary 0/1).\nWe will:\n\nSplit the data with stratification to keep the class balance.\nStandardize numeric features (important for MLP stability).\nDefine a small MLP using ReLU hidden layers and a sigmoid output.\nTrain with binary cross-entropy and report accuracy, AUC, precision, recall, and F1.\nDiscuss practical knobs: epochs, batch size, and early stopping.\n\n\n\n\nHidden layers: ReLU activations learn non-linear decision boundaries efficiently.\n\nOutput layer: Sigmoid maps predictions to probabilities in ([0,1]) for binary classification.\n\nLoss: Binary cross-entropy aligns with Bernoulli likelihood and probabilistic outputs.\n\nMetrics: Accuracy can be misleading with imbalance; AUC, precision, recall, F1 add nuance.\n\n\n\n\n\nTarget: SeriousDlqin2yrs — whether serious delinquency occurred within 2 years (0/1).\nFeatures: 10 standardized numeric predictors (after cleaning and dropping the ID column).\n\n\n\nShow the code\n# Core data handling\nimport os                           # file paths\nimport numpy as np                  # numeric arrays, vectorized ops\nimport pandas as pd                 # dataframes, CSV I/O\n\n# Model selection & preprocessing\nfrom sklearn.model_selection import train_test_split   # train/test split with stratify\nfrom sklearn.preprocessing import StandardScaler       # feature standardization (fit on train only!)\n\n# Deep learning (Keras/TensorFlow)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential                # simple stack model\nfrom tensorflow.keras.layers import Dense, Input       # fully connected layers\nfrom tensorflow.keras import metrics                   # ready-made metrics: AUC, Precision, Recall, F1",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html#data-preprocessing",
    "href": "topics/multi_layer/multi_layer_credit_score.html#data-preprocessing",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nBefore training, we need to prepare the dataset:\n\nRemove rows with missing values.\n\nSeparate features (X) from the target (y = SeriousDlqin2yrs).\n\nSplit into training and test sets (keeping class balance with stratify).\n\nStandardize features so each has mean 0 and variance 1 — this helps the neural net train smoothly.\n\n\n\nShow the code\ndef preprocess_data(df):\n    \"\"\"\n    Clean, split, and scale the dataset for classification.\n    \"\"\"\n    # 1) Remove rows with missing values\n    df = df.dropna()\n\n    # 2) Separate features and target\n    X = df.drop(\"SeriousDlqin2yrs\", axis=1)\n    y = df[\"SeriousDlqin2yrs\"]\n\n    # 3) Train/test split with stratification to preserve class ratio\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42, stratify=y\n    )\n\n    # 4) Standardize features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled  = scaler.transform(X_test)\n\n    return X_train_scaled, X_test_scaled, y_train, y_test\n\n\n\n\nShow the code\n# Path to the dataset (inside Documents/BOI_DL_website/data)\ndata_path = os.path.join(\n    os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n    \"data\\\\credit_data.csv\"\n)\n\n# Load raw dataset\nraw_df = pd.read_csv(data_path)\n\n# Apply preprocessing: clean → split → scale\nX_train, X_test, y_train, y_test = preprocess_data(raw_df)",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html#model-architecture-training-setup",
    "href": "topics/multi_layer/multi_layer_credit_score.html#model-architecture-training-setup",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "Model architecture & training setup",
    "text": "Model architecture & training setup\nWe’ll build a small MLP (multi-layer perceptron):\n\nInput (10 features): matches the scaled columns produced in preprocessing.\n\nHidden layers: Dense(60, ReLU) → Dense(5, ReLU)\n\nReLU speeds up training and handles nonlinearity well.\n\nWidth 60 then 5 is a compact architecture suitable for tabular credit data.\n\nOutput: Dense(1, Sigmoid) gives a probability for the positive class.\nLoss: binary_crossentropy (standard for binary classification).\n\nOptimizer: Adam with learning rate 1e-3 (robust default).\n\nMetrics: accuracy + AUC, precision, recall, F1 for a fuller picture on imbalanced data.\nTraining: 25 epochs, batch size 32.\n\nInstead of updating weights after every single example (which would be noisy and slow) or after the entire dataset (which would be memory-heavy and inefficient), we train in mini-batches:\n\nEach epoch, the training data is split into groups of 32 samples (mini-batches).\n\nFor each mini-batch, the model computes predictions, calculates loss, and performs a weight update.\n\nThis balances stability (less noisy than 1-sample updates) with efficiency (faster than full-dataset updates).\n\n\n\n\nThe multi-layer neural network used in this tutorial. It takes 10 standardized input features, passes them through two hidden layers (60 and 5 neurons with ReLU activations), and produces a single sigmoid output that represents the probability of default.\n\n\n\n\nShow the code\n# Define a simple feed-forward network for binary classification\nmodel = Sequential([\n    Input(shape=(10,)),                                   # Input vector of length 10 (scaled features)\n    Dense(60, activation=\"relu\", kernel_initializer=\"uniform\"),  # Hidden layer 1: 60 units + ReLU\n    Dense(5,  activation=\"relu\", kernel_initializer=\"uniform\"),  # Hidden layer 2: 5 units + ReLU\n    Dense(1,  activation=\"sigmoid\")                              # Output: probability of positive class\n])\n\n# Compile with binary cross-entropy and a useful metric set for imbalance\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),             # Adam with lr=0.001\n    loss=\"binary_crossentropy\",                           # Suitable for sigmoid output\n    metrics=[\n        \"accuracy\",\n        metrics.AUC(name=\"auc\"),\n        metrics.Precision(name=\"precision\"),\n        metrics.Recall(name=\"recall\"),\n        metrics.F1Score(name=\"f1\")                        # Requires TF &gt;= 2.11\n    ]\n)\n\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=25,                        # training passes over the dataset\n    batch_size=32,                    # mini-batch size\n    # validation_split=0.2,           # (Optional) hold out 20% of train for validation curves\n    # callbacks=[es],                 # (Optional) add early stopping\n    verbose=0\n)",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html#model-evaluation",
    "href": "topics/multi_layer/multi_layer_credit_score.html#model-evaluation",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "Model evaluation",
    "text": "Model evaluation\nAfter training, we test the model on the held-out test set.\nThe evaluate function returns the loss and all metrics we specified in compile (accuracy, AUC, precision, recall, F1).\nPresenting them in a clean, rounded format makes the results easier to interpret.\n\n\nShow the code\n# Evaluate on the test set and return metrics as a dictionary\ntest_metrics = model.evaluate(X_test, y_test, verbose=0, return_dict=True)\n\n# Format nicely: metric name + rounded value\nfor key, value in test_metrics.items():\n    print(f\"{key:&lt;10}: {value:.2f}\")\n\n\naccuracy  : 0.93\nauc       : 0.82\nf1        : 0.13\nloss      : 0.20\nprecision : 0.56\nrecall    : 0.13",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html#training-history",
    "href": "topics/multi_layer/multi_layer_credit_score.html#training-history",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "Training history",
    "text": "Training history\nLooking at metrics across epochs helps us understand model behavior:\n\nLoss curve: should generally decrease; if it rises again, the model may be overfitting.\n\nAccuracy / AUC curves: should increase and stabilize.\n\nPrecision/recall tradeoff: sometimes one rises while the other falls; looking at both is important.\n\nPlotting the training history gives a clear picture of how the network improves during training.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Convert training history to a DataFrame for easy plotting\nhistory_df = pd.DataFrame(history.history)\n\n# Plot loss\nplt.figure(figsize=(6,4))\nplt.plot(history_df[\"loss\"], label=\"Training loss\")\nplt.title(\"Training loss over epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nShow the code\n# Plot accuracy and AUC\nplt.figure(figsize=(6,4))\nplt.plot(history_df[\"accuracy\"], label=\"Accuracy\")\nplt.plot(history_df[\"auc\"], label=\"AUC\")\nplt.title(\"Training metrics over epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Metric value\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/multi_layer/multi_layer_credit_score.html#appendix-metrics-formulas",
    "href": "topics/multi_layer/multi_layer_credit_score.html#appendix-metrics-formulas",
    "title": "Multi layer Neural Network for Credit Score classification",
    "section": "Appendix: Metrics & Formulas",
    "text": "Appendix: Metrics & Formulas\nThis page reports binary classification metrics computed on a held-out test set.\nLet TP, FP, TN, FN be counts from the confusion matrix at a threshold \\(t\\) (often \\(t=0.5\\)).\nLet \\(y_{i} \\in \\{0,1\\}\\) be the true label and \\(\\hat{p}_i \\in [0,1]\\) the model’s predicted probability for the positive class.\n\nBinary Cross-Entropy (Log Loss)\nMeasures the quality of probabilistic predictions (lower is better): \\[\n\\text{BCE} = -\\frac{1}{n}\\sum_{i=1}^{n}\\Big[y_i \\log(\\hat{p}_i) + (1-y_i)\\log\\big(1-\\hat{p}_i\\big)\\Big].\n\\]\n\nProper scoring rule: encourages calibrated probabilities.\nUsed as the training loss for the sigmoid output.\n\n\n\nAccuracy\nShare of correct predictions at threshold (t): \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}.\n\\]\n\nCan be misleading under class imbalance.\n\n\n\nPrecision (Positive Predictive Value)\n“How many predicted positives are truly positive?”\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}.\n\\]\n\n\nRecall (Sensitivity, TPR)\n“How many actual positives did we catch?”\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}.\n\\]\n\n\nF1 Score\nHarmonic mean of precision and recall: \\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n= \\frac{2TP}{2TP + FP + FN}.\n\\]\n\nBalances miss rate (FN) vs. false alarms (FP).\nGeneralization: \\(F_\\beta\\) weights recall \\(\\beta\\) times more than precision:\n\n\\[\nF_\\beta = (1+\\beta^2)\\,\\frac{\\text{Precision}\\cdot\\text{Recall}}{(\\beta^2\\cdot\\text{Precision})+\\text{Recall}}.\n\\]\n\n\nAUC (ROC-AUC)\nThreshold-free measure of ranking quality (higher is better).\n- ROC curve: plot \\(\\text{TPR}=\\frac{TP}{TP+FN}\\) vs. \\(\\text{FPR}=\\frac{FP}{FP+TN}\\) as \\(t\\) varies.\n\nAUC is the area under the ROC curve and equals the probability a random positive is ranked above a random negative: \\[\n\\text{AUC} = \\Pr\\big(\\hat{p}^+ &gt; \\hat{p}^-\\big).\n\\]\n\n\n\nPractical Notes\n\nThreshold choice ((t)) trades precision vs. recall; tune (t) to business costs or by maximizing a metric (e.g., F1) on validation data.\nImbalanced data: rely less on accuracy; prefer AUC, PR curves, F1, and class-specific error analysis.\nCalibration: well-calibrated \\(\\hat{p}\\) improves decision-making when costs vary",
    "crumbs": [
      "Regression"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html",
    "href": "topics/exercises/rnn_exercise.html",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "",
    "text": "This exercise guides you through training a simple Recurrent Neural Network (RNN) to forecast a univariate CPI series. You will work with the series: Seasonal_2\nThe workflow includes:",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#load-the-seasonal_2-series",
    "href": "topics/exercises/rnn_exercise.html#load-the-seasonal_2-series",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "1. Load the Seasonal_2 Series",
    "text": "1. Load the Seasonal_2 Series\nLoad the file cpi_series.csv from your local directory and filter the data to include only rows where item_id == \"Seasonal_2\".\nTask 1 Create these objects:\n\nseries — the full target series\ntrain_raw — first 80% of observations\ntest_raw — last 20% of observations\n\nReport the lengths of each.",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#visualize-the-raw-time-series",
    "href": "topics/exercises/rnn_exercise.html#visualize-the-raw-time-series",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "2. Visualize the Raw Time Series",
    "text": "2. Visualize the Raw Time Series\nPlot Seasonal_2 over time to understand its behavior.\nYour plot should include:\n\nA proper datetime conversion for timestamp\nA line plot of target over time\nAxis labels\nA title\n\nTask 2 Produce a clean, readable plot of the raw data.",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#preprocess-the-series",
    "href": "topics/exercises/rnn_exercise.html#preprocess-the-series",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "3. Preprocess the Series",
    "text": "3. Preprocess the Series\nApply the following transformations:\n\nOptional log transform\nScaling using StandardScaler\nSliding window creation\n\nUse the window size:\nW = 24\nThis means the model will use the past 24 observations to predict the next one.\nTask 3 Implement:\n\nA preprocessing function (log transform + scaling)\nA windowing function to generate (X_train, y_train) and (X_test, y_test)\nEnsure the inputs have shape (samples, W, 1)",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#build-a-simplernn-model",
    "href": "topics/exercises/rnn_exercise.html#build-a-simplernn-model",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "4. Build a SimpleRNN Model",
    "text": "4. Build a SimpleRNN Model\nCreate a minimal RNN architecture with:\n\nSimpleRNN layer (32 units, ReLU activation)\nDense output layer with 1 unit\nAdam optimizer\nMSE loss\n\nTask 4\nWrite a function build_rnn_model(W) that returns a compiled model. Print a model summary and verify the layer dimensions.",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#train-the-model",
    "href": "topics/exercises/rnn_exercise.html#train-the-model",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "5. Train the Model",
    "text": "5. Train the Model\nTrain the model using:\n\n50 epochs\nBatch size 16\nValidation split: 0.1\n\nTrack training and validation loss.\nTask 5\nFit the model and plot the learning curves. Comment on whether the model shows stable learning, underfitting, or overfitting.",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#evaluate-the-forecast",
    "href": "topics/exercises/rnn_exercise.html#evaluate-the-forecast",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "6. Evaluate the Forecast",
    "text": "6. Evaluate the Forecast\nEvaluate the model by:\n\nPredicting on the full test window set\nInverting the scaling/log transformation\nPlotting predicted vs actual values\nComputing error metrics:\n\n\nMAE\nRMSE\n\nTask 6\nProduce the final forecast plot and compute MAE and RMSE on the inverted predictions.",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/rnn_exercise.html#reflection-questions",
    "href": "topics/exercises/rnn_exercise.html#reflection-questions",
    "title": "Practice Exercise: Recurrent Neural Network (RNN) for Time Series",
    "section": "7. Reflection Questions",
    "text": "7. Reflection Questions\nAnswer the following:\n\nDoes the RNN capture short-term patterns in Seasonal_2?\nWould using a larger or smaller window improve results?\nWhere do predictions deviate most from actual values?\nWhich preprocessing step (log, scaling, differencing) seems most important for this series?",
    "crumbs": [
      "RNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html",
    "href": "topics/exercises/cnn_exercise.html",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "In this exercise you will:\n\nDownload the MNIST digit dataset from Kaggle.\n\nPerform a short exploratory data analysis (EDA).\n\nBuild, train, and evaluate a basic Convolutional Neural Network (CNN).\n\nUse Python (Google Colab) and TensorFlow/Keras.\n\n\nIn this exercise we will use Kaggle’s Digit Recognizer dataset, which is a CSV version of the MNIST handwritten digits.\n\n\n\nGo to https://www.kaggle.com.\nLog in to your Kaggle account.\nIn the search bar, type: Digit Recognizer.\nOpen the competition named “Digit Recognizer”.\nGo to the Data tab.\nDownload the following files:\n\ntrain.csv\ntest.csv (optional, for later use such as Kaggle submission)\n\n\nTask 1\nMake sure you have the file: cnn_mnist_kaggle/data/train.csv ready for the next step.\n\n\n\n\n\n\n\nLoad train.csv into a DataFrame.\nPrint its dimensions (number of rows and columns).\nDisplay the first 5 rows to understand the structure.\n\n\n\n\n\nDisplay basic dataset info (dtypes, missing values, etc.).\nReview summary statistics of the pixel columns.\nCount how many examples there are for each digit (0–9).\nPlot a bar chart showing the frequency of each digit.\n\n\n\n\n\nSelect at least three rows from the dataset.\nExtract the pixel values and reshape them into 28×28 images.\nDisplay each image along with its true label.\nBriefly describe what you observe (e.g., clarity, noise, differences between digits).\n\nTask 2\nPerform a complete EDA: inspect structure, analyze label distribution, and visualize several example digits.\n\n\n\n\n\n\n\nSplit the dataset into:\n\nX: all pixel columns.\ny: the label column.\n\nVerify their shapes.\n\n\n\n\n\nSplit the data into a training set and a validation set (for example: 80% / 20%).\nMake sure the split preserves class proportions (stratified split).\nRecord the final sample counts for each split.\n\n\n\n\n\nReshape each observation from a flat vector of 784 values into a 28×28×1 tensor.\nConvert pixel values to floats.\nNormalize pixel values to the range [0, 1].\n\n\n\n\n\nConvert the labels into one-hot encoded vectors with 10 classes (0–9).\nConfirm the resulting shape of the encoded labels.\n\nTask 3\nPrepare the data so it is ready to be fed into a CNN: split, reshape, normalize, and one-hot encode.\n\n\n\n\n\n\nConstruct a simple CNN model using the following structure:\n\nInput layer: images of shape 28×28×1\n\nConvolution + Activation:\n\nA convolutional layer (e.g., 32 filters, 3×3 kernel)\n\nReLU activation\n\n\nMax-Pooling:\n\n2×2 pooling window\n\n\nSecond Convolution + Activation:\n\nA second convolutional layer (e.g., 64 filters, 3×3 kernel)\n\nReLU activation\n\n\nSecond Max-Pooling\n\nFlatten layer to convert feature maps to a vector\n\nFully connected (Dense) layer with a chosen number of units (e.g., 128)\n\nOutput layer with 10 units (digits 0–9) and softmax activation\n\n\n\n\n\nPrint a model summary to verify the layer shapes.\n\nConfirm that the number of parameters makes sense.\n\nTask 4\nBuild a CNN following the architecture above and generate a model summary.\n\n\n\n\n\n\n\nChoose an optimizer (e.g., Adam).\nUse an appropriate loss function for multi-class classification (e.g., categorical crossentropy).\nTrack accuracy as the evaluation metric.\nConfirm that the model compiles without errors.\n\n\n\n\n\nTrain the model for a chosen number of epochs (e.g., 5–10).\nUse a reasonable batch size (e.g., 64 or 128).\nProvide the validation set for monitoring performance.\nRecord training and validation accuracy after each epoch.\nObserve whether the model is overfitting, underfitting, or training as expected.\n\nTask 5\nCompile the CNN and train it while monitoring accuracy and loss on both training and validation sets.\n\n\n\n\n\n\n\nPlot the training accuracy vs. epochs.\nPlot the validation accuracy vs. epochs.\n(Optional) Also plot training and validation loss.\nComment on the learning curves:\n\nIs validation accuracy close to training accuracy?\nDo you see signs of overfitting?\nIs more training needed?\n\n\n\n\n\n\nEvaluate the final model on the validation set and record:\n\nValidation accuracy\n\nValidation loss\n\nGenerate predictions for the validation set.\nConvert probabilities to class labels (0–9).\n\n\n\n\n\nCreate a confusion matrix of true vs. predicted labels.\nIdentify which digits are most frequently misclassified.\nLook at a few misclassified examples:\n\nDisplay the image\nShow the true label and predicted label\nBriefly explain what might have caused the mistake\n\n\nTask 6\nEvaluate the model quantitatively (accuracy, confusion matrix) and qualitatively (misclassified examples), and interpret the results.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html#download-mnist-data-from-kaggle",
    "href": "topics/exercises/cnn_exercise.html#download-mnist-data-from-kaggle",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "In this exercise we will use Kaggle’s Digit Recognizer dataset, which is a CSV version of the MNIST handwritten digits.\n\n\n\nGo to https://www.kaggle.com.\nLog in to your Kaggle account.\nIn the search bar, type: Digit Recognizer.\nOpen the competition named “Digit Recognizer”.\nGo to the Data tab.\nDownload the following files:\n\ntrain.csv\ntest.csv (optional, for later use such as Kaggle submission)\n\n\nTask 1\nMake sure you have the file: cnn_mnist_kaggle/data/train.csv ready for the next step.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html#load-and-inspect-the-data-eda",
    "href": "topics/exercises/cnn_exercise.html#load-and-inspect-the-data-eda",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "Load train.csv into a DataFrame.\nPrint its dimensions (number of rows and columns).\nDisplay the first 5 rows to understand the structure.\n\n\n\n\n\nDisplay basic dataset info (dtypes, missing values, etc.).\nReview summary statistics of the pixel columns.\nCount how many examples there are for each digit (0–9).\nPlot a bar chart showing the frequency of each digit.\n\n\n\n\n\nSelect at least three rows from the dataset.\nExtract the pixel values and reshape them into 28×28 images.\nDisplay each image along with its true label.\nBriefly describe what you observe (e.g., clarity, noise, differences between digits).\n\nTask 2\nPerform a complete EDA: inspect structure, analyze label distribution, and visualize several example digits.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html#prepare-the-data-for-the-cnn",
    "href": "topics/exercises/cnn_exercise.html#prepare-the-data-for-the-cnn",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "Split the dataset into:\n\nX: all pixel columns.\ny: the label column.\n\nVerify their shapes.\n\n\n\n\n\nSplit the data into a training set and a validation set (for example: 80% / 20%).\nMake sure the split preserves class proportions (stratified split).\nRecord the final sample counts for each split.\n\n\n\n\n\nReshape each observation from a flat vector of 784 values into a 28×28×1 tensor.\nConvert pixel values to floats.\nNormalize pixel values to the range [0, 1].\n\n\n\n\n\nConvert the labels into one-hot encoded vectors with 10 classes (0–9).\nConfirm the resulting shape of the encoded labels.\n\nTask 3\nPrepare the data so it is ready to be fed into a CNN: split, reshape, normalize, and one-hot encode.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html#build-a-basic-convolutional-neural-network-cnn",
    "href": "topics/exercises/cnn_exercise.html#build-a-basic-convolutional-neural-network-cnn",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "Construct a simple CNN model using the following structure:\n\nInput layer: images of shape 28×28×1\n\nConvolution + Activation:\n\nA convolutional layer (e.g., 32 filters, 3×3 kernel)\n\nReLU activation\n\n\nMax-Pooling:\n\n2×2 pooling window\n\n\nSecond Convolution + Activation:\n\nA second convolutional layer (e.g., 64 filters, 3×3 kernel)\n\nReLU activation\n\n\nSecond Max-Pooling\n\nFlatten layer to convert feature maps to a vector\n\nFully connected (Dense) layer with a chosen number of units (e.g., 128)\n\nOutput layer with 10 units (digits 0–9) and softmax activation\n\n\n\n\n\nPrint a model summary to verify the layer shapes.\n\nConfirm that the number of parameters makes sense.\n\nTask 4\nBuild a CNN following the architecture above and generate a model summary.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html#compile-and-train-the-model",
    "href": "topics/exercises/cnn_exercise.html#compile-and-train-the-model",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "Choose an optimizer (e.g., Adam).\nUse an appropriate loss function for multi-class classification (e.g., categorical crossentropy).\nTrack accuracy as the evaluation metric.\nConfirm that the model compiles without errors.\n\n\n\n\n\nTrain the model for a chosen number of epochs (e.g., 5–10).\nUse a reasonable batch size (e.g., 64 or 128).\nProvide the validation set for monitoring performance.\nRecord training and validation accuracy after each epoch.\nObserve whether the model is overfitting, underfitting, or training as expected.\n\nTask 5\nCompile the CNN and train it while monitoring accuracy and loss on both training and validation sets.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/exercises/cnn_exercise.html#evaluate-the-model",
    "href": "topics/exercises/cnn_exercise.html#evaluate-the-model",
    "title": "Practice Exercise: Convolutional Neural Networks with MNIST (Kaggle)",
    "section": "",
    "text": "Plot the training accuracy vs. epochs.\nPlot the validation accuracy vs. epochs.\n(Optional) Also plot training and validation loss.\nComment on the learning curves:\n\nIs validation accuracy close to training accuracy?\nDo you see signs of overfitting?\nIs more training needed?\n\n\n\n\n\n\nEvaluate the final model on the validation set and record:\n\nValidation accuracy\n\nValidation loss\n\nGenerate predictions for the validation set.\nConvert probabilities to class labels (0–9).\n\n\n\n\n\nCreate a confusion matrix of true vs. predicted labels.\nIdentify which digits are most frequently misclassified.\nLook at a few misclassified examples:\n\nDisplay the image\nShow the true label and predicted label\nBriefly explain what might have caused the mistake\n\n\nTask 6\nEvaluate the model quantitatively (accuracy, confusion matrix) and qualitatively (misclassified examples), and interpret the results.",
    "crumbs": [
      "CNN"
    ]
  },
  {
    "objectID": "topics/cnn/convolution.html",
    "href": "topics/cnn/convolution.html",
    "title": "Convolutional Neural Network (CNN) for Image Classification",
    "section": "",
    "text": "Build, train, and evaluate a Convolutional Neural Network using the Sign Language MNIST dataset.\n\nImport and Load Data\n\nImport necessary libraries, including TensorFlow and Keras layers.\nLoad the training and test datasets from CSV files.\n\n\n\nShow the code\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n\n\nfrom tensorflow.keras import backend as K\n\n\nfrom tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\nfrom tf_keras_vis.utils.scores import CategoricalScore\nfrom tf_keras_vis.utils.model_modifiers import ReplaceToLinear\nfrom tf_keras_vis.utils import normalize\nfrom tf_keras_vis.saliency import Saliency\n\n\n\n\nShow the code\n\ntrain_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_train.csv\"))\n\ntest_set = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n\"data\\\\sign_mnist_test.csv\"))\n\n\n\n\nPreprocessing Labels\n\nThe Sign Language MNIST dataset contains labels for letters A–Z, but the letter J has no images.\nBecause of this, the dataset’s labels skip the index 9.\nTo make the labels continuous (0–23), we:\n\nKeep labels 0–8 as they are.\nSubtract 1 from all labels ≥ 10.\n\nThen we apply one-hot encoding to obtain 24 output classes.\nA helper function (reverse_remap) converts predictions back to the original A–Z index range when needed.\n\n\n\nShow the code\n\ndef preprocess_data(df, img_height=28, img_width=28):\n  \n  processed_df = df / 255.0\n\n  processed_df = processed_df.values.reshape(-1, img_height, img_width, 1).copy()\n\n  return processed_df\n\n\ndef preprocess_labels(label_series, num_classes=24):\n    \"\"\"\n    Remaps labels to skip index 9 (J) and applies one-hot encoding.\n\n    Parameters:\n    - label_series: a pandas Series or 1D array of labels (originally 0–25, with 9 missing)\n    - num_classes: total number of actual classes (default 24)\n\n    Returns:\n    - One-hot encoded labels of shape (n_samples, num_classes)\n    \"\"\"\n    labels = np.array(label_series)\n\n    remapped_labels = np.array([l - 1 if l &gt; 9 else l for l in labels])\n\n    categorical_labels = to_categorical(remapped_labels, num_classes=num_classes)\n\n    return categorical_labels\n\n\n# Reverse the earlier remapping: add 1 to all labels ≥ 9\ndef reverse_remap(labels):\n    return [l + 1 if l &gt;= 9 else l for l in labels]\n\n\n\ndef show_predictions(x_data, y_true, y_pred, indices=None, n=6):\n    if indices is None:\n        indices = np.random.choice(len(x_data), n, replace=False)\n\n    plt.figure(figsize=(12, 6))\n    for i, idx in enumerate(indices):\n        plt.subplot(2, n // 2, i + 1)\n        plt.imshow(x_data[idx].reshape(28, 28), cmap='gray')\n        plt.title(f\"Pred: {y_pred[idx]}\\nTrue: {y_true[idx]}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nShow the code\n\n\n# Separate labels\ny_train = preprocess_labels(train_set['label'])\n\ny_test = preprocess_labels(test_set['label'])\n\n# Remove labels from the pixel data\nx_train = preprocess_data(train_set.drop('label', axis=1))\n\nx_test = preprocess_data(test_set.drop('label', axis=1))\n\n\n\n\nDefine CNN Model\n\nBuild a Convolutional Neural Network using the Keras Functional API:\n\nAn input layer for images of shape (28, 28, 1).\nBlock 1: two Conv2D layers with 32 filters (3×3, ReLU, padding=‘same’), followed by a MaxPooling2D layer (2×2).\nBlock 2: two Conv2D layers with 64 filters (3×3, ReLU, padding=‘same’).\nBlock 3: two Conv2D layers with 128 filters (3×3, ReLU, padding=‘same’).\nA Flatten layer to convert feature maps into a vector.\nA Dense layer with 256 units and ReLU activation.\nA Dropout layer with rate 0.3 to reduce overfitting.\nA final Dense layer with 24 units and softmax activation for classification.\n\nCompile the model using the Adam optimizer and the categorical cross-entropy loss function.\n\n\n\nShow the code\n# Input\ninputs = Input(shape=(28, 28, 1))\n\n# Block 1\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\n\n# Block 2\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n\n# Block 3\nx = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n\n# Dense head\nx = Flatten()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.3)(x)\n\n# Output\noutputs = Dense(24, activation='softmax')(x)\n\n# Final model\nmodel = Model(inputs, outputs)\n\n# Compile\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n\n\n\nTrain the Model\n\nFit the model on the training data for 5 epochs with batch size 128.\nUse the test data as validation during training.\n\n\n\nShow the code\n\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_test, y_test),\n    epochs=10,\n    batch_size=128,\n    verbose = 0\n)\n\n\n\n\nEvaluate the Model\n\nEvaluate the model on the test data.\nReport test accuracy.\n\n\n\nShow the code\nloss, accuracy = model.evaluate(x_test, y_test)\n\n\nShow the code\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n\n\n\nVisualizing Model Decisions with Grad-CAM\nConvolutional Neural Networks can achieve high accuracy, but it is often unclear which parts of an image drive their predictions. To make the model more interpretable, we use visualization techniques such as Grad-CAM, Guided Backpropagation, and Guided Grad-CAM.\n\nGrad-CAM (Gradient-weighted Class Activation Mapping)\nHighlights the image regions that had the strongest influence on the model’s predicted class. It does this by examining the gradients flowing into the last convolutional layer, which contains spatial information.\nGuided Backpropagation\nComputes fine-grained pixel-level gradients that show which pixels most strongly support the predicted class. The result is a high-resolution saliency map.\nGuided Grad-CAM\nCombines the coarse, region-level information from Grad-CAM with the fine details from Guided Backpropagation.\nThe result is a high-resolution heatmap that highlights exactly which parts of the image contributed to the model’s decision.\n\nBelow, we apply all three methods to a selected test image to see what the model focused on when predicting the hand sign.\nThe figure below shows four visualizations side by side:\n\nOriginal Image – the input image given to the model.\nGrad-CAM – a coarse heatmap showing which regions of the image contributed most to the model’s prediction.\nGuided Backpropagation – a fine-grained gradient map showing which pixels support the predicted class.\nGuided Grad-CAM – an overlay combining Grad-CAM with Guided Backprop to produce a high-resolution explanation.\n\n\n\nShow the code\n# Choose which test image to analyze\nidx = 3\nimage = x_test[idx:idx+1]  # shape (1, 28, 28, 1)\n\n\nfor layer in reversed(model.layers):\n    if isinstance(layer, tf.keras.layers.Conv2D):\n        last_conv_layer_name = layer.name\n        break\n\n\n# ---- 1. Predict class ----\npred_probs = model.predict(image, verbose=0)[0]\npred_index = np.argmax(pred_probs)\nprint(\"Predicted class:\", pred_index)\n\n\nPredicted class: 0\n\n\nShow the code\n# ---- 2. Grad-CAM ----\nscore = CategoricalScore(pred_index)\n\ngradcam = Gradcam(\n    model,\n    model_modifier=None,\n    clone=True\n)\n\ncam = gradcam(\n    score,\n    image,\n    penultimate_layer=last_conv_layer_name\n)\nheatmap = cam[0]   # remove batch dimension\n\n\n\ndef model_modifier(m):\n    # Replace the final softmax activation with linear\n    m.layers[-1].activation = tf.keras.activations.linear\n\n\n# ---- 3. Guided Backprop ----\nsaliency = Saliency(\n    model,\n    model_modifier=model_modifier,\n    clone=True\n)\n\ngbp = saliency(score, image)   # guided backprop gradients, shape (1, 28, 28, N)\n\n\nC:\\Users\\Home\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\keras\\src\\models\\functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: keras_tensor\nReceived: inputs=['Tensor(shape=(1, 28, 28, 1))']\n  warnings.warn(msg)\n\n\nShow the code\ngbp = gbp[0]                   # remove batch dimension → shape (28, 28, N)\ngbp = normalize(gbp)           # rescale 0–1\n\n\n\n# ---- 4. Guided Grad-CAM (Grad-CAM × Guided Backprop) ----\nguided_gradcam = gbp * heatmap\n\n\n\n\nShow the code\n# Normalize Grad-CAM heatmap to 0–1 for display\nheatmap_norm = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n\n# Convert Guided Backprop to a single-channel saliency map\ngbp_gray = normalize(gbp)\n\nplt.figure(figsize=(13, 4))\n\n# Original\nplt.subplot(1, 4, 1)\nplt.title(\"Original\")\nplt.imshow(image[0].reshape(28, 28), cmap='gray')\nplt.axis('off')\n\n\n(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))\n\n\nShow the code\n# Grad-CAM\nplt.subplot(1, 4, 2)\nplt.title(\"Grad-CAM\")\nplt.imshow(heatmap_norm, cmap='jet')\nplt.axis('off')\n\n\n(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))\n\n\nShow the code\n# Guided Backprop\nplt.subplot(1, 4, 3)\nplt.title(\"Guided Backprop\")\nplt.imshow(gbp_gray, cmap='gray')\nplt.axis('off')\n\n\n(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))\n\n\nShow the code\n# Guided Grad-CAM\nplt.subplot(1, 4, 4)\nplt.title(\"Guided Grad-CAM\")\nplt.imshow(image[0].reshape(28, 28), cmap='gray')\nplt.imshow(heatmap_norm, cmap='jet', alpha=0.45)\nplt.axis('off')\n\n\n(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))\n\n\nShow the code\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/exercises/exercise_1.html",
    "href": "topics/exercises/exercise_1.html",
    "title": "Neural Networks Workshop — Practice Exercise Set",
    "section": "",
    "text": "Overview\nIn this exercise, you will work with the credit_data dataset and progress step by step through a short exploratory data analysis, a logistic regression baseline, a single-layer neural network, and an optional multi-layer neural network. Each section builds on the previous one so you can follow the workflow without getting lost.\nThe target variable is:\n\nSeriousDlqin2yrs — whether the person experienced financial distress severe enough to be 90+ days delinquent within the next two years.\n\nAll remaining columns serve as predictors.\n\n\n\n1. Load and Inspect the Data\n\n1.1 Load the dataset\nLoad the CSV file into a DataFrame named credit_data.\n\n\n1.2 Inspect its structure\nView:\n\nthe first rows\n\nthe column types\n\nsummary statistics\n\nMake sure SeriousDlqin2yrs contains only 0 and 1.\n\n\n1.3 Identify target and predictors\nConfirm that the target is binary and all other columns are features used for modeling.\n\n\n\n\n2. Handle Missing Values\n\n2.1 Detect missingness\nCount missing values per column and identify which features require imputation.\n\n\n2.2 Choose an imputation strategy\nUse the median for numeric features and explain why this is appropriate for skewed financial data.\n\n\n2.3 Create a cleaned dataset\nApply the transformations and verify that no missing values remain.\n\n\n\n\n3. Exploratory Data Analysis (EDA)\nKeep this focused and practical.\n\n3.1 Examine the target distribution\nPlot the proportion of 0’s and 1’s to understand the level of class imbalance.\n\n\n3.2 Explore important numeric features\nLook at distributions (histograms or boxplots) of a few features such as:\n\nage\n\nDebtRatio\n\nMonthlyIncome\n\nDiscuss any unusual patterns or outliers.\n\n\n3.3 Correlation inspection\nCompute a correlation matrix and observe:\n\nwhich features correlate most strongly with the target\n\nwhich features correlate strongly with each other\n\n\n\n\n\n4. Train/Test Split\n\n4.1 Split the data\nCreate a stratified split such that the test set is 25% of the data.\n\n\n4.2 Scale the predictors\nStandardize the numeric predictors. Fit the scaling on the training data and apply it to both training and test sets.\n\n\n\n\n5. Logistic Regression Baseline\n\n5.1 Fit a logistic regression model\nUse the cleaned, scaled training data.\n\n\n5.2 Evaluate performance\nProduce:\n\naccuracy\n\nprecision\n\nrecall\n\nF1 score\n\nAUC\n\nconfusion matrix\n\n\n\n5.3 Interpret the results\nDiscuss at least one strength and one limitation of logistic regression in the context of this dataset.\n\n\n\n\n6. Single-Layer Neural Network\nYou will now build a shallow neural network with one hidden layer.\n\n6.1 Define the architecture\nUse: - one hidden layer with a small number of units\n- ReLU activation in the hidden layer\n- sigmoid activation in the output layer\n\n\n6.2 Compile the model\nUse binary cross-entropy loss, the Adam optimizer, and accuracy/AUC.\n\n\n6.3 Train the model\nTrain for about 20 epochs with a validation split.\nPlot the training and validation curves and look for signs of overfitting.\n\n\n6.4 Evaluate\nCompare performance to logistic regression using the same metrics.\n\n\n\n\n7. Optional: Multi-Layer Neural Network\nThis section is for students who want to explore deeper models.\n\n7.1 Build a deeper architecture\nUse two or more hidden layers (e.g. 32 → 16 → 1).\n\n\n7.2 Consider regularization\nTry dropout or L2 regularization to reduce overfitting.\n\n\n7.3 Train and compare\nEvaluate your deep model and compare it with: - logistic regression\n- single-layer network\n\n\n7.4 Discuss findings\nExplain whether depth helped or harmed performance and why.\n\n\n\n\n8. Summary Questions\nAnswer the following:\n\nWhich model achieved the highest AUC?\n\nWhich model showed signs of overfitting?\n\nIf you had to deploy one model, which would you choose and why?\n\nWhat additional preprocessing or feature engineering steps could improve model performance?\n\n\n\n\nEnd of Exercise Set\nWork through the tasks in order, verifying each step before moving to the next. This workflow mirrors a real-world modeling pipeline and prepares you for more advanced neural network structures.",
    "crumbs": [
      "Logistic"
    ]
  },
  {
    "objectID": "topics/intro.html",
    "href": "topics/intro.html",
    "title": "Moving beyond linearity",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom patsy import dmatrix\n\nfrom sklearn.linear_model import LinearRegression\n\nimport pygam\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\Wage.csv\"))\n\ny_vec = raw_df[\"wage\"].copy()\n\nx_vec = raw_df[[\"age\"]].copy()\n\n\n\n\nShow the code\n\ndef plot_predictions(pred_df, title):\n  pred_cols = [temp_name for temp_name in pred_df.columns.values if \"pred\" in temp_name]\n\n  plt.figure(figsize=(10, 6))\n  plt.scatter(pred_df[\"age\"], pred_df[\"wage\"], label=\"Actual Wage\", alpha = 0.7)\n\n  for temp_name in pred_cols:\n    plt.scatter(pred_df[\"age\"], pred_df[temp_name], label=temp_name, alpha = 0.7)\n#  plt.scatter(pred_df[\"age\"], pred_df[\"wage_pred\"], label=\"Predicted Wage\", alpha = 0.7)\n  plt.xlabel(\"Age\")\n  plt.ylabel(\"Wage\")\n  plt.title(title)\n  plt.legend()\n  plt.grid(True)\n  plt.show()\n\n\n\nPolynomial regression\n\nConstruct polynomial features:\n\nUsing the dmatrix function, create a feature matrix that includes polynomial terms up to the fourth degree: \\(\\text{age}, \\text{age}^2, \\text{age}^3, \\text{age}^4\\)\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec from the polynomial features.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\n\n\nShow the code\npoly_x_mat = dmatrix(\"age + I(age**2) + I(age**3) + I(age**4)\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(poly_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nFalse\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nShow the code\n\npoly_pred = lin_reg.predict(poly_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": poly_pred}),\n                 \"Polynomial Regression\")\n\n\n\n\n\n\n\n\n\n\n\nStep functions\n\nDefine step function intervals (bins):\n\nUse np.percentile to determine cutoff points (knots) that divide age into quartiles.\n\nUse pd.cut to assign each age value to a bin.\n\nCreate a step function design matrix:\n\nUse the dmatrix function to encode the binned age values as categorical variables.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the step function representation of age.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\nHints:\n* Ensure that the step function bins do not have duplicate edges, which can be handled using duplicates='drop' in pd.cut.\n* You can use matplotlib.pyplot or seaborn to visualize the stepwise fitted function.\n* Since this is a piecewise constant model, expect a regression curve with horizontal segments rather than a smooth curve.\n\n\nShow the code\nknots = np.percentile(x_vec[\"age\"], [0, 15, 25, 50, 75,90, 100])\n\nx_vec_step = pd.cut(x_vec[\"age\"], bins=knots, labels=False,\n                    include_lowest=True,duplicates='drop')\n\nstep_x_mat = dmatrix(\"C(x_vec_step)\", x_vec_step)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(step_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nFalse\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nShow the code\n\nstep_pred = lin_reg.predict(step_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": step_pred}), \"Step Regression\")\n\n\n\n\n\n\n\n\n\n\n\nPiecewise polynomials\n\nDefine the knot location:\n\nSet a single knot at age = 50 to allow for a change in the polynomial relationship at this point.\n\nCreate piecewise polynomial terms:\n\n\nConstruct polynomial terms (age, age², age³) for the entire dataset.\nDefine separate squared and cubic terms for values below and above the knot to allow for discontinuity.\nUse np.maximum to ensure that terms are active only in their respective regions.\n\n\nCreate a design matrix:\n\nCombine all polynomial terms into a matrix that serves as input for the regression model.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the spline-transformed age values.\n\nMake predictions:\nUse the trained model to generate predictions for y_vec.\n\nHints:\n* B-splines create smooth, continuous fits by combining piecewise polynomial segments.\n* You can experiment with additional knots to see how the flexibility of the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curve.\n\n\nShow the code\nknot = 50\n\n# Manually create piecewise terms to enforce discontinuity at the knot\nx_less_knot = np.maximum(0, knot - x_vec)  # For x &lt; knot\nx_greater_knot = np.maximum(0, x_vec - knot)  # For x &gt;= knot\n\n# Combine the terms into a design matrix (manual spline basis)\nspline_x_mat = np.column_stack((x_vec, x_vec**2, x_vec**3, x_less_knot**2,\n                                x_greater_knot**2,x_less_knot**3, x_greater_knot**3))\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nFalse\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nFalse\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nShow the code\n\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": spline_pred}),\n\"Piecewise Spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nSplines\n\nConstruct spline basis matrices:\n\nUse the dmatrix function to create a B-spline basis with knots at age = 25, 40, 60 and a polynomial degree of 3.\nUse the dmatrix function to create a natural spline basis with the same knots.\n\nFit linear regression models:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit one model using the B-spline basis and another using the natural spline basis.\n\nMake predictions:\nUse each trained model to generate predictions for y_vec.\n\nHints:\n* B-splines allow local flexibility, adjusting the curve within defined knots.\n* Natural splines impose additional constraints that make the curve behave more smoothly at the boundaries.\n* Try adjusting the number and position of knots to see how the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curves.\n\n\nShow the code\nspline_x_mat = dmatrix(\"bs(age, knots = [25,40,60], degree=3)\", x_vec)\n\nnatural_spline_x_mat = dmatrix(\"cr(age, knots = [25,40,60])\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nFalse\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(natural_spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nFalse\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nShow the code\n\nnatural_spline_pred = lin_reg.predict(natural_spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec,\n                               \"spline_pred\": spline_pred,\n                               \"natural_spline_pred\": natural_spline_pred}),\n                               \"Spline and Natural spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nGeneral Additive Models\n\nPrepare features\n\nKeep year and age as numeric.\n\nEncode education as a categorical variable.\n\nSpecify the model\n\nUse smooth terms s() for year and age.\n\nUse f() for the categorical factor education.\n\nFit the model\n\nFit a LinearGAM to the training data.\n\n\n\n\nShow the code\nfrom pygam import LinearGAM, s, f\n\n# Encode categorical variable 'education'\nraw_df['education_code'] = raw_df['education'].astype('category').cat.codes\n\n# Features and response\nX = raw_df[['year', 'age', 'education_code']].values\n\ny = raw_df['wage'].values\n\n# Fit the GAM model\n\ngam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=8) + f(2));\n\ngam.fit(X, y);\n\n\n\nVisualize the results\n\nPlot partial dependence for each term:\n\nsmooth curve for year\n\nsmooth curve for age\n\nbar chart for education\n\n\n\n\n\nShow the code\n# Generate smooth predictions for each term\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot the effect of year\nyear_grid = np.linspace(raw_df['year'].min(), raw_df['year'].max(), 100)\nX_year = np.zeros((100, 3))\nX_year[:, 0] = year_grid\naxes[0].plot(year_grid, gam.partial_dependence(0, X=X_year), color='red');\naxes[0].set_title(r\"$f_1(\\mathrm{year})$\");\naxes[0].set_xlabel(\"year\");\naxes[0].set_ylabel(\"Effect\");\naxes[0].set_ylim(-30, 30);\n\n# Plot the effect of age\nage_grid = np.linspace(raw_df['age'].min(), raw_df['age'].max(), 100)\nX_age = np.zeros((100, 3))\nX_age[:, 1] = age_grid\naxes[1].plot(age_grid, gam.partial_dependence(1, X=X_age), color='red');\naxes[1].set_title(r\"$f_2(\\mathrm{age})$\");\naxes[1].set_xlabel(\"age\");\naxes[1].set_ylabel(\"Effect\");\naxes[1].set_ylim(-50, 40);\n\n# Plot the effect of education\neducation_levels = np.sort(raw_df['education_code'].unique())\nX_edu = np.zeros((len(education_levels), 3))\nX_edu[:, 2] = education_levels\naxes[2].bar(\n    raw_df['education'].astype('category').cat.categories,\n    gam.partial_dependence(2, X=X_edu).flatten(),\n    color='red'\n);\naxes[2].set_title(r\"$f_3(\\mathrm{education})$\");\naxes[2].set_xlabel(\"education\");\naxes[2].set_ylabel(\"Effect\");\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html",
    "href": "topics/multi_layer/optim_methods.html",
    "title": "Optimization Methods in Neural Networks",
    "section": "",
    "text": "Implement and evaluate different optimization strategies for training a simple neural network",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#model",
    "href": "topics/multi_layer/optim_methods.html#model",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nlayer_dims = [2, 4, 1]\n\ngd_parameters, gd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=params[\"num_epochs\"],\n    batch_size=X.shape[1],      # full batch for GD\n    print_cost=False,\n    print_every=params[\"num_prints\"]\n)",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#predictions",
    "href": "topics/multi_layer/optim_methods.html#predictions",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_gd = predict_nn(X, gd_parameters)\n\ngd_score = accuracy_score(pred_gd.flatten(),y.flatten())\n\nprint(f\"GD accuracy score is {gd_score}\")\n\n\nGD accuracy score is 0.169",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#model-1",
    "href": "topics/multi_layer/optim_methods.html#model-1",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nsgd_parameters, sgd_costs = model(\n    X, y, layer_dims,\n    optimizer=\"gd\",\n    learning_rate=0.01,\n    num_epochs=params[\"num_epochs\"],\n    batch_size=32,\n    print_cost=False,\n    print_every=params[\"num_prints\"]\n)",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#predictions-1",
    "href": "topics/multi_layer/optim_methods.html#predictions-1",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_sgd = predict_nn(X, sgd_parameters)\n\nsgd_score = accuracy_score(pred_sgd.flatten(),y.flatten())\n\nprint(f\"SGD accuracy score is {sgd_score}\")\n\n\nSGD accuracy score is 0.767",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#model-2",
    "href": "topics/multi_layer/optim_methods.html#model-2",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nmomentum_parameters, momentum_costs = model(\n    X, y, layer_dims,\n    optimizer=\"momentum\",\n    learning_rate=0.01,\n    num_epochs=params[\"num_epochs\"],\n    batch_size=X.shape[1],\n    print_cost=False,\n    print_every=params[\"num_prints\"],\n    beta = 0.9\n)",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#predictions-2",
    "href": "topics/multi_layer/optim_methods.html#predictions-2",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_momentum = predict_nn(X, momentum_parameters)\n\nmomentum_score = accuracy_score(pred_momentum.flatten(),y.flatten())\n\nprint(f\"Momentum accuracy score is {momentum_score}\")\n\n\nMomentum accuracy score is 0.818",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#model-3",
    "href": "topics/multi_layer/optim_methods.html#model-3",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nrmsprop_parameters, rmsprop_costs = model(\n    X, y, layer_dims,\n    optimizer=\"rmsprop\",\n    learning_rate=0.001,\n    num_epochs=params[\"num_epochs\"],\n    batch_size=X.shape[1],\n    print_cost=False,\n    print_every=params[\"num_prints\"],\n    beta2 = 0.9,\n    epsilon = 1 * 10 ** (-8)\n)",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#predictions-3",
    "href": "topics/multi_layer/optim_methods.html#predictions-3",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_rmsprop = predict_nn(X, rmsprop_parameters)\n\nrmsprop_score = accuracy_score(pred_rmsprop.flatten(),y.flatten())\n\nprint(f\"RMSProp accuracy score is {rmsprop_score}\")\n\n\nRMSProp accuracy score is 0.522",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#model-4",
    "href": "topics/multi_layer/optim_methods.html#model-4",
    "title": "Optimization Methods in Neural Networks",
    "section": "Model",
    "text": "Model\n\n\nShow the code\n\nadam_parameters, adam_costs = model(\n    X, y, layer_dims,\n    optimizer=\"adam\",\n    learning_rate=0.001,\n    num_epochs=params[\"num_epochs\"],\n    batch_size=X.shape[1],\n    print_cost=False,\n    print_every=params[\"num_prints\"],\n    beta = 0.9,\n    beta2 = 0.999,\n    epsilon = 1 * 10 ** (-8)\n)",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/multi_layer/optim_methods.html#predictions-4",
    "href": "topics/multi_layer/optim_methods.html#predictions-4",
    "title": "Optimization Methods in Neural Networks",
    "section": "Predictions",
    "text": "Predictions\n\n\nShow the code\npred_adam = predict_nn(X, adam_parameters)\n\nadam_score = accuracy_score(pred_adam.flatten(),y.flatten())\n\nprint(f\"Adam accuracy score is {adam_score}\")\n\n\nAdam accuracy score is 0.618",
    "crumbs": [
      "Optimization methods"
    ]
  },
  {
    "objectID": "topics/rnn/rnn.html",
    "href": "topics/rnn/rnn.html",
    "title": "Recurrent Neural Network for Time Series Forecasting",
    "section": "",
    "text": "Show the code\n\n# === Imports ===\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom statsmodels.tsa.ar_model import AutoReg\n\n# Ensure eager execution (important for reticulate/Quarto)\ntf.config.run_functions_eagerly(True)\n\n\nThis tutorial demonstrates how to use recurrent neural networks to forecast financial time series data. We will work with daily Dow Jones data and predict the next day’s log trading volume using information from the previous ten days. The analysis includes two recurrent neural architectures—RNN and GRU—used to forecast next-day log trading volume.\n\nFeature engineering\nBefore training the models, we perform several feature-engineering steps to extract informative signals from the raw data. We compute the log of trading volume, daily log-returns, and a 10-day rolling estimate of volatility. These features capture trading activity, price movements, and short-term uncertainty, forming the three inputs used by the models.\n\n\nShow the code\n\n# === Load data ===\n# Example: file path to local CSV (adjust to your environment)\ndata_path = os.path.join(\n    os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n    \"data\\\\dow_jones_data.csv\"\n)\nraw_df = pd.read_csv(data_path)\n\n# === Feature engineering ===\ncol_names = [\"Volume\", \"Close\"]\ndata = raw_df[[\"Date\"] + col_names].copy()\ndata['log_volume'] = np.log(data['Volume'])\ndata['return'] = np.log(data['Close'] / data['Close'].shift(1))\ndata['log_volatility'] = np.log(data['return'].rolling(window=10).std() + 1e-6)\ndata = data.dropna().reset_index(drop=True)\ndata = data.drop(col_names, axis=1).copy()\n\n\n\n\nShow the code\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata['Date'] = pd.to_datetime(data['Date'])\n\nN = 300\ndf = data.tail(N)\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 8))\n\ncols = ['log_volume', 'return', 'log_volatility']\n\nfor ax, col in zip(axes, cols):\n    ax.plot(df['Date'], df[col])\n    ax.grid(True)\n\n    # Remove axis ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Add a clean left-side label\n    ax.text(0.01, 0.85, col, transform=ax.transAxes,\n            fontsize=12, va='top', ha='left')\n\n# Suptitle with no overlap\nfig.suptitle(f\"Last {N} Observations\", fontsize=16, y=0.98)\n\nplt.subplots_adjust(top=0.90, hspace=0.35)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe construct input sequences of 10 consecutive time steps, where each sequence is used to predict the following observation. After building these sequences, we split the dataset into training and testing subsets to evaluate model performance.\nBecause neural networks are sensitive to the scale of their inputs, we standardize both the feature sequences and the target variable. Each feature is transformed to have zero mean and unit variance based on the training set only. This ensures stable training and prevents information from the test set from leaking into the model during preprocessing.\n\n\nShow the code\n# === Create sequences ===\ndef create_sequences(df, features, target, window):\n    X, y = [], []\n    for i in range(window, len(df)):\n        X.append(df[features].iloc[i-window:i].values)\n        y.append(df[target].iloc[i])\n    return np.array(X), np.array(y)\n\nfeatures = ['log_volume', 'return', 'log_volatility']\n\ntarget = 'log_volume'\n\nX, y = create_sequences(data, features, target, window = 10)\n\n# === Train/test split ===\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, shuffle=False\n)\n\n# === Scale data ===\nscaler_X = StandardScaler().fit(X_train.reshape(-1, X_train.shape[-1]))\nX_train = scaler_X.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\nX_test = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n\nscaler_y = StandardScaler().fit(y_train.reshape(-1, 1))\ny_train_scaled = scaler_y.transform(y_train.reshape(-1, 1)).flatten()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n\n\nWe now build two recurrent neural network architectures commonly used for sequential data. Each model receives a 10×3 input window (log volume, return, and log volatility) and predicts the next day’s log volume:\n\nSimpleRNN — a basic recurrent unit suitable for short-range dependencies.\n\nGRU — a gated recurrent unit capable of capturing longer patterns with fewer parameters than LSTM.\n\nBoth models use 12 hidden units and are trained for 30 epochs.\n\n\nModels fit\n\n\nShow the code\n\n# ==============================================================\n#  Recurrent Neural Networks for Financial Time Series Forecasting\n#  Forecast next-day log volume using past values of\n#  log_volume, return, and log_volatility\n# ==============================================================\n\n\n# === Build and train models ===\nLEARNING_RATE = 1e-4\nEPOCHS = 30\nBATCH_SIZE = 32\nINPUT_SHAPE = (10, 3)\n\ndef build_model(cell):\n    model = tf.keras.Sequential([\n        cell(12, activation='tanh', input_shape=INPUT_SHAPE),\n        tf.keras.layers.Dense(1)\n    ])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(optimizer=optimizer, loss='mse')\n    return model\n\nmodel_rnn  = build_model(tf.keras.layers.SimpleRNN)\nmodel_gru  = build_model(tf.keras.layers.GRU)\n\ndef train_model(model, X, y):\n    return model.fit(\n        X, y,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_split=0.2,\n        verbose=0\n    )\n\nhist_rnn  = train_model(model_rnn,  X_train, y_train_scaled)\nhist_gru  = train_model(model_gru,  X_train, y_train_scaled)\n\n\n\n\nEvaluation\n\n\nShow the code\n# === Evaluate models ===\ndef evaluate_model(model, X_test, y_test_scaled, label):\n    pred_scaled = model.predict(X_test)\n    pred = scaler_y.inverse_transform(pred_scaled)\n    y_true = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1))\n    rmse = np.sqrt(mean_squared_error(y_true, pred))\n    mae  = mean_absolute_error(y_true, pred)\n    r2   = r2_score(y_true, pred)\n    print(f\"{label:10s} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | R²: {r2:.4f}\")\n    return pred.flatten(), y_true.flatten(), rmse, mae, r2\n\npred_rnn,  y_true, rmse_rnn,  mae_rnn,  r2_rnn  = evaluate_model(model_rnn,\nX_test, y_test_scaled, \"SimpleRNN\")\n\n\nShow the code\npred_gru,  _,      rmse_gru,  mae_gru,  r2_gru  = evaluate_model(model_gru,\nX_test, y_test_scaled, \"GRU\")\n\n\nAfter training all models, we evaluate their performance on the test set using standard regression metrics. These metrics quantify different aspects of forecast accuracy:\n\nRMSE (Root Mean Squared Error): penalizes larger errors more heavily.\n\nMAE (Mean Absolute Error): reflects typical forecast error in the original units.\n\nR² (Coefficient of Determination): measures how much of the variance in the true values is explained by the model.\n\nComparing these values across the two recurrent models allows us to assess how SimpleRNN and GRU differ in accuracy and whether the additional gating mechanisms in GRU provide a measurable improvement.\n\n\nShow the code\nresults = pd.DataFrame({\n    'Model': ['SimpleRNN', 'GRU'],\n    'RMSE': [rmse_rnn, rmse_gru],\n    'MAE':  [mae_rnn, mae_gru],\n    'R²':   [r2_rnn, r2_gru]\n}).sort_values(by=\"RMSE\")\n\nprint(\"\\n=== Model Performance Comparison ===\")\n\n\n\n=== Model Performance Comparison ===\n\n\nShow the code\nprint(results.to_string(index=False))\n\n\n    Model     RMSE      MAE        R²\n      GRU 0.223929 0.156276  0.122918\nSimpleRNN 0.253002 0.179130 -0.119607\n\n\nTo visually compare model behavior, we plot the first 200 predictions from the test set. This shows how closely the SimpleRNN and GRU models follow the true log-volume values over time. A well-performing model should track both the overall level and the short-term fluctuations of the series.\n\n\nShow the code\nplt.figure(figsize=(10, 5));\n\nplt.plot(y_true[:200], label='Actual', color='black', linewidth=2);\nplt.plot(pred_rnn[:200], label='SimpleRNN', alpha=0.8);\nplt.plot(pred_gru[:200], label='GRU', alpha=0.8);\n\nplt.title('Next-day Log Volume Forecast — SimpleRNN vs GRU');\nplt.xlabel('Time Step (Test Period)');\nplt.ylabel('Log Volume');\nplt.legend();\nplt.tight_layout();\nplt.show()",
    "crumbs": [
      "Financial data"
    ]
  },
  {
    "objectID": "topics/single_layer/classification.html",
    "href": "topics/single_layer/classification.html",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "",
    "text": "Implement a simple neural network from scratch and compare its performance to logistic regression on a 2D dataset.\nShow the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "topics/single_layer/classification.html#auxiliary-functions",
    "href": "topics/single_layer/classification.html#auxiliary-functions",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nImplement Training Functions\n\nDefine helper functions for activation, parameter initialization, forward/backward propagation, and parameter update.\n\n\n\nShow the activation function code\nimport numpy as np\n\n\ndef activate(Z, activation_function=\"tanh\"):\n    \"\"\"\n    Apply an activation function elementwise.\n    \"\"\"\n    if activation_function == \"tanh\":\n        return np.tanh(Z)  # squashes values to [-1, 1]\n    elif activation_function == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-Z))  # squashes values to [0, 1]\n    else:\n        raise ValueError(\"activation_function must be 'tanh' or 'sigmoid'.\")\n\n\n\n\nShow the parameters initialization code\nimport numpy as np\n\n\ndef initialize_parameters(X, num_hidden_layer_neurons, scale_const=0.01, seed=1):\n    \"\"\"\n    Initialize weights and biases for a single hidden-layer network.\n    \"\"\"\n    np.random.seed(seed)\n\n    n_features = X.shape[1]  # number of input features\n\n    # Small random weights help avoid saturation of activations at start\n    W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n    b1 = np.zeros((num_hidden_layer_neurons, 1))\n    W2 = np.random.randn(1, num_hidden_layer_neurons) * scale_const\n    b2 = np.zeros((1, 1))\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\n\n\nForward and backward propagation in a single-hidden-layer neural network. The forward pass takes inputs \\(X\\) through weights \\(W^{[1]}, W^{[2]}\\) and biases \\(b^{[1]}, b^{[2]}\\) to produce pre-activations \\(Z^{[1]}, Z^{[2]}\\), activations \\(A^{[1]}, A^{[2]}\\), and final output \\(y\\). The backward pass computes gradients of activations, pre-activations, weights, and biases \\((dA, dZ, dW, db)\\) from the output layer back to the input layer for parameter updates.\n\n\n\n\nShow the forward propagation code\nimport numpy as np\n\n\ndef forward_propagation(parameters, X_adj):\n    \"\"\"\n    Perform one forward pass.\n    \"\"\"\n    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n\n    Z1 = np.dot(W1, X_adj) + b1  # linear transform: hidden layer\n    A1 = activate(Z1, \"tanh\")    # non-linear activation for hidden layer\n    Z2 = np.dot(W2, A1) + b2     # linear transform: output layer\n    A2 = activate(Z2, \"sigmoid\") # probability output for binary classification\n\n    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n\n\n\n\nShow the backward propagation code\nimport numpy as np\n\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n    \"\"\"\n    Compute gradients for parameters using backpropagation.\n    \"\"\"\n    N = X_adj.shape[1]  # number of samples\n\n    W2 = parameters[\"W2\"]\n    A1, A2 = forward_propagation_values[\"A1\"], forward_propagation_values[\"A2\"]\n\n    dZ2 = A2 - Y_adj                      # derivative of loss w.r.t. Z2 (sigmoid+BCE Loss)\n    dW2 = np.dot(dZ2, A1.T) / N           # gradient for W2\n    db2 = np.sum(dZ2, axis=1, keepdims=True) / N  # gradient for b2\n\n    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2) # backprop through tanh: derivative is 1 - A1^2\n    dW1 = np.dot(dZ1, X_adj.T) / N        # gradient for W1\n    db1 = np.sum(dZ1, axis=1, keepdims=True) / N  # gradient for b1\n\n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n\n\n\n\nShow the parameters update code\nimport numpy as np\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n    \"\"\"\n    Update parameters using gradient descent.\n    \"\"\"\n    # subtract learning_rate * gradient for each parameter\n    W1 = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n    b1 = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n    W2 = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n    b2 = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n\nCreate a wrapper function train_neural_network to run the training loop.\n\n\n\nShow the neural network training code\ndef train_neural_network(X, Y, num_iterations, num_hidden_layer_neurons=4):\n    \"\"\"\n    Trains a simple 1-hidden-layer neural network using gradient descent.\n    \"\"\"\n    # Transpose X so columns are examples, reshape Y to row vector\n    X_adj = X.T.copy()                         \n    Y_adj = Y.values.reshape(1, -1).copy()     \n\n    # Initialize weights and biases\n    parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n    for iteration in range(num_iterations):\n        # Forward pass\n        forward_values = forward_propagation(parameters, X_adj.copy())\n\n        # Backward pass\n        grads = backward_propagation(parameters, forward_values, X_adj.copy(), Y_adj.copy())\n\n        # Parameter update\n        parameters = update_parameters(parameters, grads)\n\n    return parameters\n\n\n\n\nImplement Prediction Function\n\nCreate a predict function that runs forward propagation and thresholds outputs.\n\n\n\nShow the code\ndef predict(nn_parameters, X, threshold=0.5):\n    \"\"\"\n    Generates binary predictions from a trained neural network.\n    \"\"\"\n    # Transpose X so columns are examples\n    X_adj = X.T.copy()\n\n    # Forward pass to get output layer activations\n    forward_values = forward_propagation(nn_parameters, X_adj.copy())\n\n    # Output probabilities from sigmoid\n    y_pred = forward_values[\"A2\"]\n\n    # Apply threshold to get class labels {0,1}\n    y_pred = y_pred &gt; threshold\n\n    return y_pred.astype(int).ravel()  # flatten to 1D array",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "topics/single_layer/classification.html#derivation-sigmoid-binary-cross-entropy",
    "href": "topics/single_layer/classification.html#derivation-sigmoid-binary-cross-entropy",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "Derivation: Sigmoid + Binary Cross-Entropy",
    "text": "Derivation: Sigmoid + Binary Cross-Entropy\nWe can show that using a sigmoid activation in the output layer with binary cross-entropy (BCE) loss leads to a very simple gradient formula.\nGoal \\[\n\\frac{\\partial L}{\\partial z} = a - y\n\\]\nSetup For a single example with label \\(y \\in \\{0,1\\}\\): \\[\nz = W^{[2]} a^{[1]} + b^{[2]}, \\qquad a = \\sigma(z) = \\frac{1}{1+e^{-z}}\n\\] The BCE loss for this example is: \\[\n\\ell(a,y) = -\\big[y \\log a + (1-y)\\log(1-a)\\big]\n\\] For a batch of \\(N\\) examples: \\[\nL = \\frac{1}{N}\\sum_{i=1}^N \\ell\\!\\big(a^{(i)}, y^{(i)}\\big)\n\\]\nStep 1 (loss wrt \\(a\\)) \\[\n\\frac{\\partial \\ell}{\\partial a}\n= -\\!\\left(\\frac{y}{a} - \\frac{1-y}{1-a}\\right)\n= \\frac{a - y}{a(1-a)}\n\\]\nStep 2 (sigmoid derivative) \\[\n\\frac{\\partial a}{\\partial z} = a(1-a)\n\\]\nStep 3 (chain rule) \\[\n\\frac{\\partial \\ell}{\\partial z}\n= \\frac{\\partial \\ell}{\\partial a}\\cdot\\frac{\\partial a}{\\partial z}\n= \\frac{a - y}{a(1-a)} \\cdot a(1-a)\n= a - y\n\\]\nBatch form \\[\n\\frac{\\partial L}{\\partial Z} = \\frac{1}{N}(A - Y)\n\\]\nCode correspondence\n\ndZ2 = A2 - Y_adj\n\ndW2 = np.dot(dZ2, A1.T) / N\n\ndb2 = np.sum(dZ2, axis=1, keepdims=True) / N",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "topics/single_layer/classification.html#neural-network-architecture-playbook",
    "href": "topics/single_layer/classification.html#neural-network-architecture-playbook",
    "title": "Single layer Neural Network for Binary Classification",
    "section": "Neural Network Architecture Playbook",
    "text": "Neural Network Architecture Playbook\nDefinitions - Depth: The number of hidden layers stacked between input and output.\n- Width: The number of neurons in each hidden layer.\n(Let d be the number of input features.)\n\nStep 1 — Build a small, trainable baseline\n\nStart with 1–3 hidden layers.\n\nSet hidden layer sizes as multiples of d (e.g., d, 2d, 4d).\n\nAdd dropout (0–0.5) and weight decay (AdamW) for regularization.\n\nAlways keep a validation split and monitor training/validation curves.\n\n\n\nStep 2 — Diagnose with learning curves\n\nUnderfitting (high train & val error): widen hidden layers first, then add more layers if needed.\n\nOverfitting (low train, high val): add data/augmentation; increase dropout or weight decay; use early stopping; simplify by reducing neurons or layers.\n\nOptimization issues (unstable or diverging): lower the learning rate; adjust batch size; use better initialization; consider normalization.\n\n\n\nStep 3 — Grow capacity deliberately\n\nIncrease width modestly before adding depth.\n\nIf you deepen the network, add residual connections or normalization to stabilize training.\n\nIncrease epochs only once the model trains stably.\n\n\n\nStep 4 — Tune by systematic search\n\nUse Random Search or Hyperband over:\n\nLayers: {1, 2, 3, 4, 6}\n\nWidths: {d, 2d, 4d, 8d}\n\nDropout: [0, 0.5]\n\nWeight decay: small grid (e.g., 1e-6 … 1e-3)",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html",
    "href": "topics/single_layer/single_layer_credit_score.html",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "",
    "text": "In this tutorial we build and evaluate a single hidden layer neural network to predict credit default risk (SeriousDlqin2yrs, 0/1). The model:\n\nUses one hidden layer with ReLU activation\nOutputs a probability via sigmoid\nTrains using full-batch gradient descent (one update per epoch)\nEvaluates accuracy, AUC, precision, recall, and F1\nWe standardize features to improve optimization stability.\n\n\n\n\nTarget: SeriousDlqin2yrs — whether serious delinquency occurred within 2 years (0/1).\nFeatures: 10 standardized numeric predictors (after cleaning and dropping the ID column).\n\n\n\nShow the code\n\nimport os\n# Core data handling\nimport numpy as np                  # numeric arrays, vectorized ops\nimport pandas as pd                 # dataframes, CSV I/O\n\n# Model selection & preprocessing\nfrom sklearn.model_selection import train_test_split   # train/test split with stratify\nfrom sklearn.preprocessing import StandardScaler       # feature standardization (fit on train only!)\n\n# Deep learning (Keras/TensorFlow)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential                # simple stack model\nfrom tensorflow.keras.layers import Dense, Input       # fully connected layers\nfrom tensorflow.keras import metrics                   # ready-made metrics: AUC, Precision, Recall, F1",
    "crumbs": [
      "Credit Score example"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html#overview",
    "href": "topics/single_layer/single_layer_credit_score.html#overview",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "",
    "text": "In this tutorial we build and evaluate a single hidden layer neural network to predict credit default risk (SeriousDlqin2yrs, 0/1). The model:\n\nUses one hidden layer with ReLU activation\nOutputs a probability via sigmoid\nTrains using full-batch gradient descent (one update per epoch)\nEvaluates accuracy, AUC, precision, recall, and F1\nWe standardize features to improve optimization stability.\n\n\n\n\nTarget: SeriousDlqin2yrs — whether serious delinquency occurred within 2 years (0/1).\nFeatures: 10 standardized numeric predictors (after cleaning and dropping the ID column).\n\n\n\nShow the code\n\nimport os\n# Core data handling\nimport numpy as np                  # numeric arrays, vectorized ops\nimport pandas as pd                 # dataframes, CSV I/O\n\n# Model selection & preprocessing\nfrom sklearn.model_selection import train_test_split   # train/test split with stratify\nfrom sklearn.preprocessing import StandardScaler       # feature standardization (fit on train only!)\n\n# Deep learning (Keras/TensorFlow)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential                # simple stack model\nfrom tensorflow.keras.layers import Dense, Input       # fully connected layers\nfrom tensorflow.keras import metrics                   # ready-made metrics: AUC, Precision, Recall, F1",
    "crumbs": [
      "Credit Score example"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html#data-preprocessing",
    "href": "topics/single_layer/single_layer_credit_score.html#data-preprocessing",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nBefore training, we need to prepare the dataset:\n\nRemove rows with missing values.\n\nSeparate features (X) from the target (y = SeriousDlqin2yrs).\n\nSplit into training and test sets (keeping class balance with stratify).\n\nStandardize features so each has mean 0 and variance 1 — this helps the neural net train smoothly.\n\n\n\nShow the code\ndef preprocess_data(df):\n    \"\"\"\n    Clean, split, and scale the dataset for classification.\n    \"\"\"\n    # 1) Remove rows with missing values\n    df = df.dropna()\n\n    # 2) Separate features and target\n    X = df.drop(\"SeriousDlqin2yrs\", axis=1)\n    y = df[\"SeriousDlqin2yrs\"]\n\n    # 3) Train/test split with stratification to preserve class ratio\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42, stratify=y\n    )\n\n    # 4) Standardize features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled  = scaler.transform(X_test)\n\n    return X_train_scaled, X_test_scaled, y_train, y_test\n\n\n\n\nShow the code\n# Path to the dataset (inside Documents/BOI_DL_website/data)\ndata_path = os.path.join(\n    os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"),\n    \"data\\\\credit_small_sample.csv\"\n)\n\n# Load raw dataset\nraw_df = pd.read_csv(data_path)\n\n# Apply preprocessing: clean → split → scale\nX_train, X_test, y_train, y_test = preprocess_data(raw_df)",
    "crumbs": [
      "Credit Score example"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html#model-architecture-training-setup",
    "href": "topics/single_layer/single_layer_credit_score.html#model-architecture-training-setup",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "Model architecture & training setup",
    "text": "Model architecture & training setup\nWe will build a single-hidden-layer neural network:\n\nInput (10 features): the standardized predictors from preprocessing.\n\nHidden layer: Dense(20, ReLU)\n\nReLU introduces nonlinearity and allows the model to learn flexible decision boundaries.\n\n\nOutput layer: Dense(1, Sigmoid) produces a probability of default in ([0,1]).\n\nLoss: binary_crossentropy to match the probabilistic output.\n\nOptimizer: Adam with learning rate 1e-3.\n\nMetrics: accuracy, AUC, precision, recall, and F1 for a balanced evaluation under class imbalance.\n\nTraining: 25 epochs using full-batch gradient descent.\n\nFull-batch training means:\n\nThe entire training set is used as one batch.\n\nEach epoch performs one forward pass and one weight update.\n\nThis approach is feasible because the dataset is small.\n\n\n\n\nThe multi-layer neural network used in this tutorial. It takes 10 standardized input features, passes them through two hidden layers (60 and 5 neurons with ReLU activations), and produces a single sigmoid output that represents the probability of default.\n\n\n\n\nShow the code\n# Define a simple feed-forward neural network\nmodel = Sequential([\n    Input(shape=(10,)),                         # The model expects 10 standardized input features\n    Dense(20, activation=\"relu\",                # Hidden layer with 20 neurons + ReLU non-linearity\n          kernel_initializer=\"uniform\"),        # Initialize weights uniformly (small random values)\n    Dense(1, activation=\"sigmoid\")              # Output layer: sigmoid gives probability of default\n])\n\n# Specify training configuration\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),   # Adam optimizer with learning rate = 0.001\n    loss=\"binary_crossentropy\",                 # Standard loss function for binary classification\n    metrics=[\n        \"accuracy\",                             # Proportion of correct predictions\n        metrics.AUC(name=\"auc\"),                # Area under ROC curve (ranking quality)\n        metrics.Precision(name=\"precision\"),    # TP / (TP + FP) — “how many predicted positives are real”\n        metrics.Recall(name=\"recall\"),          # TP / (TP + FN) — “how many true positives we catch”\n        metrics.F1Score(name=\"f1\")              # Harmonic mean of precision and recall\n    ]\n)\n\n# Use full-batch gradient descent: one weight update per epoch\nbatch_size = X_train.shape[0]\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    epochs=25,                                  # Number of passes through the full dataset\n    batch_size=batch_size,                      # Full batch = whole dataset at once\n    verbose=0                                   # Suppress training output for cleaner logs\n)",
    "crumbs": [
      "Credit Score example"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html#model-evaluation",
    "href": "topics/single_layer/single_layer_credit_score.html#model-evaluation",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "Model evaluation",
    "text": "Model evaluation\nAfter training, we test the model on the held-out test set.\nThe evaluate function returns the loss and all metrics we specified in compile (accuracy, AUC, precision, recall, F1).\nPresenting them in a clean, rounded format makes the results easier to interpret.\n\n\nShow the code\n# Evaluate on the test set and return metrics as a dictionary\ntest_metrics = model.evaluate(X_test, y_test, verbose=0, return_dict=True)\n\n# Format nicely: metric name + rounded value\nfor key, value in test_metrics.items():\n    print(f\"{key:&lt;10}: {value:.2f}\")\n\n\naccuracy  : 0.93\nauc       : 0.57\nf1        : 0.13\nloss      : 0.57\nprecision : 0.50\nrecall    : 0.01",
    "crumbs": [
      "Credit Score example"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html#training-history",
    "href": "topics/single_layer/single_layer_credit_score.html#training-history",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "Training history",
    "text": "Training history\nLooking at metrics across epochs helps us understand model behavior:\n\nLoss curve: should generally decrease; if it rises again, the model may be overfitting.\n\nAccuracy / AUC curves: should increase and stabilize.\n\nPrecision/recall tradeoff: sometimes one rises while the other falls; looking at both is important.\n\nPlotting the training history gives a clear picture of how the network improves during training.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Convert training history to a DataFrame for easy plotting\nhistory_df = pd.DataFrame(history.history)\n\n# Plot loss\nplt.figure(figsize=(6,4))\nplt.plot(history_df[\"loss\"], label=\"Training loss\")\nplt.title(\"Training loss over epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nShow the code\n# Plot accuracy and AUC\nplt.figure(figsize=(6,4))\nplt.plot(history_df[\"accuracy\"], label=\"Accuracy\")\nplt.plot(history_df[\"auc\"], label=\"AUC\")\nplt.title(\"Training metrics over epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Metric value\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Credit Score example"
    ]
  },
  {
    "objectID": "topics/single_layer/single_layer_credit_score.html#appendix-metrics-formulas",
    "href": "topics/single_layer/single_layer_credit_score.html#appendix-metrics-formulas",
    "title": "Single layer Neural Network for Credit Score classification",
    "section": "Appendix: Metrics & Formulas",
    "text": "Appendix: Metrics & Formulas\nThis page reports binary classification metrics computed on a held-out test set.\nLet TP, FP, TN, FN be counts from the confusion matrix at a threshold \\(t\\) (often \\(t=0.5\\)).\nLet \\(y_{i} \\in \\{0,1\\}\\) be the true label and \\(\\hat{p}_i \\in [0,1]\\) the model’s predicted probability for the positive class.\n\nBinary Cross-Entropy (Log Loss)\nMeasures the quality of probabilistic predictions (lower is better): \\[\n\\text{BCE} = -\\frac{1}{n}\\sum_{i=1}^{n}\\Big[y_i \\log(\\hat{p}_i) + (1-y_i)\\log\\big(1-\\hat{p}_i\\big)\\Big].\n\\]\n\nProper scoring rule: encourages calibrated probabilities.\nUsed as the training loss for the sigmoid output.\n\n\n\nAccuracy\nShare of correct predictions at threshold (t): \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}.\n\\]\n\nCan be misleading under class imbalance.\n\n\n\nPrecision (Positive Predictive Value)\n“How many predicted positives are truly positive?”\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}.\n\\]\n\n\nRecall (Sensitivity, TPR)\n“How many actual positives did we catch?”\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}.\n\\]\n\n\nF1 Score\nHarmonic mean of precision and recall: \\[\n\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n= \\frac{2TP}{2TP + FP + FN}.\n\\]\n\nBalances miss rate (FN) vs. false alarms (FP).\nGeneralization: \\(F_\\beta\\) weights recall \\(\\beta\\) times more than precision:\n\n\\[\nF_\\beta = (1+\\beta^2)\\,\\frac{\\text{Precision}\\cdot\\text{Recall}}{(\\beta^2\\cdot\\text{Precision})+\\text{Recall}}.\n\\]\n\n\nAUC (ROC-AUC)\nThreshold-free measure of ranking quality (higher is better).\n- ROC curve: plot \\(\\text{TPR}=\\frac{TP}{TP+FN}\\) vs. \\(\\text{FPR}=\\frac{FP}{FP+TN}\\) as \\(t\\) varies.\n\nAUC is the area under the ROC curve and equals the probability a random positive is ranked above a random negative: \\[\n\\text{AUC} = \\Pr\\big(\\hat{p}^+ &gt; \\hat{p}^-\\big).\n\\]\n\n\n\nPractical Notes\n\nThreshold choice ((t)) trades precision vs. recall; tune (t) to business costs or by maximizing a metric (e.g., F1) on validation data.\nImbalanced data: rely less on accuracy; prefer AUC, PR curves, F1, and class-specific error analysis.\nCalibration: well-calibrated \\(\\hat{p}\\) improves decision-making when costs vary",
    "crumbs": [
      "Credit Score example"
    ]
  }
]