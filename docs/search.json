[
  {
    "objectID": "topics/single_layer_nn.html",
    "href": "topics/single_layer_nn.html",
    "title": "Single layer Neural Network",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\n\n\nThe Algorithm\n\nDefine the Model Structure\n\nWeights (w) and bias (b).\n\nInitialize Model Parameters\n\nSet initial values for w and b.\n\nTraining Loop (Gradient Descent Iteration)\n\nCompute gradients (backward propagation).\n\nUpdate parameters (gradient descent).\n\nReturn Trained Parameters\n\nOutput optimized w and b.\n\n\n\n\nFunctions definition\n\nAuxiliary functions\n\n\nShow the code\ndef initialize_parameters(X,y, num_hidden_layer_neurons = 4):\n\n  scaling_constant = 0.01\n \n  num_features = X.shape[1]\n\n  num_output_layers = y.shape[0]\n\n  weights_hidden = np.random.randn(num_hidden_layer_neurons,num_features) * scaling_constant\n\n  weights_output = np.random.randn(num_hidden_layer_neurons, 1)* scaling_constant\n\n  bias_hidden = np.zeros((num_hidden_layer_neurons,1))\n\n  bias_output = np.zeros((1, 1))\n\n  parameters = {\"weights_hidden\": weights_hidden,\"bias_hidden\":bias_hidden,\n                \"weights_output\": weights_output,\"bias_output\":bias_output}\n\n  return parameters\n\n\ndef forward_propagation(X, parameters):\n  \n  num_samples = X.shape[0]\n\n  num_features = X.shape[1]\n  \n  num_features = num_features\n\n  weights_hidden = parameters[\"weights_hidden\"]\n  weights_output = parameters[\"weights_output\"]\n\n  bias_hidden = parameters[\"bias_hidden\"]\n  bias_output = parameters[\"bias_output\"]\n\n  values_hidden = np.dot(weights_hidden, X.T) + bias_hidden # dims: (num_of_hidden_neurons,num_samples)\n\n  values_hidden_active = np.tanh(values_hidden) # dims: (num_of_hidden_neurons,num_samples)\n\n  values_output = np.dot(weights_output.T, values_hidden_active) + bias_output # dims: (num_samples)\n\n  values_output_active = np.tanh(values_output)\n\n  predictions = values_output_active\n  \n  current_network_values = {\"values_hidden\": values_hidden,\n                            \"values_hidden_active\": values_hidden_active,\n                            \"values_output\": values_output,\n                            \"values_output_active\": values_output_active}\n  \n  return predictions, current_network_values\n\n\ndef backward_propagation(parameters, current_network_values, X, y):\n  weights_hidden = parameters[\"weights_hidden\"]\n  weights_output = parameters[\"weights_output\"]\n\n  bias_hidden = parameters[\"bias_hidden\"]\n  bias_output = parameters[\"bias_output\"]\n\n  values_hidden_active = current_network_values[\"values_hidden_active\"]\n\n  values_output = current_network_values[\"values_output\"]\n\n  values_output_active = current_network_values[\"values_output_active\"]\n\n  num_samples = X.shape[0]\n\n  #####calculate gradients\n  \n  y = y.values.reshape(1,-1)\n\n  d_values_output = values_output_active - y # dims: (num_samples,1)\n  \n  d_weights_output = (1 / num_samples) * np.dot(values_hidden_active, d_values_output.T) # dims: (num_of_hidden_neurons, 1)\n\n  d_bias_output = (1 / num_samples) * np.sum(d_values_output,axis = 1, keepdims=True)\n\n \n  d_values_hidden = np.dot(weights_output, d_values_output) *(1 - np.power(values_hidden_active, 2))\n\n  d_weights_hidden = (1 / num_samples) * np.dot(d_values_hidden, X)\n\n  d_bias_hidden = (1 / num_samples) * np.sum(d_values_hidden,axis = 1, keepdims=True) \n\n  \n  grads = {\"d_weights_hidden\": d_weights_hidden, \"d_bias_hidden\": d_bias_hidden,\n           \"d_weights_output\": d_weights_output, \"d_bias_output\": d_bias_output}\n  \n  return grads\n  \n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n\n  weights_hidden = parameters[\"weights_hidden\"]\n  weights_output = parameters[\"weights_output\"]\n\n  bias_hidden = parameters[\"bias_hidden\"]\n  bias_output = parameters[\"bias_output\"]\n\n  d_weights_hidden = grads[\"d_weights_hidden\"]\n  d_bias_hidden = grads[\"d_bias_hidden\"]\n\n  d_weights_output = grads[\"d_weights_output\"]\n  d_bias_output = grads[\"d_bias_output\"]\n\n  weights_hidden = weights_hidden - learning_rate * d_weights_hidden\n  bias_hidden = bias_hidden - learning_rate * d_bias_hidden\n\n  weights_output = weights_output - learning_rate * d_weights_output\n  bias_output = bias_output - learning_rate * d_bias_output\n\n  parameters = {\"weights_hidden\": weights_hidden,\"bias_hidden\":bias_hidden,\n                \"weights_output\": weights_output,\"bias_output\":bias_output}\n  \n  return parameters\n\n\ndef predict(parameters, X, threshold = 0.5):\n  \n  predictions, current_network_values = forward_propagation(X, parameters)\n  \n  predictions = pd.Series(predictions.ravel())\n\n  predictions = predictions.apply(lambda x: 1 if x &gt; threshold else 0)\n  \n  return predictions\n\n\n\n\nNeural network implementation\n\n\nShow the code\ndef train_neural_network(X,y,num_iterations = 100):\n  # Initialize the model's parameters\n  # Loop:\n  #  - Implement forward propagation to get the predictions\n  #  - Implement backward propagation to get the gradients\n  #  - Update parameters (gradient descent)\n\n  parameters = initialize_parameters(X,y)\n\n  for iteration in range(num_iterations):\n\n    predictions, current_network_values = forward_propagation(X.copy(), parameters)\n\n    grads = backward_propagation(parameters, current_network_values, X.copy(), y.copy())\n\n    parameters = update_parameters(parameters, grads)\n\n  return parameters\n\n\n\n\n\nApplication on planar data\n\nImport and preprocess data\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\planar_data.csv\"))\n\nfeatures = [\"x_coord\",\"y_coord\"]\n\ntarget = \"label\"\n\nX = raw_df[features].copy()\n\nY = raw_df[target].copy()\n\n\n\n\nTraining neural network\n\n\nnn_params = train_neural_network(X = X,y = Y,num_iterations = 10000)\n\n\n\nPredictions on test set\n\nfrom sklearn.metrics import accuracy_score\n\nnn_predictions = predict(nn_params, X)\n\nnn_score = accuracy_score(Y, nn_predictions)\n\nprint(f\"Neural network score is {np.round(nn_score,4)}\")\n\nNeural network score is 0.885\n\n\n\n\nComparison with Logistic regression\n\n\nShow the code\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\n\nlog_reg.fit(X,Y)\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nAuxiliary plotting functions\n\n\nShow the code\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\ndef generate_grid(x_min, x_max, y_min, y_max, step_size=0.02):\n    \"\"\"\n    Generates a grid of points covering the given range with the specified step size.\n    \n    Parameters:\n    - x_min, x_max: float, range for x-axis.\n    - y_min, y_max: float, range for y-axis.\n    - step_size: float, resolution of the grid.\n    \n    Returns:\n    - XX, YY: Meshgrid arrays for plotting.\n    - grid_points: Flattened array of grid coordinates.\n    \"\"\"\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size),\n                         np.arange(y_min, y_max, step_size))\n    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Flatten the grid\n    \n    return xx, yy, grid_points\n\ndef plot_decision_boundary(xx, yy, pred_grid, X, y, title, cmap=plt.cm.RdBu):\n    \"\"\"\n    Plots the decision boundary for a given prediction grid.\n    \n    Parameters:\n    - xx, yy: Meshgrid arrays for plotting.\n    - pred_grid: Prediction values reshaped to match xx and yy.\n    - X: Original dataset (features).\n    - y: Labels for the dataset.\n    - title: Title of the plot.\n    - cmap: Colormap for visualization.\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    \n    # Plot the decision boundary\n    plt.contourf(xx, yy, pred_grid, alpha=0.6, cmap=cmap)\n    \n    # Scatter plot of actual data points\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap=cmap, s=40)\n    \n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()\n\n\n\n\nShow the code\nxx, yy, grid_points  = generate_grid(x_min = X[\"x_coord\"].min(), x_max = X[\"x_coord\"].max(),\ny_min = X[\"y_coord\"].min(), y_max = X[\"y_coord\"].max())\n\n\nnn_pred_grid = predict(nn_params, grid_points)\n\nnn_pred_grid = np.array(nn_pred_grid).reshape(xx.shape)\n\nlog_reg_pred_grid = log_reg.predict(pd.DataFrame(grid_points, columns=X.columns))\n\nlog_reg_pred_grid = np.array(log_reg_pred_grid).reshape(xx.shape)\n\n\nThis is how actual data looks like\n\n\nShow the code\nplt.clf()\n\nplt.scatter(X[\"x_coord\"], X[\"y_coord\"], c=Y, s=40, cmap=plt.cm.Spectral);\n\nplt.title(\"Actual data\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is a classification by logistic regression\n\n\nShow the code\nplot_decision_boundary(xx, yy, log_reg_pred_grid, X.to_numpy(),\nY.to_numpy(),title=\"Logistic Regression Decision Boundary\")\n\n\n\n\n\n\n\n\n\nAnd this is a classification by neural network\n\n\nShow the code\nplot_decision_boundary(xx, yy, nn_pred_grid, X.to_numpy(),\nY.to_numpy(),title=\"Neural Network Decision Boundary\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction\nSingle (hidden) layer neural network"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction\nSingle (hidden) layer neural network"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "topics/intro.html",
    "href": "topics/intro.html",
    "title": "Intro",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom patsy import dmatrix\n\nfrom sklearn.linear_model import LinearRegression\n\nimport pygam\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\Wage.csv\"))\n\ny_vec = raw_df[\"wage\"].copy()\n\nx_vec = raw_df[[\"age\"]].copy()\n\n\n\n\nShow the code\n\ndef plot_predictions(pred_df, title):\n  pred_cols = [temp_name for temp_name in pred_df.columns.values if \"pred\" in temp_name]\n\n  plt.figure(figsize=(10, 6))\n  plt.scatter(pred_df[\"age\"], pred_df[\"wage\"], label=\"Actual Wage\", alpha = 0.7)\n\n  for temp_name in pred_cols:\n    plt.scatter(pred_df[\"age\"], pred_df[temp_name], label=temp_name, alpha = 0.7)\n#  plt.scatter(pred_df[\"age\"], pred_df[\"wage_pred\"], label=\"Predicted Wage\", alpha = 0.7)\n  plt.xlabel(\"Age\")\n  plt.ylabel(\"Wage\")\n  plt.title(title)\n  plt.legend()\n  plt.grid(True)\n  plt.show()\n\n\n\nPolynomial regression\n\nConstruct polynomial features:\n\nUsing the dmatrix function, create a feature matrix that includes polynomial terms up to the fourth degree: \\(\\text{age}, \\text{age}^2, \\text{age}^3, \\text{age}^4\\)\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec from the polynomial features.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\n\n\nShow the code\npoly_x_mat = dmatrix(\"age + I(age**2) + I(age**3) + I(age**4)\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(poly_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\npoly_pred = lin_reg.predict(poly_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": poly_pred}),\n                 \"Polynomial Regression\")\n\n\n\n\n\n\n\n\n\n\n\nStep functions\n\nDefine step function intervals (bins):\n\nUse np.percentile to determine cutoff points (knots) that divide age into quartiles.\n\nUse pd.cut to assign each age value to a bin.\n\nCreate a step function design matrix:\n\nUse the dmatrix function to encode the binned age values as categorical variables.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the step function representation of age.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\nHints:\n* Ensure that the step function bins do not have duplicate edges, which can be handled using duplicates='drop' in pd.cut.\n* You can use matplotlib.pyplot or seaborn to visualize the stepwise fitted function.\n* Since this is a piecewise constant model, expect a regression curve with horizontal segments rather than a smooth curve.\n\n\nShow the code\nknots = np.percentile(x_vec[\"age\"], [0, 15, 25, 50, 75,90, 100])\n\nx_vec_step = pd.cut(x_vec[\"age\"], bins=knots, labels=False,\n                    include_lowest=True,duplicates='drop')\n\nstep_x_mat = dmatrix(\"C(x_vec_step)\", x_vec_step)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(step_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nstep_pred = lin_reg.predict(step_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": step_pred}), \"Step Regression\")\n\n\n\n\n\n\n\n\n\n\n\nPiecewise polynomials\n\nDefine the knot location:\n\nSet a single knot at age = 50 to allow for a change in the polynomial relationship at this point.\n\nCreate piecewise polynomial terms:\n\n\nConstruct polynomial terms (age, age², age³) for the entire dataset.\nDefine separate squared and cubic terms for values below and above the knot to allow for discontinuity.\nUse np.maximum to ensure that terms are active only in their respective regions.\n\n\nCreate a design matrix:\n\nCombine all polynomial terms into a matrix that serves as input for the regression model.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the spline-transformed age values.\n\nMake predictions:\nUse the trained model to generate predictions for y_vec.\n\nHints:\n* B-splines create smooth, continuous fits by combining piecewise polynomial segments.\n* You can experiment with additional knots to see how the flexibility of the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curve.\n\n\nShow the code\nknot = 50\n\n# Manually create piecewise terms to enforce discontinuity at the knot\nx_less_knot = np.maximum(0, knot - x_vec)  # For x &lt; knot\nx_greater_knot = np.maximum(0, x_vec - knot)  # For x &gt;= knot\n\n# Combine the terms into a design matrix (manual spline basis)\nspline_x_mat = np.column_stack((x_vec, x_vec**2, x_vec**3, x_less_knot**2,\n                                x_greater_knot**2,x_less_knot**3, x_greater_knot**3))\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": spline_pred}),\n\"Piecewise Spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nSplines\n\nConstruct spline basis matrices:\n\nUse the dmatrix function to create a B-spline basis with knots at age = 25, 40, 60 and a polynomial degree of 3.\nUse the dmatrix function to create a natural spline basis with the same knots.\n\nFit linear regression models:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit one model using the B-spline basis and another using the natural spline basis.\n\nMake predictions:\nUse each trained model to generate predictions for y_vec.\n\nHints:\n* B-splines allow local flexibility, adjusting the curve within defined knots.\n* Natural splines impose additional constraints that make the curve behave more smoothly at the boundaries.\n* Try adjusting the number and position of knots to see how the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curves.\n\n\nShow the code\nspline_x_mat = dmatrix(\"bs(age, knots = [25,40,60], degree=3)\", x_vec)\n\nnatural_spline_x_mat = dmatrix(\"cr(age, knots = [25,40,60])\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(natural_spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nnatural_spline_pred = lin_reg.predict(natural_spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec,\n                               \"spline_pred\": spline_pred,\n                               \"natural_spline_pred\": natural_spline_pred}),\n                               \"Spline and Natural spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nGeneral Additive Models\n\n\nShow the code\nfrom pygam import LinearGAM, s, f\n\n# Encode categorical variable 'education'\nraw_df['education_code'] = raw_df['education'].astype('category').cat.codes\n\n# Features and response\nX = raw_df[['year', 'age', 'education_code']].values\n\ny = raw_df['wage'].values\n\n# Fit the GAM model\n\ngam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=8) + f(2));\n\ngam.fit(X, y);\n\n\n\n\nShow the code\n# Generate smooth predictions for each term\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot the effect of year\nyear_grid = np.linspace(raw_df['year'].min(), raw_df['year'].max(), 100)\nX_year = np.zeros((100, 3))\nX_year[:, 0] = year_grid\naxes[0].plot(year_grid, gam.partial_dependence(0, X=X_year), color='red');\naxes[0].set_title(r\"$f_1(\\mathrm{year})$\");\naxes[0].set_xlabel(\"year\");\naxes[0].set_ylabel(\"Effect\");\naxes[0].set_ylim(-30, 30);\n\n# Plot the effect of age\nage_grid = np.linspace(raw_df['age'].min(), raw_df['age'].max(), 100)\nX_age = np.zeros((100, 3))\nX_age[:, 1] = age_grid\naxes[1].plot(age_grid, gam.partial_dependence(1, X=X_age), color='red');\naxes[1].set_title(r\"$f_2(\\mathrm{age})$\");\naxes[1].set_xlabel(\"age\");\naxes[1].set_ylabel(\"Effect\");\naxes[1].set_ylim(-50, 40);\n\n# Plot the effect of education\neducation_levels = np.sort(raw_df['education_code'].unique())\nX_edu = np.zeros((len(education_levels), 3))\nX_edu[:, 2] = education_levels\naxes[2].bar(\n    raw_df['education'].astype('category').cat.categories,\n    gam.partial_dependence(2, X=X_edu).flatten(),\n    color='red'\n);\naxes[2].set_title(r\"$f_3(\\mathrm{education})$\");\naxes[2].set_xlabel(\"education\");\naxes[2].set_ylabel(\"Effect\");\n\n\nplt.tight_layout()\nplt.show()"
  }
]