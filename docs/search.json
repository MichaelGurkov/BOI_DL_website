[
  {
    "objectID": "topics/intro.html",
    "href": "topics/intro.html",
    "title": "Intro",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom patsy import dmatrix\n\nfrom sklearn.linear_model import LinearRegression\n\nimport pygam\n\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Plot predictions\nimport matplotlib.pyplot as plt\n\n\n\n\nShow the code\n\nraw_df = pd.read_csv(os.path.join(os.path.expanduser(\"~\\\\Documents\\\\BOI_DL_website\"), \"data\\\\Wage.csv\"))\n\ny_vec = raw_df[\"wage\"].copy()\n\nx_vec = raw_df[[\"age\"]].copy()\n\n\n\n\nShow the code\n\ndef plot_predictions(pred_df, title):\n  pred_cols = [temp_name for temp_name in pred_df.columns.values if \"pred\" in temp_name]\n\n  plt.figure(figsize=(10, 6))\n  plt.scatter(pred_df[\"age\"], pred_df[\"wage\"], label=\"Actual Wage\", alpha = 0.7)\n\n  for temp_name in pred_cols:\n    plt.scatter(pred_df[\"age\"], pred_df[temp_name], label=temp_name, alpha = 0.7)\n#  plt.scatter(pred_df[\"age\"], pred_df[\"wage_pred\"], label=\"Predicted Wage\", alpha = 0.7)\n  plt.xlabel(\"Age\")\n  plt.ylabel(\"Wage\")\n  plt.title(title)\n  plt.legend()\n  plt.grid(True)\n  plt.show()\n\n\n\nPolynomial regression\n\nConstruct polynomial features:\n\nUsing the dmatrix function, create a feature matrix that includes polynomial terms up to the fourth degree: \\(\\text{age}, \\text{age}^2, \\text{age}^3, \\text{age}^4\\)\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec from the polynomial features.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\n\n\nShow the code\npoly_x_mat = dmatrix(\"age + I(age**2) + I(age**3) + I(age**4)\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(poly_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\npoly_pred = lin_reg.predict(poly_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": poly_pred}),\n                 \"Polynomial Regression\")\n\n\n\n\n\n\n\n\n\n\n\nStep functions\n\nDefine step function intervals (bins):\n\nUse np.percentile to determine cutoff points (knots) that divide age into quartiles.\n\nUse pd.cut to assign each age value to a bin.\n\nCreate a step function design matrix:\n\nUse the dmatrix function to encode the binned age values as categorical variables.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the step function representation of age.\n\nMake predictions:\n\nUse the trained model to generate predictions for y_vec.\n\n\nHints:\n* Ensure that the step function bins do not have duplicate edges, which can be handled using duplicates='drop' in pd.cut.\n* You can use matplotlib.pyplot or seaborn to visualize the stepwise fitted function.\n* Since this is a piecewise constant model, expect a regression curve with horizontal segments rather than a smooth curve.\n\n\nShow the code\nknots = np.percentile(x_vec[\"age\"], [0, 15, 25, 50, 75,90, 100])\n\nx_vec_step = pd.cut(x_vec[\"age\"], bins=knots, labels=False,\n                    include_lowest=True,duplicates='drop')\n\nstep_x_mat = dmatrix(\"C(x_vec_step)\", x_vec_step)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(step_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nstep_pred = lin_reg.predict(step_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": step_pred}), \"Step Regression\")\n\n\n\n\n\n\n\n\n\n\n\nPiecewise polynomials\n\nDefine the knot location:\n\nSet a single knot at age = 50 to allow for a change in the polynomial relationship at this point.\n\nCreate piecewise polynomial terms:\n\n\nConstruct polynomial terms (age, age², age³) for the entire dataset.\nDefine separate squared and cubic terms for values below and above the knot to allow for discontinuity.\nUse np.maximum to ensure that terms are active only in their respective regions.\n\n\nCreate a design matrix:\n\nCombine all polynomial terms into a matrix that serves as input for the regression model.\n\nFit a linear regression model:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit the model to predict y_vec based on the spline-transformed age values.\n\nMake predictions:\nUse the trained model to generate predictions for y_vec.\n\nHints:\n* B-splines create smooth, continuous fits by combining piecewise polynomial segments.\n* You can experiment with additional knots to see how the flexibility of the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curve.\n\n\nShow the code\nknot = 50\n\n# Manually create piecewise terms to enforce discontinuity at the knot\nx_less_knot = np.maximum(0, knot - x_vec)  # For x &lt; knot\nx_greater_knot = np.maximum(0, x_vec - knot)  # For x &gt;= knot\n\n# Combine the terms into a design matrix (manual spline basis)\nspline_x_mat = np.column_stack((x_vec, x_vec**2, x_vec**3, x_less_knot**2,\n                                x_greater_knot**2,x_less_knot**3, x_greater_knot**3))\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec, \"wage_pred\": spline_pred}),\n\"Piecewise Spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nSplines\n\nConstruct spline basis matrices:\n\nUse the dmatrix function to create a B-spline basis with knots at age = 25, 40, 60 and a polynomial degree of 3.\nUse the dmatrix function to create a natural spline basis with the same knots.\n\nFit linear regression models:\n\nUse LinearRegression from sklearn and set fit_intercept=False since the intercept is already included in the design matrix.\nFit one model using the B-spline basis and another using the natural spline basis.\n\nMake predictions:\nUse each trained model to generate predictions for y_vec.\n\nHints:\n* B-splines allow local flexibility, adjusting the curve within defined knots.\n* Natural splines impose additional constraints that make the curve behave more smoothly at the boundaries.\n* Try adjusting the number and position of knots to see how the model changes.\n* Use matplotlib.pyplot or seaborn to visualize the fitted curves.\n\n\nShow the code\nspline_x_mat = dmatrix(\"bs(age, knots = [25,40,60], degree=3)\", x_vec)\n\nnatural_spline_x_mat = dmatrix(\"cr(age, knots = [25,40,60])\", x_vec)\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\nspline_pred = lin_reg.predict(spline_x_mat.copy())\n\nlin_reg = LinearRegression(fit_intercept=False)\n\nlin_reg.fit(natural_spline_x_mat.copy(), y_vec)\n\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression(fit_intercept=False) \n\n\nShow the code\n\nnatural_spline_pred = lin_reg.predict(natural_spline_x_mat.copy())\n\n\n\nVisualize the results (optional but recommended):\n\n\n\nShow the code\nplot_predictions(pd.DataFrame({\"age\": x_vec[\"age\"], \"wage\": y_vec,\n                               \"spline_pred\": spline_pred,\n                               \"natural_spline_pred\": natural_spline_pred}),\n                               \"Spline and Natural spline Regression\")\n\n\n\n\n\n\n\n\n\n\n\nGeneral Additive Models\n\n\nShow the code\nfrom pygam import LinearGAM, s, f\n\n# Encode categorical variable 'education'\nraw_df['education_code'] = raw_df['education'].astype('category').cat.codes\n\n# Features and response\nX = raw_df[['year', 'age', 'education_code']].values\n\ny = raw_df['wage'].values\n\n# Fit the GAM model\n\ngam = LinearGAM(s(0, n_splines=8) + s(1, n_splines=8) + f(2));\n\ngam.fit(X, y);\n\n\n\n\nShow the code\n# Generate smooth predictions for each term\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot the effect of year\nyear_grid = np.linspace(raw_df['year'].min(), raw_df['year'].max(), 100)\nX_year = np.zeros((100, 3))\nX_year[:, 0] = year_grid\naxes[0].plot(year_grid, gam.partial_dependence(0, X=X_year), color='red');\naxes[0].set_title(r\"$f_1(\\mathrm{year})$\");\naxes[0].set_xlabel(\"year\");\naxes[0].set_ylabel(\"Effect\");\naxes[0].set_ylim(-30, 30);\n\n# Plot the effect of age\nage_grid = np.linspace(raw_df['age'].min(), raw_df['age'].max(), 100)\nX_age = np.zeros((100, 3))\nX_age[:, 1] = age_grid\naxes[1].plot(age_grid, gam.partial_dependence(1, X=X_age), color='red');\naxes[1].set_title(r\"$f_2(\\mathrm{age})$\");\naxes[1].set_xlabel(\"age\");\naxes[1].set_ylabel(\"Effect\");\naxes[1].set_ylim(-50, 40);\n\n# Plot the effect of education\neducation_levels = np.sort(raw_df['education_code'].unique())\nX_edu = np.zeros((len(education_levels), 3))\nX_edu[:, 2] = education_levels\naxes[2].bar(\n    raw_df['education'].astype('category').cat.categories,\n    gam.partial_dependence(2, X=X_edu).flatten(),\n    color='red'\n);\naxes[2].set_title(r\"$f_3(\\mathrm{education})$\");\naxes[2].set_xlabel(\"education\");\naxes[2].set_ylabel(\"Effect\");\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "BOI_DL_website",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "topics/logistic_regression.html",
    "href": "topics/logistic_regression.html",
    "title": "Logistic regression as Neural Network",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "topics/logistic_regression.html#define-functions",
    "href": "topics/logistic_regression.html#define-functions",
    "title": "Logistic regression as Neural Network",
    "section": "Define functions",
    "text": "Define functions\n\n\nShow the code\ndef calculate_cost(w,b,X,y):\n\n  A = sigmoid(np.dot(w.T,X) + b)\n\n  cost = -(np.dot(y,np.log(A).T) + np.dot((1-y),np.log(1-A).T))\n\n  return cost\n\n\ndef improve_weights(w, b,X,y,learning_rate = 0.09):\n\n  grads = propagate(w,b,X,y)\n\n  dw = grads[\"dw\"]\n\n  db = grads[\"db\"]\n\n  w = w - learning_rate * dw\n\n  b = b - learning_rate * db\n\n  params = [w,b]\n\n  return params\n\n\n\n\nShow the code\ndef propagate(w,b,X,y):\n\n  m = X.shape[0]\n\n  A = sigmoid(np.dot(w.T,X) + b)\n\n  dz = A - y\n\n  dw = np.dot(X, dz.T) / m\n\n  db = np.sum(dz) / m\n\n  grads = {\"dw\": dw,\n            \"db\": db}\n\n  return grads\n\n\n\n\nShow the code\ndef neural_network_weights(X,y, num_rounds = 100, learning_rate = 0.09):\n\n  w,b = initialize_weights(X.shape[0])\n\n  for i in range(num_rounds):\n    w,b = improve_weights(w,b,X,y,learning_rate = learning_rate)\n    \n  return w,b"
  },
  {
    "objectID": "topics/logistic_regression.html#backward-propagation-gradient-calculation",
    "href": "topics/logistic_regression.html#backward-propagation-gradient-calculation",
    "title": "Logistic regression as Neural Network",
    "section": "Backward propagation (gradient calculation)",
    "text": "Backward propagation (gradient calculation)\n\n\nShow the code\ndef propagate(w,b,X,y):\n\n  m = X.shape[0]\n\n  A = sigmoid(np.dot(w.T,X) + b)\n\n  dz = A - y\n\n  dw = np.dot(X, dz.T) / m\n\n  db = np.sum(dz) / m\n\n  grads = {\"dw\": dw,\n            \"db\": db}\n\n  return grads"
  },
  {
    "objectID": "topics/logistic_regression.html#neural-network-implementation",
    "href": "topics/logistic_regression.html#neural-network-implementation",
    "title": "Logistic regression as Neural Network",
    "section": "Neural network implementation",
    "text": "Neural network implementation\n\n\nShow the code\ndef neural_network_weights(X,y, num_rounds = 100, learning_rate = 0.09):\n\n  w,b = initialize_weights(X.shape[0])\n\n  for i in range(num_rounds):\n    w,b = improve_weights(w,b,X,y,learning_rate = learning_rate)\n    \n  return w,b"
  },
  {
    "objectID": "topics/logistic_regression.html#algorithm-backward-propagation-in-logistic-regression",
    "href": "topics/logistic_regression.html#algorithm-backward-propagation-in-logistic-regression",
    "title": "Logistic regression as Neural Network",
    "section": "Algorithm: Backward Propagation in Logistic Regression",
    "text": "Algorithm: Backward Propagation in Logistic Regression\n\n1. Define the Model Structure\n\nThe model consists of:\n\nWeights: A vector of parameters corresponding to input features, denoted as (\\(w\\)).\nBias: A scalar parameter, denoted as ( \\(b\\) ).\n\n\n\n\n2. Initialize Model Parameters\n\nCall initialize_weights(n_{\\text{features}}), which initializes:\n\n( \\(w\\) ) (weights) to small random values or zeros.\n( \\(b\\) ) (bias) to zero.\n\n\n\n\n3. Training Loop (Gradient Descent Iteration)\n\nRepeat for a given number of iterations (\\(\\text{num\\_rounds}\\)):\na. Compute Gradients (Backward Propagation)\n\nUse propagate(w, b, X, y):\n\nCompute predicted probabilities using the sigmoid function: \\[\nA = \\sigma(w^T X + b)\n\\] where: \\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nCompute error (difference between predicted and actual values): \\[\ndz = A - y\n\\]\nCompute gradients (partial derivatives):\n\nGradient of weights: \\[\ndw = \\frac{1}{m} X dz^T\n\\]\nGradient of bias: \\[\ndb = \\frac{1}{m} \\sum dz\n\\]\n\nReturn (\\(dw\\)) and (\\(db\\)).\n\n\nb. Update Parameters (Gradient Descent)\n\nCall improve_weights(w, b, X, y, $\\text{learning_rate})$, which:\n\nRetrieves gradients from propagate(w, b, X, y).\nUpdates parameters using gradient descent:\n\nUpdate weights: \\(w = w - \\text{learning rate} \\times dw\\)\nUpdate bias: \\(b = b - \\text{learning rate} \\times db\\)\n\n\n\n\n\n\n4. Return Trained Parameters\n\nAfter completing the training loop, return (\\(w\\)) and (\\(b\\)), the optimized parameters."
  },
  {
    "objectID": "topics/single_layer_nn.html",
    "href": "topics/single_layer_nn.html",
    "title": "Single layer Neural Network",
    "section": "",
    "text": "Show the code\n\nimport pandas as pd\n\nimport numpy as np\n\nimport os"
  },
  {
    "objectID": "topics/single_layer_nn.html#auxiliary-functions",
    "href": "topics/single_layer_nn.html#auxiliary-functions",
    "title": "Single layer Neural Network",
    "section": "Auxiliary functions",
    "text": "Auxiliary functions\n\nNN training functions\n\n\nShow the code\ndef activate(Z, activation_function = \"tanh\"):\n  \n  if activation_function == \"tanh\":\n    \n    A = np.tanh(Z)\n  \n  elif activation_function == \"sigmoid\":\n    \n    A = 1 / (1 + np.exp(-Z))\n    \n  return(A)\n\n\ndef initialize_parameters(X_adj,num_hidden_layer_neurons, scale_const = 0.01):\n  np.random.seed(1)\n\n  n_features = X_adj.shape[1]  # Number of input features\n\n  W1 = np.random.randn(num_hidden_layer_neurons, n_features) * scale_const\n\n  b1 = np.zeros((num_hidden_layer_neurons,1))\n\n  W2 = np.random.randn(1,num_hidden_layer_neurons) * scale_const\n\n  b2 = np.zeros((1,1))\n\n  parameters = {\"W1\": W1, \"b1\": b1, \"W2\":W2, \"b2\":b2}\n\n  return parameters\n\ndef forward_propagation(parameters,X_adj):\n  \n  W1 = parameters[\"W1\"]\n  \n  b1 = parameters[\"b1\"]\n\n  W2 = parameters[\"W2\"]\n\n  b2 = parameters[\"b2\"]\n\n  Z1 = np.dot(W1,X_adj) + b1\n\n  A1 = activate(Z1)\n\n  Z2 = np.dot(W2,A1) + b2\n\n  A2 = activate(Z2,activation_function=\"sigmoid\")\n\n  forward_propagation_values = {\"Z1\":Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n\n  return forward_propagation_values\n\ndef backward_propagation(parameters, forward_propagation_values, X_adj, Y_adj):\n\n  p = X_adj.shape[1]\n\n  W1 = parameters[\"W1\"]\n\n  W2 = parameters[\"W2\"]\n\n  A1 = forward_propagation_values[\"A1\"]\n\n  A2 = forward_propagation_values[\"A2\"]\n\n  dZ2 = A2 - Y_adj\n\n  dW2 = np.dot(dZ2, A1.T) / p\n\n  db2 = np.sum(dZ2, axis=1, keepdims=True) / p\n\n  dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n\n  dW1 = np.dot(dZ1,X_adj.T) / p\n\n  db1 = np.sum(dZ1, axis=1, keepdims=True) / p\n\n  grads = {\"dW1\":dW1, \"db1\":db1, \"dW2\":dW2, \"db2\":db2}\n\n  return grads\n\n\ndef update_parameters(parameters, grads, learning_rate=0.01):\n  \n  W1 = parameters[\"W1\"]\n\n  b1 = parameters[\"b1\"]\n\n  W2 = parameters[\"W2\"]\n\n  b2 = parameters[\"b2\"]\n\n  \n  dW1 = grads[\"dW1\"]\n\n  db1 = grads[\"db1\"]\n\n  dW2 = grads[\"dW2\"]\n\n  db2 = grads[\"db2\"]\n\n  \n  W1 = W1 - learning_rate * dW1\n\n  b1 = b1 - learning_rate * db1\n\n  W2 = W2 - learning_rate * dW2\n\n  b2 = b2 - learning_rate * db2\n\n  \n  parameters = {\"W1\": W1, \"b1\": b1, \"W2\":W2, \"b2\":b2}\n\n  return parameters\n\n\n\nNeural network implementation\n\n\nShow the code\ndef train_neural_network(X,Y, num_iterations,num_hidden_layer_neurons = 4):\n    \n    # Initialize the model's parameters\n    # Loop:\n    #  - Implement forward propagation to get the predictions\n    #  - Implement backward propagation to get the gradients\n    #  - Update parameters (gradient descent)\n\n\n  X_adj = X.T.copy()\n\n  Y_adj = Y.values.reshape(1,-1).copy()\n\n  parameters = initialize_parameters(X, num_hidden_layer_neurons=num_hidden_layer_neurons)\n\n  for interation in range(num_iterations):\n    \n    forward_propagation_values = forward_propagation(parameters,X_adj.copy())\n\n    grads = backward_propagation(parameters, forward_propagation_values, X_adj.copy(), Y_adj.copy())\n\n    parameters = update_parameters(parameters, grads)\n\n  return parameters\n\n\n\n\n\nAuxilary functions\n\n\nShow the code\ndef predict(nn_parameters, X, threshold = 0.5):\n\n  X_adj = X.T.copy()\n\n  forward_propagation_values = forward_propagation(nn_parameters, X_adj.copy())\n\n  y_pred = forward_propagation_values[\"A2\"]\n\n  y_pred = y_pred &gt; threshold\n\n  return y_pred.astype(int).ravel()"
  }
]