{
  "hash": "5b15ba61cb31b93754d655b662e3db46",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Python for Deep Learning\"\nformat: html\npage-layout: full\n---\n\n\n# ðŸ§  Python for Deep Learning Workshop\n\nThis hands-on workshop introduces the essential Python skills needed for deep learning. You'll run Python code directly in your environment (e.g., RStudio with reticulate or Jupyter), and practice every concept along the way.\n\n---\n\n\n::: {.cell}\n\n:::\n\n\n## 1. Introduction & Environment Setup\n\nWe will use a local Python environment via `reticulate`, which allows running Python code directly inside this Quarto document.\n\n### Topics Covered\n\n* Why Python for Deep Learning\n* Setting up your local Python environment\n* Reading local files using pandas\n\n### Task\n\nRead a CSV file from your computer using `pandas` and display its first few rows.\n\n---\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nimport pandas as pd\nimport numpy as np\nimport os\n```\n:::\n\n\n## 2. Python Fundamentals\n\nGet familiar with Pythonâ€™s basic building blocks: variables, lists, dictionaries, control flow, and functions.\n\n### Topics Covered\n\n\n* Lists and dictionaries\n* `if`, `for`, and `while` statements\n* Writing and calling functions\n\n\n### 2.1 Lists and dictionaries\n\n**Concept:**  \n- **Lists** are ordered collections. Each item has a position called an *index* (starting from `0`).  \n  - You can get a single element using `list[index]` (e.g., `scores[0]` â†’ first element).  \n  - You can get a *slice* using `list[start:stop]`, which returns elements from `start` up to but **not including** `stop`.  \n- **Dictionaries** store keyâ€“value pairs. You look up a value by its key (like a label).  \n  - You can add new keys or update existing ones using `dict[key] = value`.  \n  - Keys must be unique; values can be any data type.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A list of exam scores for Alice\nscores_alice = [88, 92, 79]\n\n# Indexing: position 0 is the first score\nprint(\"First score:\", scores_alice[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst score: 88\n```\n\n\n:::\n\n```{.python .cell-code}\n# Slicing: [:2] means start at index 0, stop before index 2 â†’ positions 0 and 1\nprint(\"First two scores:\", scores_alice[:2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst two scores: [88, 92]\n```\n\n\n:::\n\n```{.python .cell-code}\n# A dictionary mapping student names to their list of scores\nstudent_scores = {\n    \"Alice\": [88, 92, 79],\n    \"Bob\":   [75, 83, 80],\n    \"Carol\": [90, 85, 95],\n    \"Dave\":  [72, 78, 70],\n}\n\n# Lookup: use the key (student's name) to get the list of scores\nprint(\"Carol's scores:\", student_scores[\"Carol\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCarol's scores: [90, 85, 95]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Add a new student by assigning to a new key\nstudent_scores[\"Eve\"] = [85, 88, 91]\n\n# Update an existing student's scores (overwrites the old list)\nstudent_scores[\"Bob\"] = [78, 84, 82]\n\n# Get all keys (student names) and values (lists of scores)\nprint(\"Students:\", list(student_scores.keys()))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStudents: ['Alice', 'Bob', 'Carol', 'Dave', 'Eve']\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"Sample scores:\", list(student_scores.values())[:2])  # [:2] â†’ first two values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample scores: [[88, 92, 79], [78, 84, 82]]\n```\n\n\n:::\n:::\n\n\n\n### 2.2 Control flow\n\n**Concept:**  \n- `if / elif / else` lets the program choose actions based on conditions.  \n- `for` loops iterate over items in a collection, letting you process each element in turn. \n\nWe'll use the `student_scores` dictionary from the previous section.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Example 1: if / elif / else\nx = 87\nif x >= 90:\n    grade = \"A\"\nelif x >= 80:\n    grade = \"B\"\nelse:\n    grade = \"C or below\"\nprint(\"Grade bucket:\", grade)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGrade bucket: B\n```\n\n\n:::\n\n```{.python .cell-code}\n# Example 2: for loop over a student's scores\n# Reusing the student_scores dictionary from earlier\nscores_alice = student_scores[\"Alice\"]  # a list of Alice's scores\ntotal = 0\nfor s in scores_alice:\n    total = total + s\naverage = total / len(scores_alice)\nprint(\"Alice's average score:\", average)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAlice's average score: 86.33333333333333\n```\n\n\n:::\n:::\n\n\n### 2.3 Functions\n\n**Concept:**  \n- A function is a reusable block of code that takes **inputs** (parameters) and can **return** an output.  \n- Use `def function_name(parameters):` to define it.  \n- Use `return` to send a result back to the caller.  \n- You can reuse loops and calculations inside a function so you donâ€™t repeat the same code.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef average_score(scores):\n    \"\"\"\n    Calculate the average from a list of scores.\n    \"\"\"\n    total = 0\n    for s in scores:\n        total = total + s\n    return total / len(scores)\n\n# Example: Calculate averages for all students\nfor name in student_scores:  # loops over keys (student names)\n    avg = average_score(student_scores[name])\n    print(f\"{name}: {avg:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAlice: 86.33\nBob: 81.33\nCarol: 90.00\nDave: 73.33\nEve: 88.00\n```\n\n\n:::\n:::\n\n\n\n\n## 3. Numerical Computing with NumPy\n\n**Concept:**  \nNumPy arrays are like lists but optimized for fast mathematical operations.  \n- **1D array** â†’ like a row of numbers.  \n- **2D array** â†’ like a table (rows Ã— columns).  \n- `.shape` tells you the size of the array.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 1D array: vector of exam scores\nscores_1d = np.array([88, 92, 79])\nprint(\"1D array:\", scores_1d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1D array: [88 92 79]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"Shape:\", scores_1d.shape)  # (3,) â†’ 3 elements in 1 dimension\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShape: (3,)\n```\n\n\n:::\n\n```{.python .cell-code}\n# 2D array: scores for two students across three exams\nscores_2d = np.array([\n    [88, 92, 79],  # student 1\n    [75, 83, 80]   # student 2\n])\nprint(\"\\n2D array:\\n\", scores_2d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n2D array:\n [[88 92 79]\n [75 83 80]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"Shape:\", scores_2d.shape)  # (2, 3) â†’ 2 rows, 3 columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShape: (2, 3)\n```\n\n\n:::\n\n```{.python .cell-code}\n# Now convert student_scores dictionary values into a 2D array\ngrades_matrix = np.array(list(student_scores.values()))\nprint(\"\\nGrades matrix:\\n\", grades_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"Shape:\", grades_matrix.shape)  # rows = students, columns = exams\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShape: (5, 3)\n```\n\n\n:::\n:::\n\n\n\n### 3.1 Indexing and slicing\n\n**Concept:**  \n- Indexing works like Python lists: `array[row_index, col_index]` (0-based).  \n- Slicing lets you select a range: `start:stop` returns elements from `start` up to (but not including) `stop`.  \n- You can slice rows, columns, or both.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Example array for reference\nprint(\"Grades matrix:\\n\", grades_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGrades matrix:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Get the score of the first student in the first exam\nprint(\"\\nFirst student's first exam score:\", grades_matrix[0, 0])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst student's first exam score: 88\n```\n\n\n:::\n\n```{.python .cell-code}\n# Get all exam scores for the second student (row index 1)\nprint(\"Second student's scores:\", grades_matrix[1, :])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSecond student's scores: [78 84 82]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Get all scores for the third exam (column index 2)\nprint(\"Scores in third exam:\", grades_matrix[:, 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nScores in third exam: [79 82 95 70 91]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Slice: first two students' scores\nprint(\"First two students' scores:\\n\", grades_matrix[0:2, :])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst two students' scores:\n [[88 92 79]\n [78 84 82]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Slice: first two exams for all students\nprint(\"First two exams for all students:\\n\", grades_matrix[:, 0:2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst two exams for all students:\n [[88 92]\n [78 84]\n [90 85]\n [72 78]\n [85 88]]\n```\n\n\n:::\n:::\n\n\n\n### 3.2 Element-wise operations and broadcasting\n\n**Concept:**  \n- **Element-wise operations** apply a calculation to each element of an array.  \n- **Broadcasting** lets NumPy apply operations between arrays of different shapes by \"stretching\" one to match the other (without copying data).  \n- This is much faster and cleaner than using Python loops.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Add 5 points to every score (element-wise addition)\nprint(\"Original:\\n\", grades_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\n+5 to every score:\\n\", grades_matrix + 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n+5 to every score:\n [[ 93  97  84]\n [ 83  89  87]\n [ 95  90 100]\n [ 77  83  75]\n [ 90  93  96]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Multiply all scores by 1.1 to simulate a 10% bonus\nprint(\"\\n10% bonus:\\n\", grades_matrix * 1.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n10% bonus:\n [[ 96.8 101.2  86.9]\n [ 85.8  92.4  90.2]\n [ 99.   93.5 104.5]\n [ 79.2  85.8  77. ]\n [ 93.5  96.8 100.1]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Broadcasting: subtract the minimum score in each column (exam) from that column\nmin_scores_per_exam = grades_matrix.min(axis=0)  # shape: (n_exams,)\nprint(\"\\nMinimum scores per exam:\", min_scores_per_exam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMinimum scores per exam: [72 78 70]\n```\n\n\n:::\n\n```{.python .cell-code}\nadjusted = grades_matrix - min_scores_per_exam  # broadcasting happens here\nprint(\"\\nScores adjusted by exam minimum:\\n\", adjusted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nScores adjusted by exam minimum:\n [[16 14  9]\n [ 6  6 12]\n [18  7 25]\n [ 0  0  0]\n [13 10 21]]\n```\n\n\n:::\n:::\n\n\n\n### 3.3 Matrix multiplication and axis-based operations\n\n**Concept:**  \n- **Matrix multiplication** (`np.dot`) combines rows and columns, often used in deep learning layers to combine inputs with weights.  \n- **Axis-based operations** let you apply functions (mean, sum, etc.) across rows or columns:\n\n  - `axis=0` â†’ operate down columns (across rows)\n  \n  - `axis=1` â†’ operate across columns (per row)\n\n**Extra notes:**\n- **`np.ones(shape)`** creates an array of ones with the given shape.  \n  - Here we use it for **equal weights** when averaging scores: each exam gets the same weight.  \n- **`.flatten()`** converts a multi-dimensional array into a 1D array.  \n  - After matrix multiplication, the result might be shape `(n_students, 1)`; flattening makes it easier to print and work with.\n\n\n![\"Matrix multiplication producing a column vector, then flattening to a 1D array. Here, each row of the grades matrix is multiplied by the weights vector to produce a single average score per student.\"](images/matrix_flatten.png){fig-align=\"center\" width=\"80%\"}\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Example: equal-weight average across exams (axis=1 â†’ per student)\navg_scores_axis = grades_matrix.mean(axis=1)\nprint(\"Average score per student (axis=1):\", [f\"{x:.2f}\" for x in avg_scores_axis])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage score per student (axis=1): ['86.33', '81.33', '90.00', '73.33', '88.00']\n```\n\n\n:::\n\n```{.python .cell-code}\n# Example: average score per exam (axis=0 â†’ per exam)\navg_scores_exam = grades_matrix.mean(axis=0)\nprint(\"Average score per exam (axis=0):\", [f\"{x:.2f}\" for x in avg_scores_exam])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage score per exam (axis=0): ['82.60', '85.40', '83.40']\n```\n\n\n:::\n\n```{.python .cell-code}\n# Using matrix multiplication to compute averages\nn_exams = grades_matrix.shape[1]\nweights = np.ones((n_exams, 1)) / n_exams  # shape: (n_exams, 1)\naverages_via_dot = np.dot(grades_matrix, weights).flatten()\nprint(\"\\nAverages via matrix multiplication:\",\n      [f\"{x:.2f}\" for x in averages_via_dot])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAverages via matrix multiplication: ['86.33', '81.33', '90.00', '73.33', '88.00']\n```\n\n\n:::\n\n```{.python .cell-code}\n# Weighted sum example: suppose exams have weights 0.5, 0.3, 0.2\nexam_weights = np.array([0.5, 0.3, 0.2]).reshape(-1, 1)  # shape: (n_exams, 1)\nweighted_scores = np.dot(grades_matrix, exam_weights).flatten()\nprint(\"\\nWeighted average per student:\",\n      [f\"{x:.2f}\" for x in weighted_scores])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWeighted average per student: ['87.40', '80.60', '89.50', '73.40', '87.10']\n```\n\n\n:::\n:::\n\n\n\n### 3.4 From NumPy Arrays to pandas DataFrames\n\n**Concept:**  \n- A **NumPy array** is efficient for numerical operations but has no column or row labels â€” you must remember indexes yourself.  \n- A **pandas DataFrame** wraps a NumPy array with **labels** (row and column names), allowing:\n\n  - Easier indexing by name (`df[\"Math\"]`) instead of position.\n  - Mixed data types in one table (numbers, text, dates).\n  - Built-in data inspection methods (`.head()`, `.info()`, `.describe()`).\n\n**Key point:** Deep learning libraries often use NumPy arrays internally, but pandas is more convenient for data cleaning and exploration.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Our NumPy grades_matrix (from Section 3)\nprint(\"NumPy array:\\n\", grades_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumPy array:\n [[88 92 79]\n [78 84 82]\n [90 85 95]\n [72 78 70]\n [85 88 91]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"Shape:\", grades_matrix.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShape: (5, 3)\n```\n\n\n:::\n\n```{.python .cell-code}\n# Convert to DataFrame with labels\nexam_names = [\"Exam 1\", \"Exam 2\", \"Exam 3\"]\nstudent_names = list(student_scores.keys())\ngrades_df = pd.DataFrame(grades_matrix, index=student_names, columns=exam_names)\n\nprint(\"\\nDataFrame:\\n\", grades_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDataFrame:\n        Exam 1  Exam 2  Exam 3\nAlice      88      92      79\nBob        78      84      82\nCarol      90      85      95\nDave       72      78      70\nEve        85      88      91\n```\n\n\n:::\n\n```{.python .cell-code}\n# Accessing data\nprint(\"\\nScore of Carol in Exam 2 (by labels):\", grades_df.loc[\"Carol\", \"Exam 2\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nScore of Carol in Exam 2 (by labels): 85\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"Score of Carol in Exam 2 (by position):\", grades_matrix[2, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nScore of Carol in Exam 2 (by position): 85\n```\n\n\n:::\n\n```{.python .cell-code}\n# Quick stats for each exam\nprint(\"\\nExam averages:\\n\", grades_df.mean().round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nExam averages:\n Exam 1    82.6\nExam 2    85.4\nExam 3    83.4\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n## 4. Working with Data\n\n**Concept:**  \nIn real projects, we often load datasets from CSV or Excel files.  \nPandas DataFrames are perfect for this stage because they:\n\n1. Read files directly into a labeled table.\n2. Make it easy to explore and summarize the data.\n3. Allow quick selection of features (`X`) and target labels (`y`) for model training.\n\nWeâ€™ll load a **planar dataset** with two numeric features (`x_coord`, `y_coord`) and a binary label (`label`).\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Load into DataFrame\nraw_df = pd.read_csv(data_path_here)\n\n# Inspect\nprint(\"Shape:\", raw_df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShape: (400, 3)\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nFirst 5 rows:\\n\", raw_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst 5 rows:\n     x_coord   y_coord  label\n0  1.204442  3.576114      0\n1  0.158710 -1.482171      0\n2  0.095247 -1.279955      0\n3  0.349178 -2.064380      0\n4  0.694150  2.889109      0\n```\n\n\n:::\n\n```{.python .cell-code}\n# Separate features and target\nfeatures = [\"x_coord\", \"y_coord\"]\ntarget = \"label\"\n\nX = raw_df[features].copy()\ny = raw_df[target].copy()\n\nprint(\"\\nFeatures sample:\\n\", X.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFeatures sample:\n     x_coord   y_coord\n0  1.204442  3.576114\n1  0.158710 -1.482171\n2  0.095247 -1.279955\n3  0.349178 -2.064380\n4  0.694150  2.889109\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nLabels sample:\\n\", y.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLabels sample:\n 0    0\n1    0\n2    0\n3    0\n4    0\nName: label, dtype: int64\n```\n\n\n:::\n:::\n\n\n\n### 4.1 Visualizing the dataset\n\n**Concept:**  \nA scatter plot lets us see how the two features (`x_coord`, `y_coord`) relate to the class label.  \nIf classes are not linearly separable, a simple logistic regression will likely underperform compared to a neural network.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 5))\n\n# Bright, high-contrast colors\ncolors = {0: \"orange\", 1: \"teal\"}\n\n# Scatter plot\nfor label_value in sorted(y.unique()):\n    subset = X[y == label_value]\n    plt.scatter(\n        subset[\"x_coord\"], subset[\"y_coord\"],\n        c=colors[label_value],\n        edgecolor=\"k\",\n        s=50,\n        label=f\"Class {label_value}\"\n    )\n\nplt.xlabel(\"x_coord\")\nplt.ylabel(\"y_coord\")\nplt.title(\"Planar Dataset by Label\")\nplt.legend(title=\"Label\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](python_workshop_files/figure-html/unnamed-chunk-11-1.png){width=576}\n:::\n:::\n\n\n\n\n## 5. Logistic Regression vs Neural Network\n\n**Concept:**  \nWeâ€™ll train two models on the planar dataset:\n\n1. **Logistic Regression** â€” a single-layer model that produces a linear decision boundary.\n2. **Shallow Neural Network** â€” one hidden layer that can model non-linear boundaries.\n\nThis comparison shows why neural networks can outperform linear models on complex patterns.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\n# Logistic Regression model: single Dense layer\nlog_reg_model = Sequential([\n    Dense(1, activation='sigmoid', input_shape=(2,))\n])\nlog_reg_model.compile(optimizer=Adam(),\n                      loss='binary_crossentropy',\n                      metrics=['accuracy'])\nlog_reg_history = log_reg_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nlog_acc = log_reg_model.evaluate(X, y, verbose=0)[1]\n\n\nprint(f\"Logistic Regression Accuracy: {log_acc:.2f} \\n \")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Accuracy: 0.47 \n \n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Neural Network model: one hidden layer\nnn_model = Sequential([\n    Dense(10, activation='relu', input_shape=(2,)),\n    Dense(1, activation='sigmoid')\n])\nnn_model.compile(optimizer=Adam(),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])\nnn_history = nn_model.fit(X, y, epochs=100, batch_size=32, verbose=0)\nnn_acc = nn_model.evaluate(X, y, verbose=0)[1]\n\nprint(f\"Neural Network Accuracy:     {nn_acc:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNeural Network Accuracy:     0.58\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Extract loss values\nlog_loss = log_reg_history.history['loss']\nnn_loss = nn_history.history['loss']\n\n# Extract accuracy values\nlog_acc_hist = log_reg_history.history['accuracy']\nnn_acc_hist = nn_history.history['accuracy']\n\nepochs_range = range(1, len(log_loss) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, log_loss, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_loss, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, log_acc_hist, label='Logistic Regression', color='orange')\nplt.plot(epochs_range, nn_acc_hist, label='Neural Network', color='teal')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](python_workshop_files/figure-html/unnamed-chunk-14-3.png){width=1152}\n:::\n:::\n\n\n\n### 5.1 Visualizing the Decision Boundary\n\nTo better understand how each model separates the two classes, we will define an **auxiliary plotting function** called `plot_decision_boundary`.  \n\nThis function will:\n1. Create a **grid** over the feature space.\n2. Use the model to **predict the class** for each point in the grid.\n3. Display the predicted regions as a **colored background**.\n4. Overlay the **actual data points** on top.\n\nAfter defining this function, we will call it for both the logistic regression and neural network models to visually compare their decision boundaries.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y, title):\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[\"x_coord\"].min() - 1, X[\"x_coord\"].max() + 1\n    y_min, y_max = X[\"y_coord\"].min() - 1, X[\"y_coord\"].max() + 1\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 300),\n        np.linspace(y_min, y_max, 300)\n    )\n    \n    # Predict over the grid\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(grid_points, verbose=0)\n    Z = (Z > 0.5).astype(int).reshape(xx.shape)\n\n    # Plot contour and points\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Oranges, alpha=0.3)\n    \n    colors = {0: \"orange\", 1: \"teal\"}\n    for label_value in sorted(y.unique()):\n        subset = X[y == label_value]\n        plt.scatter(\n            subset[\"x_coord\"], subset[\"y_coord\"],\n            c=colors[label_value],\n            edgecolor=\"k\",\n            s=50,\n            label=f\"Class {label_value}\"\n        )\n    plt.xlabel(\"x_coord\")\n    plt.ylabel(\"y_coord\")\n    plt.title(title)\n    plt.legend()\n\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Plot for both models side by side\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplot_decision_boundary(log_reg_model, X, y, \"Logistic Regression Decision Boundary\")\n\nplt.subplot(1, 2, 2)\nplot_decision_boundary(nn_model, X, y, \"Neural Network Decision Boundary\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](python_workshop_files/figure-html/unnamed-chunk-15-5.png){width=1152}\n:::\n:::\n\n\n",
    "supporting": [
      "python_workshop_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}